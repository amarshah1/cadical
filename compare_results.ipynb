{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   file_name                     result            time_limit\n",
      "0                                                                           ef330d1b144055436a2d576601191ea5-crn_11_99_u.cnf                      UNSAT     9.137923240661621\n",
      "1                                                               46d75199933980fdb856f13e5b1817dd-1-TC-256-K-64.sanitized.cnf  a non-zero exit -6 status    300.32532501220703\n",
      "2                                                                edceb8782e72e290fa54757dbfdd0173-x9-09057.sat.sanitized.cnf                      UNSAT     402.2980101108551\n",
      "3                                                    70ef2b6bdc4101a0e35cf3d165571fe3-qwh.50.1250.shuffled-as.sat03-1655.cnf                        SAT      738.501430273056\n",
      "4                                                                  c5ae0ec49de0959cd14431ce851c14f8-Circuit_multiplier22.cnf                        SAT     787.3294777870178\n",
      "5                                                                d195412a62cdcbb851136f60af76f463-x9-09098.sat.sanitized.cnf                      UNSAT     875.9901928901672\n",
      "6                                                        63732c330adb8de8dd47ce1693b86d0e-FmlaEquivChain_4_8_8.sanitized.cnf  a non-zero exit -6 status    172.75074291229248\n",
      "7                                                                         83b330c934d6dd35d56e1b1ca3638b3c-j3037_1_mdd_b.cnf                        SAT     1302.574092388153\n",
      "8                                                               a58b8a5eb61d3110d9fa36a03de588d2-stb_495_168.apx_1_DC-AD.cnf                      UNSAT    1318.8199121952057\n",
      "9                                               1009c791cee542cdf19651fe25e6881a-summle_X4053_steps8_I1-2-2-4-4-8-25-100.cnf                        SAT    1329.1933035850525\n",
      "10                                                               14e4cfcf0d83b2185fad41684d00d4dc-x9-12035.sat.sanitized.cnf                        SAT    1409.4695329666138\n",
      "11                                                              dd7164f2592c9675618c5c99af9b70f3-1-ZC-512-K-64.sanitized.cnf  a non-zero exit -6 status    1443.2060046195984\n",
      "12                                                   543e67dd5abc272c37775b1b742a1d9a-qwh.60.1728.shuffled-as.sat03-1659.cnf                        SAT     1590.087703704834\n",
      "13                                                               e08f11f0a3bd266ee5c78ce332de107f-x9-10096.sat.sanitized.cnf                      UNSAT     1616.926666021347\n",
      "14                                                                      22b4cf412c811872d6ba5078106aeb6c-j3037_10_rggt_b.cnf                        SAT     1784.839429140091\n",
      "15                                                               13ae2628d8e113db1786dba41a65fe38-x9-10027.sat.sanitized.cnf                        SAT    1303.6495292186737\n",
      "16                                                                08ccc34df5d8eb9e9d45278af3dc093d-simon-r16-1.sanitized.cnf                        SAT    0.3664882183074951\n",
      "17                                                               1afa2d7a3d817c3149da432eece66da8-worker_550_550_550_0.3.cnf                        SAT    312.13178753852844\n",
      "18                                                               ad4e151d80c7012d88dd79bcfceaade5-x9-08075.sat.sanitized.cnf                      UNSAT    186.06001138687134\n",
      "19                                                                7f7109dce621ef361a72b3e8cee9a962-Break_unsat_06_07.xml.cnf  a non-zero exit -6 status    7.0252439975738525\n",
      "20                                                                                                       track_main_2024.uri   a non-zero exit 1 status  0.034845590591430664\n",
      "21                                                                cdce6277b01ae06ddb95468c5f05de71-simon-r17-0.sanitized.cnf                        SAT   0.07528519630432129\n",
      "22                                                                      3988a60c6e93167763c6fd2a347d5859-Break_08_24.xml.cnf                        SAT     5.629793882369995\n",
      "23                                                                2b043efb4bde6d83f7c95a8e8e2d7bf8-simon-r21-0.sanitized.cnf                        SAT   0.11589694023132324\n",
      "24                                                       f8443057c188f6c9cfb711ff3d4aa6ff-constraints_17_0.5_2.sanitized.cnf  a non-zero exit -6 status    16.997069120407104\n",
      "25                                                                        04ded94454830d4ea960327e8b91f5a3-mdp-28-14-sat.cnf                        SAT     566.1208407878876\n",
      "26                                                         a38affaa741c958fc32769d5fe89b06c-frb65-12-2.used-as.sat04-874.cnf                        SAT      97.6516604423523\n",
      "27                                                               3a75ad246dbc750a7391ad887c5b0835-x9-11093.sat.sanitized.cnf                        SAT    252.96102213859558\n",
      "28                      303480ca7e8322d771c94caf4ebd4e95-circuit_48in64out_with_700gates_4in4out_dist128_seed1.sanitized.cnf                        SAT    266.42641735076904\n",
      "29                                                             6543793076c30fc7cdfa5a3c819cedc7-oisc-subrv-sll-nested-11.cnf  a non-zero exit -9 status     313.8017773628235\n",
      "30                                                                     0cace8a29a1d6b225a8da561d35e8f5a-lru_10.sanitized.cnf                        SAT    338.01461720466614\n",
      "31                                                                  50019e4419d48196bb4b95933a8b5030-noL-11-14.sanitized.cnf                        SAT    382.07967019081116\n",
      "32                                                                                                                  unzip.py   a non-zero exit 1 status   0.03413963317871094\n",
      "33                                                       6add7e416c1126607afeb2666af330ac-constraints_16_0.5_1.sanitized.cnf                      UNSAT     571.8437790870667\n",
      "34                      170b13af977e962321c493544b2bd0a9-circuit_48in64out_with_800gates_4in4out_dist128_seed4.sanitized.cnf                        SAT     627.1558392047882\n",
      "35                                                                            ec84eecb124c63d4757e083dd0e5a9ff-mchess_15.cnf                      UNSAT       949.07559466362\n",
      "36                                                                      c8e64404361f2426490d39459832c66a-64_25.sanitized.cnf                        SAT      773.176908493042\n",
      "37                                                                          334aa882de28856fc8c75885285d2a3c-HCP-446-105.cnf  a non-zero exit -6 status      918.214837551117\n",
      "38                                                                  ca14adcb9296a7b31d7815c2ed16d0f1-ITC2021_Early_3.xml.cnf                        SAT    12.432169437408447\n",
      "39                                                               915a25bd189357e4c6d7771b69a6849f-x9-09004.sat.sanitized.cnf                      UNSAT     669.5578818321228\n",
      "40                                                                     f50eaf02a8041510b64104998cc81d2f-sted5_0x24204-50.cnf                        SAT    1007.3599445819855\n",
      "41                                                               19e2c3a0865c8c1b4543d11213bebe5f-x9-09024.sat.sanitized.cnf                      UNSAT    271.48260283470154\n",
      "42                                                        7429e380834066c206394139c9e1e17d-af-synthesis_stb_50_100_9_sat.cnf                        SAT    1604.1653547286987\n",
      "43                                                               7b9d8b9b31b530effd634f5a8f1f4411-stb_531_83.apx_2_DC-ST.cnf                      UNSAT    1642.8901689052582\n",
      "44                                                                            8704094951693f99fd21403a039c8131-mchess_16.cnf                      UNSAT     580.1816735267639\n",
      "45                                                               f376d4c191518ed704326960b6b19a4b-x9-10084.sat.sanitized.cnf                      UNSAT     1575.468246459961\n",
      "46                                                               c5a98231dd54cbca06135293bb7e1985-x9-11053.sat.sanitized.cnf                        SAT     411.8127737045288\n",
      "47                                                                5ee7de2bd112aa39485e79c9d487bf8f-simon-r23-1.sanitized.cnf                        SAT    0.0673668384552002\n",
      "48                       6f7a0e1cf94b6b26eafc08a827a692ce-circuit_64in64out_with_64gates_8in5out_dist256_seed1.sanitized.cnf                        SAT     189.1146879196167\n",
      "49                                                               475803537529d9d14a034957f48a6d38-x9-09076.sat.sanitized.cnf                      UNSAT     329.0576856136322\n",
      "50                                             dc7817dfa2817916b266c1cfacd2ee66-constraints_25_4_5_12_12_0_0_0.sanitized.cnf  a non-zero exit -6 status     18.04038691520691\n",
      "51                                                            e6cdc2687fa53506021f05b60ad0c6a2-GracefulGraph-K05-P02_c18.cnf                        SAT    429.04988145828247\n",
      "52                                                                                  ecca77b5350eca6a4323edd5b38208c6-004.cnf                        SAT     455.5075840950012\n",
      "53                                                 1a20d903db74ab566efc68a35e6b0ec5-hwmcc20miters-iso-rast-p11.sanitized.cnf  a non-zero exit -6 status     590.4017977714539\n",
      "54                                       bd8bc25be2b36c64b38459c17e815814-pcmax-scheduling-m11-1517-6802-UNSAT.sanitized.cnf                      UNSAT     763.9916117191315\n",
      "55                                                              4a8285a53f30b35016b0c85ea17ba155-1-TC-256-K-68.sanitized.cnf  a non-zero exit -6 status     788.8074033260345\n",
      "56                                                              dd5fc8da5454a990a0e7999884158c64-1-ET-256-K-70.sanitized.cnf  a non-zero exit -6 status     849.2520611286163\n",
      "57                                                                  0d81711a3d73c828e8c6e12607eda82d-noL-11-20.sanitized.cnf                        SAT     746.9651825428009\n",
      "58                                                                  de7a6b03999e2bc5bd750831c2662a4d-noL-11-18.sanitized.cnf                        SAT    1626.6760551929474\n",
      "59                                             f054205a7cef98e5021016f864c69816-summle_X11112_steps6_I1-2-2-4-4-8-25-100.cnf                        SAT     1485.724224805832\n",
      "60                                      0ac6aaf6db6a0e4ec3a06e865d01086f-pcmax-scheduling-m13-2011-12813-UNSAT.sanitized.cnf                      UNSAT    1102.7353370189667\n",
      "61                                                                4073757aae06fc2b50c043f088b132b4-simon-r19-1.sanitized.cnf                        SAT   0.06573343276977539\n",
      "62                                                     0fa9521ff633b27be11525a7b0f7d8b6-jgiraldezlevy.2200.9086.08.40.41.cnf  a non-zero exit -6 status    0.9676859378814697\n",
      "63                                                                     07e6413459f92b613498a719125b6239-j3037_10_mdd_bm1.cnf                      UNSAT    1322.3377676010132\n",
      "64                                                                2fcd8533eba981967292f1b6e41f7433-simon-r20-0.sanitized.cnf                        SAT       0.1177978515625\n",
      "65                                                                d3893e43819a907055a84e48a6ee97ba-g2-ak128boothbg2msaig.cnf                        SAT    5.9773170948028564\n",
      "66                                                              cd361d33986dccd7f2d86016d6c35241-ecarev-110-4099-22-30-7.cnf  a non-zero exit -6 status     6.926194667816162\n",
      "67                                                       8e720686372c5037f30b4fc7b1c71d48-constraints_17_0.4_1.sanitized.cnf                        SAT    176.34985542297363\n",
      "68                                                                e99e266b422513a2898c13898a1de501-rbsat-v760c43649gyes9.cnf                        SAT     469.6948320865631\n",
      "69                                                               bbfed8974655bca520259deb10d2347b-x9-09054.sat.sanitized.cnf                        SAT    281.30061531066895\n",
      "70                                                               f296fe701a562022c0de0cf565fbca7d-x9-08014.sat.sanitized.cnf                      UNSAT     285.0987012386322\n",
      "71                                                     8b31606e10656ff7eb2936262b647443-stable-300-0.1-20-98765432130020.cnf                        SAT    1267.4575281143188\n",
      "72                                                               57f4ea7ab160d996e38e69fac59869c4-x9-09047.sat.sanitized.cnf                      UNSAT    413.28000497817993\n",
      "73                                                   2a53e9c6a25a50d753a94ead66065826-mp1-ps_5000_21250_3_0_0.8_0_1.50_6.cnf                      UNSAT     340.5752010345459\n",
      "74                                                       fb0e505b8bd19a34f1f80b8e020e7856-constraints_17_0.4_2.sanitized.cnf                      UNSAT     514.4041433334351\n",
      "75                                                               eddd68e14d69cce7190b99f4e7abdafb-x9-10098.sat.sanitized.cnf                        SAT    415.11739683151245\n",
      "76                                                       b4b41b2ff14427e5715cb9bee06d4602-constraints_17_0.3_2.sanitized.cnf                      UNSAT    394.00223183631897\n",
      "77                                                               c6568fc8805127e876c4c23551bf49fa-x9-10076.sat.sanitized.cnf                      UNSAT    1577.3451402187347\n",
      "78                                                               ded23680dfeab2879c05bc0e4de21126-x9-09014.sat.sanitized.cnf                      UNSAT     639.3673434257507\n",
      "79                                                               8b18bb75459a4161633ba2a3c8ee183e-x9-11062.sat.sanitized.cnf                        SAT    1783.5335357189178\n",
      "80                                                               5cb1ca8fe8a9d9125ea9accd498445f1-stb_531_83.apx_1_DC-ST.cnf                      UNSAT    1671.9876749515533\n",
      "81                                                               f45e5faf1bcccbdd3065dd6367c3bd16-x9-10083.sat.sanitized.cnf                      UNSAT    1702.7818467617035\n",
      "82                                                       8530c911b75ab1b276042043d118a875-constraints_16_0.4_1.sanitized.cnf                      UNSAT     525.0124001502991\n",
      "83                                              3bf8ba6bb4e4ea9ad08b1b058661ba2e-summle_X4044_steps7_I1-2-2-4-4-8-25-100.cnf                        SAT     1093.916494846344\n",
      "84                                                                           04e219c640ed59dc68ea2d50493de5b5-mp1-Nb5T15.cnf                        SAT    188.81486463546753\n",
      "85                                                                      812926407774771b3bd9885f7bfa4841-lru_9.sanitized.cnf                        SAT    298.11488366127014\n",
      "86                           1427381a809c64c721838894ece6756d-shuffling-2-s25242449-of-bench-sat04-727.used-as.sat04-753.cnf                        SAT     612.4407312870026\n",
      "87                                                       9cd3acdb765c15163bc239ae3a57f880-FmlaEquivChain_4_6_6.sanitized.cnf  a non-zero exit -6 status    17.990535259246826\n",
      "88                                                                      e85fb114c33f450dc2622c78bc6fa019-lru_7.sanitized.cnf                        SAT    230.87739896774292\n",
      "89                                                                       be6411f4784a3c879886dda807cdc607-j3037_10_mdd_b.cnf                        SAT    1688.4262206554413\n",
      "90                                                          aacfb8797097f698d14337d3a04f3065-barman-pfile06-022.sas.ex.7.cnf                      UNSAT     35.36276412010193\n",
      "91                                                                3129198788f182ce6955b18aa3c7e61e-simon-r24-1.sanitized.cnf                        SAT   0.16776108741760254\n",
      "92                                                                  52aaf653a79ca14fa1127cda32aa94ce-noL-11-10.sanitized.cnf                        SAT    1504.3001515865326\n",
      "93                                                             ee57608ea4b74f25aebca809d59711c9-oisc-subrv-sll-nested-13.cnf  a non-zero exit -9 status    438.45024061203003\n",
      "94                                                                 c801a020a6c8bc3c287fea495203b114-worker_20_40_20_0.95.cnf                        SAT     2.269409656524658\n",
      "95                                                                            ddc0720fa5a91d9cc0dc726644ab9e9f-6s167-opt.cnf                      UNSAT    139.74547839164734\n",
      "96                                                          44fd38bd0c5fc7336be468161dee4eda-post-cbmc-aes-ee-r3-noholes.cnf  a non-zero exit -6 status      436.597820520401\n",
      "97                                                                7e1d279559b202016e5797901e731a39-simon-r25-0.sanitized.cnf                        SAT   0.11684298515319824\n",
      "98                                                                089f909e37b3ef0c4d90687f7e22b68f-simon-r18-0.sanitized.cnf                        SAT   0.06708645820617676\n",
      "99                                                                      aceab5a3452e2901e645065bda3e8847-lru_8.sanitized.cnf                        SAT    263.57140731811523\n",
      "100                     f86dad4ba35369eb720a0c9ddc45037a-combined-crypto1-wff-seed-1-wffvars-450-cryptocplx-40-overlap-2.cnf                        SAT    50.035874366760254\n",
      "101                                                              7fb202a51c0223f3119887a57086ca4d-x9-09051.sat.sanitized.cnf                      UNSAT     716.2665989398956\n",
      "102                                                                     282a02a1743eb47c6b340e52ecce40a2-lru_6.sanitized.cnf                        SAT     197.9287142753601\n",
      "103                                                             05ed64e4e6229f446082752936768489-stb_495_168.apx_2_DC-AD.cnf                      UNSAT    1462.3832039833069\n",
      "104                                                                      02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12.cnf  a non-zero exit -6 status    1.4679875373840332\n",
      "105                                                                    af750c18578d52e60472315692ad83c0-si2-b03m-m800-03.cnf                        SAT    167.18946385383606\n",
      "106                                                               0cfe9c90d3a51435a5e4dba7634b882f-g2-ak128boothbg2msisc.cnf                        SAT    4.7717835903167725\n",
      "107                                                               7cbc3ce2052ba7c5b501f75af58ab3c4-simon-r22-1.sanitized.cnf                        SAT   0.11594986915588379\n",
      "108                                     7f1c7da531539f3be4a1e0e916913086-pcmax-scheduling-m19-2974-16501-UNSAT.sanitized.cnf                      UNSAT    1041.5570814609528\n",
      "109                                                              a45a0358685867bd4f1c7f7c0b0e379c-x9-10014.sat.sanitized.cnf                        SAT    366.20573234558105\n",
      "110                                                      fb51311320bb42bdb893249998a77f40-constraints_16_0.3_1.sanitized.cnf                      UNSAT     953.5022830963135\n",
      "111                                              ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf                        SAT    0.3671703338623047\n",
      "112                                                             bd19da88800086971e554e9d66bc641d-stb_588_138.apx_1_DC-ST.cnf                      UNSAT    1522.2179458141327\n",
      "113                                                               16c27d738cb45b766b8823ca4f428cf0-rbsat-v760c43649gyes7.cnf                        SAT    1197.9142060279846\n",
      "114                                                              195852083a05edee1902233698eec14a-x9-11077.sat.sanitized.cnf                        SAT    1646.7619407176971\n",
      "115                                                           35789168b67a74985804902008251269-tseitingrid6x200_shuffled.cnf                    TIMEOUT    1800.0054864883423\n",
      "116                                                              b7273af3d468ea2595f11a6dbd6ef6ce-x9-10007.sat.sanitized.cnf                    TIMEOUT    1800.0076081752777\n",
      "117                                                              3c6e1d1c4b8d3d08aa4c1df3805f4f7d-x9-10031.sat.sanitized.cnf                    TIMEOUT    1800.0111331939697\n",
      "118                                                      8d2dfc50c1759dc11a6564d5c368c6df-FmlaImplyChain_3_7_8.sanitized.cnf                    TIMEOUT    1800.0098173618317\n",
      "119                                                                  47e1ada0070708c2953d322c06aea00d-noL-11-8.sanitized.cnf                    TIMEOUT    1800.0079078674316\n",
      "120                                                                      a08e66296d00f480e9ccadd79fa8b904-j3045_4_gmto_b.cnf                    TIMEOUT    1800.0089240074158\n",
      "121                                                       2d6d26e1c5b4f2763c03697a6d00d3cf-fixedbandwidth-eq-31_shuffled.cnf                    TIMEOUT    1800.0131556987762\n",
      "122                                                           45a09efb026036ff4b8d19024a7563a9-fermat-931960058139995587.cnf                    TIMEOUT    1800.0105838775635\n",
      "123                                     74f145bb935650f5c982d7eec6967945-pcmax-scheduling-m43-38782-385402-SAT.sanitized.cnf                    TIMEOUT    1800.0160808563232\n",
      "124                                                        8b91c5bf4dfd87ffb46c5ef88a3ef1cd-lec_mult_DvW_12x11.sanitized.cnf                    TIMEOUT    1800.0114388465881\n",
      "125                                                               5e5fe73a2e0ffc8e19873298566919fb-rbsat-v760c43649gyes3.cnf                    TIMEOUT    1800.0126576423645\n",
      "126                                                 bbfe2b27182d2ee7fefdb557f458ac9c-cliquecolouring_n21_k6_c5.sanitized.cnf                    TIMEOUT    1800.0103979110718\n",
      "127                                                           24bdfb34654b1aa703fae31ea91e7c7f-tseitin_d3_n158.sanitized.cnf                    TIMEOUT    1800.0116515159607\n",
      "128                                                              1507d9812624b3e0eaf15e40100be020-x9-12014.sat.sanitized.cnf                    TIMEOUT    1800.0117304325104\n",
      "129                                                                 c73edc350cfa8e07af58db50054aea45-noL-11-12.sanitized.cnf                    TIMEOUT     1800.009258031845\n",
      "130                                                      36dbc206c7d9773bebb03f752070cee9-tseitin_grid_n11_m20.sanitized.cnf                    TIMEOUT    1800.0115849971771\n",
      "131                                                                     fae08ac5b6d7eeb4f56f1cfac6c51703-j3045_4_mdd_bm1.cnf                    TIMEOUT    1800.0128667354584\n",
      "132                                                              695e287a447fcdd924985a6e73057a38-rbsat-v1150c84314gyes1.cnf                    TIMEOUT    1800.0111801624298\n",
      "133                                                            e85e793a66a2c9d390f40acf29a5346b-1-ET-512-K-110.sanitized.cnf                    TIMEOUT    1800.0480601787567\n",
      "134                                                                     23daec2c71ce3c1e346e6042f3dc42eb-mdp-32-14-unsat.cnf                    TIMEOUT    1800.0126011371613\n",
      "135                                                                     9a06700729bf3426beb8a62040f92960-mdp-36-12-unsat.cnf                    TIMEOUT     1800.010308265686\n",
      "136                                        f48735588e3515135f039e8bc8efaee5-asconhashv12_opt64_H11_M2-MxJOnbQIXNd_m5_6.c.cnf                    TIMEOUT     1800.019085407257\n",
      "137                                                       72c0d81e16d91bcaed808efcde2e5069-Folkman-175-1251868.sanitized.cnf                    TIMEOUT    1800.0139698982239\n",
      "138                                                     c9f511adbb3d6e1b37b410f972f270c6-af-synthesis_stb_50_140_3_unsat.cnf                    TIMEOUT    1800.0183947086334\n",
      "139                                                             b11374f7fe60b9a63fbfde24b1a0a439-stb_418_125.apx_2_DC-AD.cnf                    TIMEOUT     1800.016399383545\n",
      "140                                                                      91e0db01b254eb78f6643328045bfeb9-sted5_0x1e3-20.cnf                    TIMEOUT    1800.0226938724518\n",
      "141                                                              af05a6b68a1cff165b684d9ff0ae3b3b-x9-12098.sat.sanitized.cnf                    TIMEOUT    1800.0185742378235\n",
      "142                                                                     4f2d6659b5c9bf594ea12e3e1a85799f-mdp-32-11-unsat.cnf                    TIMEOUT    1800.0154645442963\n",
      "143                                                     e7248b57a310ad461924eb17956cdf3a-Folkman-190-358004741.sanitized.cnf                    TIMEOUT    1800.0115804672241\n",
      "144                                                             ff9e9b01d50d01ecb6740472d7dd36cd-1-ZC-512-K-61.sanitized.cnf                    TIMEOUT    1800.0327188968658\n",
      "145                                                     1a1c9d1dc1ae7fcff92e3c3499864b75-af-synthesis_stb_50_200_4_unsat.cnf                    TIMEOUT    1800.0306794643402\n",
      "146                                                                           5690b9b0380aa9508699e56cae5918b5-170058440.cnf                    TIMEOUT     1800.011248588562\n",
      "147                                                                 b8c7c777e76995e5bf7b517f2db234ba-noL-11-16.sanitized.cnf                    TIMEOUT    1800.0164196491241\n",
      "148                                                          79b9e24dd9af185dbec18c9b0a32b1e2-g2-slp-synthesis-aes-top30.cnf                    TIMEOUT      1800.02210354805\n",
      "149                                                               f32bb347996c351bc3e9a91c58e8601d-DLTM_twitter774_83_17.cnf                    TIMEOUT    1800.0204997062683\n",
      "150                                                                           d7dbb2cba940af33b2bce37ea99b8110-6s130-opt.cnf                    TIMEOUT    1800.0168585777283\n",
      "151                                                              aa9b67fd19d54ad51b93ee4ba5dc75fc-x9-10002.sat.sanitized.cnf                    TIMEOUT    1800.0165894031525\n",
      "152                                                               0ac5ff376b826f68d384ba05a1d00de0-sokoban-p20.sas.cr.33.cnf                    TIMEOUT    1800.0481548309326\n",
      "153                                                                     3593875d75d6e836a3ac328c5426c1b5-Break_18_32.xml.cnf                    TIMEOUT     1800.017255783081\n",
      "154                                                              5865fb9a6575d2ae6542c36ab96646a9-x9-11088.sat.sanitized.cnf                    TIMEOUT    1800.0149846076965\n",
      "155                                                                       d1940d3d830eb16a7d2270ca4af4acba-mdp-28-11-sat.cnf                    TIMEOUT    1800.0153391361237\n",
      "156                                                             cccee4b0d4c70a5d5422663e2d8887da-1-ET-512-K-96.sanitized.cnf                    TIMEOUT     1800.052708864212\n",
      "157                                                                5ea72bcdccc86bd1a924029f7b81aec5-atco_enc1_opt1_10_15.cnf                    TIMEOUT    1800.0232467651367\n",
      "158                                                              2dd4b14a8a820e7baa073554ae2c6cb0-rphp_p8_r160.sanitized.cnf                    TIMEOUT    1800.0208339691162\n",
      "159                                                                cdd131110acc861a5a01fae6c4936c91-6g_6color_366_050_04.cnf                    TIMEOUT    1800.1296381950378\n",
      "160                                           f54b352934be08821aa75b3424112956-linked_list_swap_contents_safety_unwind54.cnf                    TIMEOUT    1800.4804527759552\n",
      "161                                                                    210fe1c0ffefde5bcd03d59b1efa6984-32_325.sanitized.cnf                    TIMEOUT    1800.5088603496552\n",
      "162                                           0f262afd5b117000f8b4734d175aadab-linked_list_swap_contents_safety_unwind73.cnf                    TIMEOUT    1800.6609058380127\n",
      "163                                                                    88e89266e58f1f125ebc4e0f66e2c060-32_200.sanitized.cnf                    TIMEOUT    1800.7862794399261\n",
      "164                                                                   093fa3ff9f7bc9979c43f9d2310ac21d-128_125.sanitized.cnf                    TIMEOUT    1801.1004102230072\n",
      "165                                                                5952b9f18b73dabb531c60e3ac85d26f-ctl_4291_567_9_unsat.cnf                    TIMEOUT    1800.0038528442383\n",
      "166                                                1545660e72158c7e90da5ad4c877a124-hwmcc20miters-iso-rast-p06.sanitized.cnf                    TIMEOUT    1800.0038685798645\n",
      "167                                                                 29e61b1317804fb897e84237613d42bb-worker_30_60_25_0.9.cnf                    TIMEOUT     1800.004323720932\n",
      "168                                                              df1bd67978b9b0ec1d326ba174bc273c-x9-12021.sat.sanitized.cnf                    TIMEOUT    1800.0087413787842\n",
      "169                                                             ea827f81bd0e9735739adc0d9da1b446-ER_500_30_3.apx_2_DC-AD.cnf                    TIMEOUT    1800.0063571929932\n",
      "170                                                                7083b70c1976162e2693d7a493717ffd-battleship-14-26-sat.cnf                    TIMEOUT    1800.0052380561829\n",
      "171                                                                            f746dae2050dd4aa48bcd53e8bd897b4-ex065_25.cnf                    TIMEOUT    1800.0059242248535\n",
      "172                                        c6e03256680b96b52e3345a497333c2b-hwb-n24-02-S786928571.shuffled-as.sat03-1618.cnf                    TIMEOUT    1800.0059814453125\n",
      "173                                                             39277cab188349aee0f229cb7341b5c5-crafted_n12_d6_c4_num23.cnf                    TIMEOUT     1800.100216627121\n",
      "174                                                       71a3dd6156463c0c55c4b38394faa753-af-synthesis_stb_50_120_4_sat.cnf                    TIMEOUT     1800.007712841034\n",
      "175                                                              0265448c232e3a25aa5bcd29b1b14567-x9-10051.sat.sanitized.cnf                    TIMEOUT     1800.004546403885\n",
      "176                                            25cc054738293c0d9377d7b72b78a341-string_compare_safety_cbmc_unwinding_900.cnf                    TIMEOUT    1800.1504995822906\n",
      "177                                                        385042a6042a0df5ddfec6e17f285ae7-lec_mult_CvD_11x11.sanitized.cnf                    TIMEOUT    1800.0047686100006\n",
      "178                                                                       8ffd718f763ed7a3f691cf46e57f8d98-mdp-32-10-sat.cnf                    TIMEOUT    1800.0038657188416\n",
      "179                                                     b3167d999edd81291f33636464f2f8e6-Folkman-190-104806020.sanitized.cnf                    TIMEOUT     1800.004268169403\n",
      "180                                           e8abb9c83efdd96a99990f3b914ea873-linked_list_swap_contents_safety_unwind57.cnf                    TIMEOUT    1803.7073583602905\n",
      "181                                                                      e481a97a93fd6b4303f39024d19c2867-j3037_1_gmto_b.cnf                    TIMEOUT     1800.004183769226\n",
      "182                                                                     3184e52e950ac39095a801c418f68d50-mdp-28-10-unsat.cnf                    TIMEOUT     1800.020488500595\n",
      "183                                                                 2b60f9be1439f63b7f174c6b8ca7fedf-sgen1-unsat-121-100.cnf                    TIMEOUT     1800.013333082199\n",
      "184                                                                  05c8e94aaee86390eaf6e68dd3ec3570-noL-11-2.sanitized.cnf                    TIMEOUT    1800.0138263702393\n",
      "185                                                                     b172b4c218f1e44e205575d2b51e82c4-Schur_161_5_d38.cnf                    TIMEOUT     1800.010621547699\n",
      "186                                                             d7f2e4ec8e631387c928c79e1054c663-stb_418_125.apx_1_DC-AD.cnf                    TIMEOUT    1800.0100059509277\n",
      "187                                                     ef16970ab9da31165d3c401ff9b29168-Folkman-185-152478531.sanitized.cnf                    TIMEOUT    1800.0088748931885\n",
      "188                                                     170a7d847694ee4cfa5c421757672882-af-synthesis_stb_50_140_0_unsat.cnf                    TIMEOUT     1800.008761882782\n",
      "189                                                            16c5482d8e658b54e20d59cfd4b1d588-two-trees-511v.sanitized.cnf                    TIMEOUT    1800.0081150531769\n",
      "190                                                                     4e366e723d75fe39bf6db9a24ffb059b-Dodecahedron-k7.cnf                    TIMEOUT    1800.0103487968445\n",
      "191                                                    f70788e8446c1bc42684316de0a6c164-hwmcc20miters-iso-mul3.sanitized.cnf                    TIMEOUT    1800.0233108997345\n",
      "192                                                    1f44da3598ccf8bcbe9a90fb1b2f1ebd-REGRandom-K3-L3-Seed25.sanitized.cnf                    TIMEOUT     1800.018126487732\n",
      "193                                                             40737c2f46512069d54b0d2bb3b47a20-ER_400_20_4.apx_2_DC-AD.cnf                    TIMEOUT    1800.0071511268616\n",
      "194                                            7f2b4b2bde9b866b93fbfe01341213da-asconhashv12_opt64_H5_M2-xEJ8F_m0_3_U5.c.cnf                    TIMEOUT    1800.0150129795074\n",
      "195                                                                5a068ba62fe30e435a76e2ec80896ef4-ITC2021_Middle_1.xml.cnf                    TIMEOUT    1800.0090975761414\n",
      "196                                    bfadd34e71516e95d0ba2a3a8fb2ba03-pcmax-scheduling-m19-10199-62102-UNSAT.sanitized.cnf                    TIMEOUT    1800.0106518268585\n",
      "197                                                         96a6796405061571e12d383efe27b703-WS_500_16_70_10.apx_1_DC-ST.cnf                    TIMEOUT     1800.010935306549\n",
      "198                                  a552a058e6376a36b1f1b2724f228364-IBM_FV_2004_rule_batch_1_31_1_SAT_dat.k40.debugged.cnf                    TIMEOUT     1800.023086309433\n",
      "199                                                        5b0f16de2c5419b80b608db37d07d994-lec_mult_KvW_11x10.sanitized.cnf                    TIMEOUT     1800.012903213501\n",
      "200                                                                       ab3438b504b296fcee78ec9e71969863-mdp-36-14-sat.cnf                    TIMEOUT    1800.0130553245544\n",
      "201                                                        dacb03ea801d459ef08db23a8b1d104f-lec_mult_DvW_12x12.sanitized.cnf                    TIMEOUT    1800.0100836753845\n",
      "202                      396fd56f3fd7b85afbba4254ea6e746c-circuit_32in32out_with_80gates_7in7out_dist128_seed1.sanitized.cnf                    TIMEOUT    1800.0433831214905\n",
      "203                                                              5e1c11b77cdf3717b81b957120f0f477-x9-12001.sat.sanitized.cnf                    TIMEOUT    1800.0117404460907\n",
      "204                      70bfbd054dc9ffd394fab32845b492d3-circuit_32in32out_with_100gates_7in7out_dist64_seed2.sanitized.cnf                    TIMEOUT    1800.0407621860504\n",
      "205                                      31bb67fe67e57dbeb8c4819b01892588-hwmcc17miters-xits-iso-oski15a08b08s.sanitized.cnf                    TIMEOUT    1800.0851690769196\n",
      "206                                                                 29ab150158d641568f888e8401d2ac13-bmc_QICE_snp_vld_30.cnf                    TIMEOUT    1800.1660089492798\n",
      "207                                           8d4b50ec0cf99097e9ab0835937afee5-hwmcc17miters-xits-iso-6s281b35.sanitized.cnf                    TIMEOUT    1800.1176509857178\n",
      "208                                                                     37a11ae189df2834a3f294736f34608f-mdp-28-16-unsat.cnf                    TIMEOUT    1800.0071558952332\n",
      "209                                              f0426369f61595aee97055965ee7e6a3-hwmcc12miters-xits-iso-6s111.sanitized.cnf                    TIMEOUT    1800.2376537322998\n",
      "210                                                                            88d151d4e3c6ba9bd78454de141cd108-T105.2.0.cnf                    TIMEOUT     1800.523600578308\n",
      "211                                                             cb2e8b7fada420c5046f587ea754d052-clique_n2_k10.sanitized.cnf                    TIMEOUT     1800.007176399231\n",
      "212                                                 768956cc8d1f2d18ae1929f6bb26557a-cliquecolouring_n13_k8_c7.sanitized.cnf                    TIMEOUT     1800.006587266922\n",
      "213                                                        e38601380bfbab35dd8c5927283533fb-lec_mult_CvK_11x11.sanitized.cnf                    TIMEOUT    1800.0051217079163\n",
      "214                                                                    26b648475cfd06695a17ac95f4469744-128_75.sanitized.cnf                    TIMEOUT    1800.4645195007324\n",
      "215                                                              1156429862d3c03160406d6d1a786f11-rphp_p8_r170.sanitized.cnf                    TIMEOUT    1800.0032229423523\n",
      "216                                                             4214f45042f8c504f9837afaec88bf8f-ER_400_20_7.apx_2_DC-AD.cnf                    TIMEOUT    1800.0037252902985\n",
      "217                                                       c76f3c6c2e2eae85fc5f3d499f3db88d-mp1-blockpuzzle_5x10_s7_free4.cnf                    TIMEOUT    1800.0053594112396\n",
      "218                                                             2eb039a5aa6bfb6e41d3afd41071ca55-ER_500_30_3.apx_1_DC-ST.cnf                    TIMEOUT     1800.004774093628\n",
      "219                                                        b76fa2f50a42120ec54fa98c29113326-lec_mult_DvK_12x12.sanitized.cnf                    TIMEOUT    1800.0036499500275\n",
      "220                                                              fa5c6d6736a42650656c5bc018413254-bphp_p23_h22.sanitized.cnf                    TIMEOUT    1800.0043303966522\n",
      "221                                                    2b8a711debc3a6edd01e7b4e305342d9-REGRandom-K4-L2-Seed35.sanitized.cnf                    TIMEOUT    1800.0196797847748\n",
      "222                                                                     16ca57f2cdffd5b0260fc771017e1f04-mdp-36-10-unsat.cnf                    TIMEOUT    1800.0084965229034\n",
      "223                                                                           792495b57d6145fd21d7076eab2ab06a-grs-64-64.cnf                    TIMEOUT    1800.0337798595428\n",
      "224                                                      eed5189b73738270ae3fdb8b33bf31c8-Folkman-180-11710376.sanitized.cnf                    TIMEOUT    1800.0044820308685\n",
      "225                                                                          3b59c5d9b6729f39c8483446701f4ed0-g2-T93.2.1.cnf                    TIMEOUT    1800.0916602611542\n",
      "226                                           02f6ca52f1ada872d82035088701b66a-linked_list_swap_contents_safety_unwind69.cnf                    TIMEOUT    1800.3825170993805\n",
      "227                   9276ce38c625b2d00de247f8588f1542-combined-crypto1-wff-seed-102-wffvars-500-cryptocplx-31-overlap-2.cnf                    TIMEOUT    1800.0040302276611\n",
      "228                                                      461df1a7056560279d532bc2743022b6-Folkman-185-75415683.sanitized.cnf                    TIMEOUT    1800.0043659210205\n",
      "229                                                      14ea5857d75ae2b235a3ba97373ea732-af-synthesis_stb_50_40_9_unsat.cnf                    TIMEOUT    1800.0046973228455\n",
      "230                                                                   41f4cb4992a481c9d43e2e1a4e2349ac-sgen1-sat-180-100.cnf                    TIMEOUT    1800.0051605701447\n",
      "231                                                                           2c3c28f6d939d157e909c57a265fc908-mchess_17.cnf                    TIMEOUT    1800.1265442371368\n",
      "232                                                         9e749596a8de36d8ab706c96cf128455-WS_500_16_70_10.apx_2_DC-AD.cnf                    TIMEOUT    1800.0048224925995\n",
      "233                                                            5ab3040d8617bf9f4d4cef4e56dcfd03-g2-hwmcc15deep-6s161-k17.cnf                    TIMEOUT    1800.0042088031769\n",
      "234                                                                    e371cd7cdde3e7ccb9834290fd4a92d0-32_350.sanitized.cnf                    TIMEOUT    1800.4542167186737\n",
      "235                                                                     eda4aa84aeeb306468396fa82a6bba5a-pb_300_05_lb_17.cnf                    TIMEOUT    1800.0090322494507\n",
      "236                                           0d3160b80aa406bfc718c007265b9e73-linked_list_swap_contents_safety_unwind65.cnf                    TIMEOUT    1800.1841583251953\n",
      "237                                                           6a674acad2aeac729ecb39ad9e4a2298-1-ZC-1024-K-116.sanitized.cnf                    TIMEOUT     1800.041759967804\n",
      "238                                                        00d1fe07ab948b348bb3fb423b1ef40d-lec_mult_KvW_12x11.sanitized.cnf                    TIMEOUT    1800.0044915676117\n",
      "239                                                      77b7f7bbf75faaee28f473b9941de103-Folkman-185-19924337.sanitized.cnf                    TIMEOUT    1800.0042097568512\n",
      "240                                                            4517202fdd0ccc8a97d83684b021ea96-VanDerWaerden_2-3-14_186.cnf                    TIMEOUT    1800.0042514801025\n",
      "241                                                       12b4a08e412a3bffb513ca65639c7c69-Folkman-175-7416734.sanitized.cnf                    TIMEOUT     1800.004737854004\n",
      "242                                                                     425c81fc3af7f5124b6282583d80df11-mdp-32-12-unsat.cnf                    TIMEOUT    1800.0052444934845\n",
      "243                                                       e067414e153ee98a7842bd6d6dafeee5-af-synthesis_stb_50_200_0_sat.cnf                    TIMEOUT    1800.0040700435638\n",
      "244                                                     94571e8a1081385029d1ecd53ffcdf8e-af-synthesis_stb_50_200_0_unsat.cnf                    TIMEOUT     1800.004585981369\n",
      "245                                   dafc03ea784fee849febca7b64230558-pcmax-scheduling-m30-14113-167638-UNSAT.sanitized.cnf                    TIMEOUT    1800.0108964443207\n",
      "246                                                      82d9e8d2cfea101dfe52c8359b7bb163-af-synthesis_stb_50_40_2_unsat.cnf                    TIMEOUT    1800.0097031593323\n",
      "247                                                         08be288536c3178e6874a5676493923c-g2-hwmcc15deep-bob12s02-k16.cnf                    TIMEOUT    1800.0103588104248\n",
      "248                                                      07b476c83150d3cb7e08f5d045e255ab-marg5x5.shuffled-as.sat03-1455.cnf                    TIMEOUT    1800.0069024562836\n",
      "249                                    a5419a63d913bde0ba5bcd8a8571342f-asconhashv12_opt64_H11_M2-tBi5i1RIgRz_m0_1_U23.c.cnf                    TIMEOUT    1800.0121607780457\n",
      "250                                                                          9dcbf221b8d2bb01f30bcca283f3608d-EDP3-11000.cnf                    TIMEOUT    1800.0133202075958\n",
      "251                                                    e06be7a69dbd6d6866a2799cd40c4bfe-hwmcc20miters-iso-mul7.sanitized.cnf                    TIMEOUT     1800.022893190384\n",
      "252                                                     2a15a30186afdad41a49c5c5366d01be-Timetable_C_392_E_62_Cl_26_S_28.cnf                    TIMEOUT    1800.0238156318665\n",
      "253                                                                     1e74212168124b696531fed471b480b1-mdp-32-10-unsat.cnf                    TIMEOUT     1800.008240699768\n",
      "254                                                               fb45d41807bcbbbe689de282c1310798-Break_unsat_16_27.xml.cnf                    TIMEOUT    1800.0103149414062\n",
      "255                                                                               aa39c6d885d283a085d033ab7b89f8c8-urq45.cnf                    TIMEOUT    1800.0064039230347\n",
      "256                                                      f18c7c7b8666458fd04ea7a253ae1e20-FmlaImplyChain_3_7_7.sanitized.cnf                    TIMEOUT    1800.0089709758759\n",
      "257                                                                  994783ef4ed3a0366842e1b6f9128a6f-noL-11-6.sanitized.cnf                    TIMEOUT    1800.0117659568787\n",
      "258                                                    912ce8c21d8fb8abc491cb205ec23e9f-hwmcc20miters-iso-mul2.sanitized.cnf                    TIMEOUT    1800.0097305774689\n",
      "259                                      62dcbf79f5ecef5618c9d9a00311326c-pcmax-scheduling-m13-1655-9604-UNSAT.sanitized.cnf                    TIMEOUT    1800.0064289569855\n",
      "260                                                 973d699ec01b88da869233a79aaa1912-cliquecolouring_n13_k9_c8.sanitized.cnf                    TIMEOUT    1800.0065925121307\n",
      "261                                           c21cea390a50204944aa00babd56b53c-linked_list_swap_contents_safety_unwind70.cnf                    TIMEOUT    1800.2428550720215\n",
      "262                                                        3ed56242f55e3653dbceeb4a70221787-rbsat-v945c61409gyes9-sc2009.cnf                    TIMEOUT    1800.0076596736908\n",
      "263                                        305ad7d773d6c72be9b1efbf7bf482e0-asconhashv12_opt64_H9_M2-MIC4kfhiA_m0_6_U2.c.cnf                    TIMEOUT    1800.0103106498718\n",
      "264                                                                   84c6d7e4a18aacf166105aaa3cd6e3de-128_100.sanitized.cnf                    TIMEOUT     1800.613404750824\n",
      "265                                     6fc528fc3d0fd5a2c50992feb8bf0357-pcmax-scheduling-m24-17855-226744-SAT.sanitized.cnf                    TIMEOUT      1800.00657248497\n",
      "266                                                       773f3bd29e202ff700d8b5b459857a2c-Folkman-175-9054056.sanitized.cnf                    TIMEOUT    1800.0063285827637\n",
      "267                                                                               34fa0922e66aee3b6f6ffe1760f459d4-f9idw.cnf                    TIMEOUT    1800.0190916061401\n",
      "268                                                                         1b3c7ac17705be94daaddb5f5ae4816b-rook-56-0-0.cnf                    TIMEOUT    1800.0095896720886\n",
      "269                                                 67c533489a498495525efee429340958-cliquecolouring_n15_k9_c8.sanitized.cnf                    TIMEOUT     1800.004343032837\n",
      "270                                                        25447f441df2f134223d3e43319c99ad-lec_mult_CvW_12x11.sanitized.cnf                    TIMEOUT     1800.003727197647\n",
      "271                                                         55199d55fa9c3751a525d6c0a0e649bd-WS_500_16_90_70.apx_2_DC-AD.cnf                    TIMEOUT     1800.004753112793\n",
      "272                                                        462d72f9b78ab1cd080667ff12a114ac-lec_mult_CvK_12x12.sanitized.cnf                    TIMEOUT    1800.0050191879272\n",
      "273                                                                     1548977b4e27032fe07cda357782cd6a-mdp-28-14-unsat.cnf                    TIMEOUT     1800.004992723465\n",
      "274                                                               8bf63d50ebdb645b75a24370b8f94d31-sokoban-p20.sas.cr.25.cnf                    TIMEOUT     1800.003642320633\n",
      "275                                                                     5411047bc24f4321f0e490e01e4a0910-goldcrest-and-9.cnf                    TIMEOUT    1800.0056953430176\n",
      "276                      7ac7fabd8c078aea420087a0c80e5563-circuit_32in32out_with_400gates_6in6out_dist64_seed1.sanitized.cnf                    TIMEOUT     1800.005405664444\n",
      "277                                                              b3c587501567db72e6e66c6930cf15ed-StConn_7_128.sanitized.cnf                    TIMEOUT    1800.0055329799652\n",
      "278                                             0e5ffe8651c6d9c3cccc3f6cb72be39a-g2-test_v5_r10_vr10_c1_s21502.smt2-cvc4.cnf                    TIMEOUT      1800.01304769516\n",
      "279                                                    1ea68d0008f01c50a1e4fec77dd0775e-REGRandom-K4-L1-Seed30.sanitized.cnf                    TIMEOUT    1800.0115728378296\n",
      "280                                                              11cc532ecddfb10a47e0a869b4867c1b-Break_triple_20_36.xml.cnf                    TIMEOUT    1800.0080802440643\n",
      "281                                                                       db15a85651e0d941a11bf8640625edd8-mdp-32-11-sat.cnf                    TIMEOUT     1800.004471063614\n",
      "282                                       da929c2aefde5e59878ad87c4323e581-pcmax-scheduling-m12-8049-55035-SAT.sanitized.cnf                    TIMEOUT    1800.0038414001465\n",
      "283                                           1e0da402100982e53001a64281149dd9-linked_list_swap_contents_safety_unwind63.cnf                    TIMEOUT    1800.1943845748901\n",
      "284                                                           7db30d12cb06f0dc2f30abff80d96d6a-two-trees-1023v.sanitized.cnf                    TIMEOUT    1800.0057654380798\n",
      "285                                                                     1165e12b6addad01b491c4616306186c-mulhs016-sc2009.cnf                    TIMEOUT    1800.0056216716766\n",
      "286                                                              07c50b89d97a55b6e1629999a445a2ba-ctl_4201_555_unsat_pre.cnf                    TIMEOUT    1800.0723984241486\n",
      "287                      71ec94c233016219e12d671594dc88e5-circuit_32in32out_with_70gates_7in7out_dist128_seed1.sanitized.cnf                    TIMEOUT     1800.004799604416\n",
      "288                                                                     0d3fcbb89bfb8e821058ba3ea4284de1-j3045_10_gmto_b.cnf                    TIMEOUT    1800.0054881572723\n",
      "289                                                 18c0eb461bda29214bd43b84199a3b61-cliquecolouring_n31_k5_c4.sanitized.cnf                    TIMEOUT    1800.0046091079712\n",
      "290                                                                       d11944fbca2dd6540f18bd05b6ccda0c-mdp-32-14-sat.cnf                    TIMEOUT    1800.0045738220215\n",
      "291                                                                 907e7dc703e16ad2c7e56a33fe3b3e5f-manol-pipe-g10bid_i.cnf                    TIMEOUT    1800.0044522285461\n",
      "292                                                      479b1413f8bd1d6fc0723856ffca9792-constraints_18_0.4_2.sanitized.cnf                    TIMEOUT    1800.0043494701385\n",
      "293                               0fa8623240b5c14c1308f863a6dc88ab-urqh1c5x5.shuffled-as.sat03-1468.cnf.mis-103.debugged.cnf                    TIMEOUT     1800.004417181015\n",
      "294                                                                bd12a2b66110451b050bd5d2943b1854-ITC2021_Early_10.xml.cnf                    TIMEOUT    1800.0045583248138\n",
      "295                                                                     43e492bccfd57029b758897b17d7f04f-pb_300_09_lb_07.cnf                    TIMEOUT    1800.0046164989471\n",
      "296                                                              cd33a862b81ba7b887845d3510c41c62-MASG0_72_keystream76_0.cnf                    TIMEOUT     1800.004114151001\n",
      "297                                                                     44c25de7963c45e92a2407ad839f6e8b-Break_20_72.xml.cnf                    TIMEOUT    1800.0049397945404\n",
      "298                                                           aa6e6f67719ca43aef922f2ebabc409a-1-ZC-1024-K-117.sanitized.cnf                    TIMEOUT     1800.032216310501\n",
      "299                                                                         6f0e236c999de0a5e3c46fd3463b1d0d-rook-56-1-1.cnf                    TIMEOUT    1800.0112943649292\n",
      "300                                                                  634a271f5fe339007a186539d615e92f-noL-11-4.sanitized.cnf                    TIMEOUT    1800.0053567886353\n",
      "301                                                      61069e0e3339af80000b774a041b7d96-constraints_18_0.3_2.sanitized.cnf                    TIMEOUT    1800.0043349266052\n",
      "302                                                        dd57f6ebf07a7f932a82bed1877788e6-lec_mult_KvW_10x10.sanitized.cnf                    TIMEOUT    1800.0038404464722\n",
      "303                                           6008b958ff6877f32c26b72728df4523-linked_list_swap_contents_safety_unwind76.cnf                    TIMEOUT     1800.215030670166\n",
      "304                                     5432e4dd44480fadbd66647db1750c80-pcmax-scheduling-m24-24102-255206-SAT.sanitized.cnf                    TIMEOUT    1800.0133407115936\n",
      "305                                                             29d86d9a3b85c349e10ad68d7a713da2-1-ZC-512-K-63.sanitized.cnf                    TIMEOUT    1800.0163893699646\n",
      "306                                                    02e4f23ed59566d343b605dc2cc30a01-REGRandom-K4-L3-Seed40.sanitized.cnf                    TIMEOUT    1800.0832915306091\n",
      "307                                                             e84b00b1f134e0e071a2baf54cfc90e8-ER_400_20_4.apx_2_DC-ST.cnf                    TIMEOUT    1800.0067157745361\n",
      "308                                                                         59825ed96dc3bbdbd8789a4870b323ec-hcp_CP18_18.cnf                    TIMEOUT     1800.013833284378\n",
      "309                                                             2b032f8a8976a302ad125eb50a3e8445-1-ZC-512-K-60.sanitized.cnf                    TIMEOUT    1800.0151450634003\n",
      "310                                                             4d7f6efbae05b66ffbbd509a430d57f8-stb_418_125.apx_1_DC-ST.cnf                    TIMEOUT     1800.005304813385\n",
      "311                     e2deb375e56da2360ecd08fc1179fb7d-circuit_48in24out_with_100gates_7in7out_dist128_seed1.sanitized.cnf                    TIMEOUT    1800.0351660251617\n",
      "312                                                             f15ad4206d95a3f8f1358b33301539be-ER_400_20_7.apx_1_DC-AD.cnf                    TIMEOUT    1800.0067210197449\n",
      "313                                                                       a9cb77454c6cfdf4092fb304d3aae8b7-mdp-32-16-sat.cnf                    TIMEOUT    1800.0060226917267\n",
      "314                                           878bcc11e1e243680f7946bcf428f465-linked_list_swap_contents_safety_unwind45.cnf                    TIMEOUT    1802.0185735225677\n",
      "315                                                                                 26254890fa7107f85242ec9190da2a7a-002.cnf                    TIMEOUT    1800.0064826011658\n",
      "316  83dc8f675657b3e6d148ef5cf897fb82-openstacks-sequencedstrips-nonadl-nonnegated-os-sequencedstrips-p30_3.025-NOTKNOWN.cnf                    TIMEOUT    1800.0101170539856\n",
      "317                                                               94dd280b1562ee7dae44b303b8fed233-Break_unsat_18_31.xml.cnf                    TIMEOUT    1800.0043907165527\n",
      "318                                                       0bad2ce307bf5b68db26fa34e252c9d4-af-synthesis_stb_50_100_4_sat.cnf                    TIMEOUT    1800.0040547847748\n",
      "319                                                             6c52aca5b5fd284bf1940b84df90a367-1-TC-256-K-71.sanitized.cnf                    TIMEOUT    1800.0038049221039\n",
      "320                                                                              6077d4e5f755124ddc84068d6ccb630f-Nb44T6.cnf                    TIMEOUT    1800.0362203121185\n",
      "321                                                                     cc30e4eedbcf9c3bc110894d08781246-j3037_9_mdd_bm1.cnf                    TIMEOUT    1800.0045948028564\n",
      "322                                                              9d9c4fa425282759eb9e98b82fb5f56e-x9-12087.sat.sanitized.cnf                    TIMEOUT    1800.0049214363098\n",
      "323                                           c03f7f84f6ac9e523c17e391c3895183-linked_list_swap_contents_safety_unwind68.cnf                    TIMEOUT    1800.1523356437683\n",
      "324                                                            1e3c3e8d349759b8b482a6f2721762c4-apn-sbox5-cut3-symmbreak.cnf                    TIMEOUT    1800.0040345191956\n",
      "325                                                                        f561c52b987bdb9031a477864070b759-Ptn-7824-b18.cnf                    TIMEOUT    1800.0041253566742\n",
      "326                                                 ac347c21ca0759079c0be9a758e4e924-cliquecolouring_n41_k5_c4.sanitized.cnf                    TIMEOUT    1800.0044434070587\n",
      "327                                     32585e66749f42e161e94fc49fd64750-pcmax-scheduling-m26-6398-62377-UNSAT.sanitized.cnf                    TIMEOUT     1800.004954814911\n",
      "328                                                        69e34d0f49c71a7e145d8104ebe0273f-sokoban-p16.sas.ex.15-sc2016.cnf                    TIMEOUT    1800.1104607582092\n",
      "329                                                       0876c518e5653369e20fb1ee0bb8db40-mp1-klieber2017s-0500-023-t12.cnf                    TIMEOUT    1800.0055129528046\n",
      "330                      37ca184832fc6fa43a22ae900f1756a2-circuit_32in32out_with_350gates_6in6out_dist64_seed1.sanitized.cnf                    TIMEOUT    1800.0136387348175\n",
      "331                                                              c0e6e6eeebd48ca600cfc7d662fa804c-x9-10093.sat.sanitized.cnf                    TIMEOUT     1800.005212545395\n",
      "332                      8942dca5dc0876fc3f723f738d72de1c-circuit_32in32out_with_64gates_8in6out_dist128_seed2.sanitized.cnf                    TIMEOUT     1800.011203289032\n",
      "333                                                                 d7627176604d5b86f34d2f5d14d8ee99-worker_40_80_40_0.9.cnf                    TIMEOUT    1800.0187737941742\n",
      "334                                                     1b0f4ff7984b8d4cf2873200fb1680fb-af-synthesis_stb_50_140_1_unsat.cnf                    TIMEOUT    1800.0047974586487\n",
      "335                                     31e788a12ddc8b43ec20e77d53abaa23-pcmax-scheduling-m40-26287-324155-SAT.sanitized.cnf                    TIMEOUT    1800.0368802547455\n",
      "336                                                                       a5dc6226e4c0bebb06926efe55640995-mdp-36-10-sat.cnf                    TIMEOUT    1800.0098848342896\n",
      "337                                                                      576ebc1333f2c466c2dec98792721e1f-j3037_9_rggt_b.cnf                    TIMEOUT    1800.0045711994171\n",
      "338                                                             a917b02a7c31e74c8ccf8b8f001abf37-1-ZC-512-K-67.sanitized.cnf                    TIMEOUT    1800.0113060474396\n",
      "339                                     207b28c3cefc55b143259222cbfd4962-pcmax-scheduling-m37-28831-324346-SAT.sanitized.cnf                    TIMEOUT    1800.0069682598114\n",
      "340                                                             a6ed647f85f20be4aedd1e9ce31cbdcd-1-ET-256-K-55.sanitized.cnf                    TIMEOUT    1800.0061740875244\n",
      "341                     adf6dacdd64c93f9de1aa0eadf427faa-circuit_48in64out_with_800gates_4in4out_dist128_seed1.sanitized.cnf                    TIMEOUT    1800.0051696300507\n",
      "342                                                                            897acc1858ce0887f286aba5c0d56d71-Nb13T165.cnf                    TIMEOUT    1800.0499694347382\n",
      "343                                                             4be4ae25aae88528bc10f8369bba86df-ER_400_20_4.apx_1_DC-AD.cnf                    TIMEOUT    1800.0042383670807\n",
      "344                                                             7dc0e1b5fedbd8351907f1316b58e68f-1-ZC-512-K-65.sanitized.cnf                    TIMEOUT    1800.0044286251068\n",
      "345                                                             67b4844c4bfbaf961cbbd79b953aa5c2-stb_588_138.apx_1_DC-AD.cnf                    TIMEOUT    1800.0075943470001\n",
      "346                      9e6c2e7b0d6f58e449716deb9305525a-circuit_32in32out_with_96gates_7in7out_dist128_seed1.sanitized.cnf                    TIMEOUT    1800.0049240589142\n",
      "347                                                      761651d4b6454108403bdefe22720fb3-constraints_18_0.5_2.sanitized.cnf                    TIMEOUT      1800.00874209404\n",
      "348                                                     c82eee3badbc8432dad72d1d575a0ea6-preimage_80r_495m_160h_seed_379.cnf                    TIMEOUT    1800.0041344165802\n",
      "349                                                           c69c5163c9087ffe770743bd4cdafcc9-tseitin_d3_n162.sanitized.cnf                    TIMEOUT    1800.0063939094543\n",
      "350                                         39fba35826ce8c87cd8e8de1969b2dd2-SGI_30_80_26_70_4-log.shuffled-as.sat03-208.cnf                    TIMEOUT    1800.0044858455658\n",
      "351                                                        4c54efa61599e33d514bf718ff23ad09-lec_mult_CvK_12x11.sanitized.cnf                    TIMEOUT    1800.0033872127533\n",
      "352                                                               dcf5b8224d1e0748871c83ee10067255-2dlx_ca_bp_f_liveness.cnf                    TIMEOUT    1800.0774521827698\n",
      "353                                                        778e0fd8f034b7cfd539b1b523ed4c4a-lec_mult_CvW_12x12.sanitized.cnf                    TIMEOUT     1800.004276752472\n",
      "354                                                                          9777418f75e1dfbd7d34e9b0ba4bf0b5-grs-64-128.cnf                    TIMEOUT    1800.1061265468597\n",
      "355                                                                            e0bf26a7527cd162c7d3016d4b2ab9fb-md5_48_1.cnf                    TIMEOUT    1800.0057022571564\n",
      "356                                                         7fca438c38e5b7a70e38778ec146cfe0-ctl_4291_567_2_unsat-sc2013.cnf                    TIMEOUT    1800.0052013397217\n",
      "357                                           573cc82ec61e01fd6e6308493ee56289-linked_list_swap_contents_safety_unwind80.cnf                    TIMEOUT    1800.2287056446075\n",
      "358                                                               b3f53d4c67b234d27e21f5e56bfd39e8-StConn_8_32.sanitized.cnf                    TIMEOUT     1800.005299091339\n",
      "359                                                              697c96ac45534726c7dbd96faa11a86a-x9-11094.sat.sanitized.cnf                    TIMEOUT    1800.0045564174652\n",
      "360                                      7b48f9354d25521bd542d859c0ecdde9-hwmcc17miters-xits-iso-oski15a08b00s.sanitized.cnf                    TIMEOUT    1800.0248413085938\n",
      "361                                                       d40a68825bdbcdd7642b249325a7b6a2-Folkman-180-5383714.sanitized.cnf                    TIMEOUT    1800.0060985088348\n",
      "362                                                        bb591c6447fb6b4180f6d288eb2ff62a-lec_mult_DvW_11x10.sanitized.cnf                    TIMEOUT       1800.0047955513\n",
      "363                                                               b087c195e5b7bb191d3a701aa0aa8cf1-Break_unsat_14_23.xml.cnf                    TIMEOUT    1800.0046854019165\n",
      "364                                                     f16b7329ef5728f722c291156b3b098e-af-synthesis_stb_50_100_9_unsat.cnf                    TIMEOUT    1800.0044021606445\n",
      "365                                                                  0a27eb7c16c1e69ff4d087d217ac89cb-noL-11-0.sanitized.cnf                    TIMEOUT    1800.0064170360565\n",
      "366                                                                          c0293283432a9b15cd743702163f5184-mp1-Nb7T42.cnf                    TIMEOUT    1800.0065824985504\n",
      "367                                           b8f5fc2425facb3aec52a665f32a500c-asconhashv12_opt64_H6_M2-PgbpwX_m0_4_U1.c.cnf                    TIMEOUT    1800.0077352523804\n",
      "368                                         6f4df706246d29bf38adadd274fadab3-SGI_30_60_19_60_6-dir.shuffled-as.sat03-112.cnf                    TIMEOUT    1800.0045652389526\n",
      "369                                                             d5f6b2092795aad67d1df1fd3a329552-ER_400_20_7.apx_2_DC-ST.cnf                    TIMEOUT    1800.0064043998718\n",
      "370                                                                     f7e855b18105170b0718086d3f5b5923-j3045_10_rggt_b.cnf                    TIMEOUT    1800.0048730373383\n",
      "371                                                                    a14d9ee17051ec08a4334ba43089502c-64_200.sanitized.cnf                    TIMEOUT    1800.4685900211334\n",
      "372                                                        bdcdb52d16f87ea20f091ca4b0b6a36f-lec_mult_CvK_11x10.sanitized.cnf                    TIMEOUT    1800.0042386054993\n",
      "373                                                                    c214f1eb2576af00e3fb09f7d0305764-64_150.sanitized.cnf                    TIMEOUT    1800.3779528141022\n",
      "374                     fa50d69adc71fe4daf7f9088cd02864a-circuit_32in64out_with_150gates_6in6out_dist256_seed1.sanitized.cnf                    TIMEOUT    1800.0165367126465\n",
      "375                                           aaa3654cc92d80da84ecf5d18040c634-linked_list_swap_contents_safety_unwind62.cnf                    TIMEOUT    1800.1751267910004\n",
      "376                                                              09b61bbf19748094a7d896aac314ab36-x9-12063.sat.sanitized.cnf                    TIMEOUT     1800.004239320755\n",
      "377                                                   0ef029cb7616d4a89eb6d1f63e89f67e-or_randxor_k3_n510_m510.sanitized.cnf                    TIMEOUT    1800.0043931007385\n",
      "378                                                           556cf803f9de4b2ec688825b5ff7e325-tseitin_d3_n174.sanitized.cnf                    TIMEOUT      1800.13290476799\n",
      "379                                                              54c2da6d387a6f5ad6e014ae4d4decfc-x9-10038.sat.sanitized.cnf                    TIMEOUT    1800.0038795471191\n",
      "380                                                                    65c5a5d228ec5e52d1a72f918086f584-32_100.sanitized.cnf                    TIMEOUT    1800.1359343528748\n",
      "381                                                      30f0db845937bbda3ffde60e5ed4cb3f-Folkman-190-66337703.sanitized.cnf                    TIMEOUT    1800.0046226978302\n",
      "382                                                        be18894105e006399cc018abc14b204f-af-synthesis_stb_50_40_9_sat.cnf                    TIMEOUT    1800.0039417743683\n",
      "383                                                              09c1b79b1cfe3522364fe60aef780703-x9-12092.sat.sanitized.cnf                    TIMEOUT    1800.0058319568634\n",
      "384                                                 096e485489025895cdc59887baafa08b-test_v7_r17_vr5_c1_s25451.smt2-cvc4.cnf                    TIMEOUT    1800.0076513290405\n",
      "385                                                                         2115182958f6cfb5a172a24f989b86dd-rook-42-0-1.cnf                    TIMEOUT    1800.0062725543976\n",
      "386                                                              b44ea915362c3a140269003d45b1d053-x9-11034.sat.sanitized.cnf                    TIMEOUT    1800.0061156749725\n",
      "387                                                        a5fc113e7d4899f4e4af14b87b6fd6ae-Kakuro-easy-097-ext.xml.hg_4.cnf                    TIMEOUT     1800.109860420227\n",
      "388                                     9e85aad5707405fb349c6c5548040cf4-pcmax-scheduling-m35-32274-371389-SAT.sanitized.cnf                    TIMEOUT     1800.004306793213\n",
      "389                                                                    4106867bc76b8794330a205cf8a303ad-bvsub_19952.smt2.cnf                    TIMEOUT    1800.0082483291626\n",
      "390                                           c8b129b9e4835b8a40d5ef0d27a45bef-linked_list_swap_contents_safety_unwind78.cnf                    TIMEOUT    1800.2295107841492\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_results(file_path,start):\n",
    "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(start):\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    file_name, result, time_limit = match.groups()\n",
    "                    data.append((file_name, result, time_limit))\n",
    "\n",
    "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(start):\n",
    "                match = pattern.search(line)\n",
    "                if match:\n",
    "                    file_name, time_limit = match.groups()\n",
    "                    data.append((file_name, \"TIMEOUT\", time_limit)) \n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
    "    return df\n",
    "\n",
    "gbc_df = extract_results(\"slurm-29516543.out\", \"1.\")\n",
    "\n",
    "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
    "\n",
    "print(gbc_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   file_name                    result             time_limit\n",
      "0                                                                           ef330d1b144055436a2d576601191ea5-crn_11_99_u.cnf                     UNSAT      7.376698970794678\n",
      "1                                                                edceb8782e72e290fa54757dbfdd0173-x9-09057.sat.sanitized.cnf                     UNSAT     375.36069917678833\n",
      "2                                                    70ef2b6bdc4101a0e35cf3d165571fe3-qwh.50.1250.shuffled-as.sat03-1655.cnf                       SAT      224.8674235343933\n",
      "3                                                                  c5ae0ec49de0959cd14431ce851c14f8-Circuit_multiplier22.cnf                       SAT      707.7964084148407\n",
      "4                                                                d195412a62cdcbb851136f60af76f463-x9-09098.sat.sanitized.cnf                     UNSAT       620.841639995575\n",
      "5                                                                 08ccc34df5d8eb9e9d45278af3dc093d-simon-r16-1.sanitized.cnf                       SAT    0.03329801559448242\n",
      "6                                                                       22b4cf412c811872d6ba5078106aeb6c-j3037_10_rggt_b.cnf                       SAT      332.8107979297638\n",
      "7                                                                   b8c7c777e76995e5bf7b517f2db234ba-noL-11-16.sanitized.cnf                       SAT      524.2708959579468\n",
      "8                                                                      88e89266e58f1f125ebc4e0f66e2c060-32_200.sanitized.cnf                       SAT      770.8946743011475\n",
      "9                                                               a58b8a5eb61d3110d9fa36a03de588d2-stb_495_168.apx_1_DC-AD.cnf                     UNSAT     1381.6919436454773\n",
      "10                                                               1afa2d7a3d817c3149da432eece66da8-worker_550_550_550_0.3.cnf                       SAT     156.67394828796387\n",
      "11                                                                7f7109dce621ef361a72b3e8cee9a962-Break_unsat_06_07.xml.cnf                     UNSAT     6.9258811473846436\n",
      "12                                              1009c791cee542cdf19651fe25e6881a-summle_X4053_steps8_I1-2-2-4-4-8-25-100.cnf                       SAT     1576.8737621307373\n",
      "13                                                                                                       track_main_2024.uri  a non-zero exit 1 status   0.006631612777709961\n",
      "14                                                               ad4e151d80c7012d88dd79bcfceaade5-x9-08075.sat.sanitized.cnf                     UNSAT      182.2089774608612\n",
      "15                                                                     210fe1c0ffefde5bcd03d59b1efa6984-32_325.sanitized.cnf                       SAT       1255.48628282547\n",
      "16                                                        72c0d81e16d91bcaed808efcde2e5069-Folkman-175-1251868.sanitized.cnf                       SAT       1276.52694106102\n",
      "17                                                               e08f11f0a3bd266ee5c78ce332de107f-x9-10096.sat.sanitized.cnf                     UNSAT       1640.28568649292\n",
      "18                                                                 5ea72bcdccc86bd1a924029f7b81aec5-atco_enc1_opt1_10_15.cnf                       SAT     1573.2466278076172\n",
      "19                                                               13ae2628d8e113db1786dba41a65fe38-x9-10027.sat.sanitized.cnf                       SAT     1455.0293724536896\n",
      "20                                                                cdce6277b01ae06ddb95468c5f05de71-simon-r17-0.sanitized.cnf                       SAT    0.06718182563781738\n",
      "21                                                                      3988a60c6e93167763c6fd2a347d5859-Break_08_24.xml.cnf                       SAT     1.0175721645355225\n",
      "22                                                                2b043efb4bde6d83f7c95a8e8e2d7bf8-simon-r21-0.sanitized.cnf                       SAT    0.06553506851196289\n",
      "23                                                       f8443057c188f6c9cfb711ff3d4aa6ff-constraints_17_0.5_2.sanitized.cnf                     UNSAT       275.585209608078\n",
      "24                                                         a38affaa741c958fc32769d5fe89b06c-frb65-12-2.used-as.sat04-874.cnf                       SAT      261.4643633365631\n",
      "25                      303480ca7e8322d771c94caf4ebd4e95-circuit_48in64out_with_700gates_4in4out_dist128_seed1.sanitized.cnf                       SAT      210.7985589504242\n",
      "26                                                                                                                  unzip.py  a non-zero exit 1 status  0.0052127838134765625\n",
      "27                                                                     0cace8a29a1d6b225a8da561d35e8f5a-lru_10.sanitized.cnf                       SAT      331.8655457496643\n",
      "28                                                                        04ded94454830d4ea960327e8b91f5a3-mdp-28-14-sat.cnf                       SAT      698.9475491046906\n",
      "29                                                                      c8e64404361f2426490d39459832c66a-64_25.sanitized.cnf                       SAT     155.52382159233093\n",
      "30                      170b13af977e962321c493544b2bd0a9-circuit_48in64out_with_800gates_4in4out_dist128_seed4.sanitized.cnf                       SAT      316.0367782115936\n",
      "31                                                                  ca14adcb9296a7b31d7815c2ed16d0f1-ITC2021_Early_3.xml.cnf                       SAT     2.3688273429870605\n",
      "32                                                       6add7e416c1126607afeb2666af330ac-constraints_16_0.5_1.sanitized.cnf                     UNSAT     507.50465273857117\n",
      "33                                                                 7083b70c1976162e2693d7a493717ffd-battleship-14-26-sat.cnf                       SAT      320.4366466999054\n",
      "34                                                                     f50eaf02a8041510b64104998cc81d2f-sted5_0x24204-50.cnf                       SAT      415.2292757034302\n",
      "35                                                               19e2c3a0865c8c1b4543d11213bebe5f-x9-09024.sat.sanitized.cnf                     UNSAT     237.43327927589417\n",
      "36                                                               3a75ad246dbc750a7391ad887c5b0835-x9-11093.sat.sanitized.cnf                       SAT     1246.2681241035461\n",
      "37                                                                  50019e4419d48196bb4b95933a8b5030-noL-11-14.sanitized.cnf                       SAT      1144.640493631363\n",
      "38                                                                            ec84eecb124c63d4757e083dd0e5a9ff-mchess_15.cnf                     UNSAT     1014.3680686950684\n",
      "39                                                               915a25bd189357e4c6d7771b69a6849f-x9-09004.sat.sanitized.cnf                     UNSAT      842.1606323719025\n",
      "40                                                                       e481a97a93fd6b4303f39024d19c2867-j3037_1_gmto_b.cnf                       SAT     373.71688866615295\n",
      "41                                                        71a3dd6156463c0c55c4b38394faa753-af-synthesis_stb_50_120_4_sat.cnf                       SAT     1451.3811123371124\n",
      "42                       70bfbd054dc9ffd394fab32845b492d3-circuit_32in32out_with_100gates_7in7out_dist64_seed2.sanitized.cnf                       SAT      926.4122495651245\n",
      "43                                                                5ee7de2bd112aa39485e79c9d487bf8f-simon-r23-1.sanitized.cnf                       SAT    0.06699275970458984\n",
      "44                                                               c5a98231dd54cbca06135293bb7e1985-x9-11053.sat.sanitized.cnf                       SAT       598.113231420517\n",
      "45                                                               7b9d8b9b31b530effd634f5a8f1f4411-stb_531_83.apx_2_DC-ST.cnf                     UNSAT     1470.9252364635468\n",
      "46                       396fd56f3fd7b85afbba4254ea6e746c-circuit_32in32out_with_80gates_7in7out_dist128_seed1.sanitized.cnf                       SAT     1363.5233099460602\n",
      "47                                                                            8704094951693f99fd21403a039c8131-mchess_16.cnf                     UNSAT     1424.6406893730164\n",
      "48                                                                     26b648475cfd06695a17ac95f4469744-128_75.sanitized.cnf                       SAT     1471.3590421676636\n",
      "49                       6f7a0e1cf94b6b26eafc08a827a692ce-circuit_64in64out_with_64gates_8in5out_dist256_seed1.sanitized.cnf                       SAT     376.67070746421814\n",
      "50                                                               475803537529d9d14a034957f48a6d38-x9-09076.sat.sanitized.cnf                     UNSAT      335.3701696395874\n",
      "51                                                            e6cdc2687fa53506021f05b60ad0c6a2-GracefulGraph-K05-P02_c18.cnf                       SAT       42.9225959777832\n",
      "52                                                               f376d4c191518ed704326960b6b19a4b-x9-10084.sat.sanitized.cnf                     UNSAT       1568.57333445549\n",
      "53                                                              2eb039a5aa6bfb6e41d3afd41071ca55-ER_500_30_3.apx_1_DC-ST.cnf                     UNSAT     1236.5666477680206\n",
      "54                                                                                  ecca77b5350eca6a4323edd5b38208c6-004.cnf                       SAT      485.8935627937317\n",
      "55                                             dc7817dfa2817916b266c1cfacd2ee66-constraints_25_4_5_12_12_0_0_0.sanitized.cnf                     UNSAT     1271.2375054359436\n",
      "56                                       bd8bc25be2b36c64b38459c17e815814-pcmax-scheduling-m11-1517-6802-UNSAT.sanitized.cnf                     UNSAT      554.9501452445984\n",
      "57                    9276ce38c625b2d00de247f8588f1542-combined-crypto1-wff-seed-102-wffvars-500-cryptocplx-31-overlap-2.cnf                       SAT     1438.4343104362488\n",
      "58                                                                  0d81711a3d73c828e8c6e12607eda82d-noL-11-20.sanitized.cnf                       SAT      663.4637122154236\n",
      "59                                                                     e371cd7cdde3e7ccb9834290fd4a92d0-32_350.sanitized.cnf                       SAT      1356.385588169098\n",
      "60                                                                  de7a6b03999e2bc5bd750831c2662a4d-noL-11-18.sanitized.cnf                       SAT      1048.843889951706\n",
      "61                                                                4073757aae06fc2b50c043f088b132b4-simon-r19-1.sanitized.cnf                       SAT    0.06565594673156738\n",
      "62                                             f054205a7cef98e5021016f864c69816-summle_X11112_steps6_I1-2-2-4-4-8-25-100.cnf                       SAT      860.8580038547516\n",
      "63                                                         3ed56242f55e3653dbceeb4a70221787-rbsat-v945c61409gyes9-sc2009.cnf                       SAT     1351.6575939655304\n",
      "64                                      0ac6aaf6db6a0e4ec3a06e865d01086f-pcmax-scheduling-m13-2011-12813-UNSAT.sanitized.cnf                     UNSAT      879.1315953731537\n",
      "65                                                                2fcd8533eba981967292f1b6e41f7433-simon-r20-0.sanitized.cnf                       SAT    0.06647515296936035\n",
      "66                                                                d3893e43819a907055a84e48a6ee97ba-g2-ak128boothbg2msaig.cnf                       SAT      4.872403621673584\n",
      "67                                                       8e720686372c5037f30b4fc7b1c71d48-constraints_17_0.4_1.sanitized.cnf                       SAT     31.105908393859863\n",
      "68                                                                     07e6413459f92b613498a719125b6239-j3037_10_mdd_bm1.cnf                     UNSAT       452.024973154068\n",
      "69                                                               bbfed8974655bca520259deb10d2347b-x9-09054.sat.sanitized.cnf                       SAT      68.65973567962646\n",
      "70                                                               f296fe701a562022c0de0cf565fbca7d-x9-08014.sat.sanitized.cnf                     UNSAT     227.41920399665833\n",
      "71                                                       fb0e505b8bd19a34f1f80b8e020e7856-constraints_17_0.4_2.sanitized.cnf                     UNSAT      247.4937083721161\n",
      "72                                                               eddd68e14d69cce7190b99f4e7abdafb-x9-10098.sat.sanitized.cnf                       SAT      37.91703271865845\n",
      "73                                                               57f4ea7ab160d996e38e69fac59869c4-x9-09047.sat.sanitized.cnf                     UNSAT      395.7560987472534\n",
      "74                                                   2a53e9c6a25a50d753a94ead66065826-mp1-ps_5000_21250_3_0_0.8_0_1.50_6.cnf                     UNSAT      335.8668301105499\n",
      "75                                                                e99e266b422513a2898c13898a1de501-rbsat-v760c43649gyes9.cnf                       SAT      548.6508481502533\n",
      "76                                                       b4b41b2ff14427e5715cb9bee06d4602-constraints_17_0.3_2.sanitized.cnf                     UNSAT      217.8538875579834\n",
      "77                       7ac7fabd8c078aea420087a0c80e5563-circuit_32in32out_with_400gates_6in6out_dist64_seed1.sanitized.cnf                       SAT     1597.1843464374542\n",
      "78                                                     0fa9521ff633b27be11525a7b0f7d8b6-jgiraldezlevy.2200.9086.08.40.41.cnf                       SAT     1623.3516631126404\n",
      "79                                                                      0d3fcbb89bfb8e821058ba3ea4284de1-j3045_10_gmto_b.cnf                       SAT      934.3283338546753\n",
      "80                                                               ded23680dfeab2879c05bc0e4de21126-x9-09014.sat.sanitized.cnf                     UNSAT      511.9489929676056\n",
      "81                                                     8b31606e10656ff7eb2936262b647443-stable-300-0.1-20-98765432130020.cnf                       SAT     1331.5935943126678\n",
      "82                                                                           04e219c640ed59dc68ea2d50493de5b5-mp1-Nb5T15.cnf                       SAT     123.63361144065857\n",
      "83                           1427381a809c64c721838894ece6756d-shuffling-2-s25242449-of-bench-sat04-727.used-as.sat04-753.cnf                       SAT      34.26231241226196\n",
      "84                                                       8530c911b75ab1b276042043d118a875-constraints_16_0.4_1.sanitized.cnf                     UNSAT      419.1235044002533\n",
      "85                                            878bcc11e1e243680f7946bcf428f465-linked_list_swap_contents_safety_unwind45.cnf                     UNSAT     469.18995666503906\n",
      "86                                                                      812926407774771b3bd9885f7bfa4841-lru_9.sanitized.cnf                       SAT      299.7694089412689\n",
      "87                      e2deb375e56da2360ecd08fc1179fb7d-circuit_48in24out_with_100gates_7in7out_dist128_seed1.sanitized.cnf                       SAT       648.273307800293\n",
      "88                                                                      e85fb114c33f450dc2622c78bc6fa019-lru_7.sanitized.cnf                       SAT     230.57322311401367\n",
      "89                                                               c6568fc8805127e876c4c23551bf49fa-x9-10076.sat.sanitized.cnf                     UNSAT     1775.3398451805115\n",
      "90                                                          aacfb8797097f698d14337d3a04f3065-barman-pfile06-022.sas.ex.7.cnf                     UNSAT      6.674400329589844\n",
      "91                                                                3129198788f182ce6955b18aa3c7e61e-simon-r24-1.sanitized.cnf                       SAT    0.06597065925598145\n",
      "92                                              3bf8ba6bb4e4ea9ad08b1b058661ba2e-summle_X4044_steps7_I1-2-2-4-4-8-25-100.cnf                       SAT     1043.1441295146942\n",
      "93                                                                         f561c52b987bdb9031a477864070b759-Ptn-7824-b18.cnf                       SAT     368.95574855804443\n",
      "94                                                       61069e0e3339af80000b774a041b7d96-constraints_18_0.3_2.sanitized.cnf                     UNSAT     1505.7670259475708\n",
      "95                                                               8b18bb75459a4161633ba2a3c8ee183e-x9-11062.sat.sanitized.cnf                       SAT     1765.6675164699554\n",
      "96                                            c03f7f84f6ac9e523c17e391c3895183-linked_list_swap_contents_safety_unwind68.cnf                     UNSAT      672.9688153266907\n",
      "97                                                                       be6411f4784a3c879886dda807cdc607-j3037_10_mdd_b.cnf                       SAT      795.9820137023926\n",
      "98                                                              e84b00b1f134e0e071a2baf54cfc90e8-ER_400_20_4.apx_2_DC-ST.cnf                     UNSAT     1583.1410853862762\n",
      "99                                                                 c801a020a6c8bc3c287fea495203b114-worker_20_40_20_0.95.cnf                       SAT        2.1688232421875\n",
      "100                                                              5cb1ca8fe8a9d9125ea9accd498445f1-stb_531_83.apx_1_DC-ST.cnf                     UNSAT     1716.1208493709564\n",
      "101                                                              f45e5faf1bcccbdd3065dd6367c3bd16-x9-10083.sat.sanitized.cnf                     UNSAT     1713.8383798599243\n",
      "102                                                               7e1d279559b202016e5797901e731a39-simon-r25-0.sanitized.cnf                       SAT     0.0687403678894043\n",
      "103                                                               089f909e37b3ef0c4d90687f7e22b68f-simon-r18-0.sanitized.cnf                       SAT    0.06573867797851562\n",
      "104                                                                           ddc0720fa5a91d9cc0dc726644ab9e9f-6s167-opt.cnf                     UNSAT      233.6772174835205\n",
      "105                                                             6c52aca5b5fd284bf1940b84df90a367-1-TC-256-K-71.sanitized.cnf                     UNSAT      1683.166949748993\n",
      "106                     f86dad4ba35369eb720a0c9ddc45037a-combined-crypto1-wff-seed-1-wffvars-450-cryptocplx-40-overlap-2.cnf                       SAT       131.290922164917\n",
      "107                                                      9cd3acdb765c15163bc239ae3a57f880-FmlaEquivChain_4_6_6.sanitized.cnf                     UNSAT     1688.1374084949493\n",
      "108                                                       0876c518e5653369e20fb1ee0bb8db40-mp1-klieber2017s-0500-023-t12.cnf                       SAT      959.7066051959991\n",
      "109                                                                     aceab5a3452e2901e645065bda3e8847-lru_8.sanitized.cnf                       SAT      262.7683570384979\n",
      "110                                                                      02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12.cnf                     UNSAT      41.82173252105713\n",
      "111                                                                      576ebc1333f2c466c2dec98792721e1f-j3037_9_rggt_b.cnf                       SAT      487.0581126213074\n",
      "112                                                                     282a02a1743eb47c6b340e52ecce40a2-lru_6.sanitized.cnf                       SAT     197.57694101333618\n",
      "113                                                               0cfe9c90d3a51435a5e4dba7634b882f-g2-ak128boothbg2msisc.cnf                       SAT      3.520162343978882\n",
      "114                      37ca184832fc6fa43a22ae900f1756a2-circuit_32in32out_with_350gates_6in6out_dist64_seed1.sanitized.cnf                       SAT     1092.5742709636688\n",
      "115                                                                    af750c18578d52e60472315692ad83c0-si2-b03m-m800-03.cnf                       SAT      80.06874203681946\n",
      "116                                                               7cbc3ce2052ba7c5b501f75af58ab3c4-simon-r22-1.sanitized.cnf                       SAT    0.06535625457763672\n",
      "117                     adf6dacdd64c93f9de1aa0eadf427faa-circuit_48in64out_with_800gates_4in4out_dist128_seed1.sanitized.cnf                       SAT     358.69352293014526\n",
      "118                                                              7fb202a51c0223f3119887a57086ca4d-x9-09051.sat.sanitized.cnf                     UNSAT      529.2611603736877\n",
      "119                                              ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf                       SAT    0.31643152236938477\n",
      "120                                                              a45a0358685867bd4f1c7f7c0b0e379c-x9-10014.sat.sanitized.cnf                       SAT     431.34886264801025\n",
      "121                                     7f1c7da531539f3be4a1e0e916913086-pcmax-scheduling-m19-2974-16501-UNSAT.sanitized.cnf                     UNSAT       712.863198518753\n",
      "122                                                             05ed64e4e6229f446082752936768489-stb_495_168.apx_2_DC-AD.cnf                     UNSAT     1466.4253423213959\n",
      "123                                                      fb51311320bb42bdb893249998a77f40-constraints_16_0.3_1.sanitized.cnf                     UNSAT      821.0706450939178\n",
      "124                                                                     f7e855b18105170b0718086d3f5b5923-j3045_10_rggt_b.cnf                       SAT      376.9145140647888\n",
      "125                                                      761651d4b6454108403bdefe22720fb3-constraints_18_0.5_2.sanitized.cnf                     UNSAT     1722.9544517993927\n",
      "126                                                                    65c5a5d228ec5e52d1a72f918086f584-32_100.sanitized.cnf                       SAT      376.9617793560028\n",
      "127                                           aaa3654cc92d80da84ecf5d18040c634-linked_list_swap_contents_safety_unwind62.cnf                     UNSAT      623.0283586978912\n",
      "128                                                                    c214f1eb2576af00e3fb09f7d0305764-64_150.sanitized.cnf                       SAT      938.1148698329926\n",
      "129                                                             bd19da88800086971e554e9d66bc641d-stb_588_138.apx_1_DC-ST.cnf                     UNSAT     1314.1917536258698\n",
      "130                                                                          c0293283432a9b15cd743702163f5184-mp1-Nb7T42.cnf                       SAT     1040.5262396335602\n",
      "131                                                              b44ea915362c3a140269003d45b1d053-x9-11034.sat.sanitized.cnf                       SAT      200.6296353340149\n",
      "132                                                                    a14d9ee17051ec08a4334ba43089502c-64_200.sanitized.cnf                       SAT     1261.3492050170898\n",
      "133                                                              54c2da6d387a6f5ad6e014ae4d4decfc-x9-10038.sat.sanitized.cnf                       SAT      1277.369847536087\n",
      "134                                                             46d75199933980fdb856f13e5b1817dd-1-TC-256-K-64.sanitized.cnf                   TIMEOUT      1800.004067659378\n",
      "135                                                      63732c330adb8de8dd47ce1693b86d0e-FmlaEquivChain_4_8_8.sanitized.cnf                   TIMEOUT     1800.0052812099457\n",
      "136                                                                       83b330c934d6dd35d56e1b1ca3638b3c-j3037_1_mdd_b.cnf                   TIMEOUT     1800.0056850910187\n",
      "137                                                              14e4cfcf0d83b2185fad41684d00d4dc-x9-12035.sat.sanitized.cnf                   TIMEOUT     1800.0044691562653\n",
      "138                                                             dd7164f2592c9675618c5c99af9b70f3-1-ZC-512-K-64.sanitized.cnf                   TIMEOUT     1800.0047583580017\n",
      "139                                                  543e67dd5abc272c37775b1b742a1d9a-qwh.60.1728.shuffled-as.sat03-1659.cnf                   TIMEOUT      1800.115089893341\n",
      "140                                                           35789168b67a74985804902008251269-tseitingrid6x200_shuffled.cnf                   TIMEOUT     1800.0079407691956\n",
      "141                                                              b7273af3d468ea2595f11a6dbd6ef6ce-x9-10007.sat.sanitized.cnf                   TIMEOUT     1800.0075409412384\n",
      "142                                                              3c6e1d1c4b8d3d08aa4c1df3805f4f7d-x9-10031.sat.sanitized.cnf                   TIMEOUT     1800.0090794563293\n",
      "143                                                      8d2dfc50c1759dc11a6564d5c368c6df-FmlaImplyChain_3_7_8.sanitized.cnf                   TIMEOUT     1800.0109267234802\n",
      "144                                                                  47e1ada0070708c2953d322c06aea00d-noL-11-8.sanitized.cnf                   TIMEOUT      1800.012532711029\n",
      "145                                                                      a08e66296d00f480e9ccadd79fa8b904-j3045_4_gmto_b.cnf                   TIMEOUT     1800.0101253986359\n",
      "146                                                       2d6d26e1c5b4f2763c03697a6d00d3cf-fixedbandwidth-eq-31_shuffled.cnf                   TIMEOUT     1800.0110790729523\n",
      "147                                                           45a09efb026036ff4b8d19024a7563a9-fermat-931960058139995587.cnf                   TIMEOUT       1800.01122713089\n",
      "148                                     74f145bb935650f5c982d7eec6967945-pcmax-scheduling-m43-38782-385402-SAT.sanitized.cnf                   TIMEOUT      1800.006305217743\n",
      "149                                                        8b91c5bf4dfd87ffb46c5ef88a3ef1cd-lec_mult_DvW_12x11.sanitized.cnf                   TIMEOUT     1800.0086603164673\n",
      "150                                                               5e5fe73a2e0ffc8e19873298566919fb-rbsat-v760c43649gyes3.cnf                   TIMEOUT     1800.0081992149353\n",
      "151                                                 bbfe2b27182d2ee7fefdb557f458ac9c-cliquecolouring_n21_k6_c5.sanitized.cnf                   TIMEOUT       1800.01029753685\n",
      "152                                                           24bdfb34654b1aa703fae31ea91e7c7f-tseitin_d3_n158.sanitized.cnf                   TIMEOUT     1800.0093567371368\n",
      "153                                                              1507d9812624b3e0eaf15e40100be020-x9-12014.sat.sanitized.cnf                   TIMEOUT     1800.0072178840637\n",
      "154                                                                 c73edc350cfa8e07af58db50054aea45-noL-11-12.sanitized.cnf                   TIMEOUT     1800.0096290111542\n",
      "155                                                      36dbc206c7d9773bebb03f752070cee9-tseitin_grid_n11_m20.sanitized.cnf                   TIMEOUT     1800.0085356235504\n",
      "156                                                                     fae08ac5b6d7eeb4f56f1cfac6c51703-j3045_4_mdd_bm1.cnf                   TIMEOUT     1800.0079538822174\n",
      "157                                                            e85e793a66a2c9d390f40acf29a5346b-1-ET-512-K-110.sanitized.cnf                   TIMEOUT     1800.0071985721588\n",
      "158                                                              695e287a447fcdd924985a6e73057a38-rbsat-v1150c84314gyes1.cnf                   TIMEOUT     1800.0123970508575\n",
      "159                                                                     9a06700729bf3426beb8a62040f92960-mdp-36-12-unsat.cnf                   TIMEOUT      1800.011340379715\n",
      "160                                                     c9f511adbb3d6e1b37b410f972f270c6-af-synthesis_stb_50_140_3_unsat.cnf                   TIMEOUT      1800.006919145584\n",
      "161                                                                     23daec2c71ce3c1e346e6042f3dc42eb-mdp-32-14-unsat.cnf                   TIMEOUT     1800.0201487541199\n",
      "162                                        f48735588e3515135f039e8bc8efaee5-asconhashv12_opt64_H11_M2-MxJOnbQIXNd_m5_6.c.cnf                   TIMEOUT     1800.0148327350616\n",
      "163                                                             b11374f7fe60b9a63fbfde24b1a0a439-stb_418_125.apx_2_DC-AD.cnf                   TIMEOUT      1800.009774684906\n",
      "164                                                                      91e0db01b254eb78f6643328045bfeb9-sted5_0x1e3-20.cnf                   TIMEOUT     1800.0090641975403\n",
      "165                                                              af05a6b68a1cff165b684d9ff0ae3b3b-x9-12098.sat.sanitized.cnf                   TIMEOUT     1800.0086359977722\n",
      "166                                                                     4f2d6659b5c9bf594ea12e3e1a85799f-mdp-32-11-unsat.cnf                   TIMEOUT     1800.0113537311554\n",
      "167                                                     1a1c9d1dc1ae7fcff92e3c3499864b75-af-synthesis_stb_50_200_4_unsat.cnf                   TIMEOUT      1800.007064819336\n",
      "168                                                     e7248b57a310ad461924eb17956cdf3a-Folkman-190-358004741.sanitized.cnf                   TIMEOUT     1800.0167758464813\n",
      "169                                                                           5690b9b0380aa9508699e56cae5918b5-170058440.cnf                   TIMEOUT     1800.0100417137146\n",
      "170                                                             ff9e9b01d50d01ecb6740472d7dd36cd-1-ZC-512-K-61.sanitized.cnf                   TIMEOUT     1800.0212922096252\n",
      "171                                                          79b9e24dd9af185dbec18c9b0a32b1e2-g2-slp-synthesis-aes-top30.cnf                   TIMEOUT      1800.009337425232\n",
      "172                                                                           d7dbb2cba940af33b2bce37ea99b8110-6s130-opt.cnf                   TIMEOUT      1800.009972333908\n",
      "173                                                               f32bb347996c351bc3e9a91c58e8601d-DLTM_twitter774_83_17.cnf                   TIMEOUT     1800.0142784118652\n",
      "174                                                              aa9b67fd19d54ad51b93ee4ba5dc75fc-x9-10002.sat.sanitized.cnf                   TIMEOUT      1800.007788181305\n",
      "175                                                               0ac5ff376b826f68d384ba05a1d00de0-sokoban-p20.sas.cr.33.cnf                   TIMEOUT     1800.0115821361542\n",
      "176                                                                     3593875d75d6e836a3ac328c5426c1b5-Break_18_32.xml.cnf                   TIMEOUT     1800.0110576152802\n",
      "177                                                              5865fb9a6575d2ae6542c36ab96646a9-x9-11088.sat.sanitized.cnf                   TIMEOUT      1800.011411190033\n",
      "178                                                                       d1940d3d830eb16a7d2270ca4af4acba-mdp-28-11-sat.cnf                   TIMEOUT     1800.0095133781433\n",
      "179                                                             cccee4b0d4c70a5d5422663e2d8887da-1-ET-512-K-96.sanitized.cnf                   TIMEOUT      1800.014059305191\n",
      "180                                                              2dd4b14a8a820e7baa073554ae2c6cb0-rphp_p8_r160.sanitized.cnf                   TIMEOUT     1800.0105421543121\n",
      "181                                                                cdd131110acc861a5a01fae6c4936c91-6g_6color_366_050_04.cnf                   TIMEOUT     1800.0732080936432\n",
      "182                                           f54b352934be08821aa75b3424112956-linked_list_swap_contents_safety_unwind54.cnf                   TIMEOUT      1800.157734155655\n",
      "183                                           0f262afd5b117000f8b4734d175aadab-linked_list_swap_contents_safety_unwind73.cnf                   TIMEOUT     1800.2487163543701\n",
      "184                                                                   093fa3ff9f7bc9979c43f9d2310ac21d-128_125.sanitized.cnf                   TIMEOUT      1800.570250749588\n",
      "185                                                                5952b9f18b73dabb531c60e3ac85d26f-ctl_4291_567_9_unsat.cnf                   TIMEOUT     1800.0048632621765\n",
      "186                                                1545660e72158c7e90da5ad4c877a124-hwmcc20miters-iso-rast-p06.sanitized.cnf                   TIMEOUT     1800.0045969486237\n",
      "187                                                                 29e61b1317804fb897e84237613d42bb-worker_30_60_25_0.9.cnf                   TIMEOUT     1800.0042805671692\n",
      "188                                                              df1bd67978b9b0ec1d326ba174bc273c-x9-12021.sat.sanitized.cnf                   TIMEOUT     1800.0062482357025\n",
      "189                                                            6543793076c30fc7cdfa5a3c819cedc7-oisc-subrv-sll-nested-11.cnf                   TIMEOUT     1800.1452481746674\n",
      "190                                                             ea827f81bd0e9735739adc0d9da1b446-ER_500_30_3.apx_2_DC-AD.cnf                   TIMEOUT      1800.005981206894\n",
      "191                                                                         334aa882de28856fc8c75885285d2a3c-HCP-446-105.cnf                   TIMEOUT     1800.0911910533905\n",
      "192                                                                            f746dae2050dd4aa48bcd53e8bd897b4-ex065_25.cnf                   TIMEOUT       1800.00546169281\n",
      "193                                        c6e03256680b96b52e3345a497333c2b-hwb-n24-02-S786928571.shuffled-as.sat03-1618.cnf                   TIMEOUT     1800.0436940193176\n",
      "194                                                             39277cab188349aee0f229cb7341b5c5-crafted_n12_d6_c4_num23.cnf                   TIMEOUT     1800.0413436889648\n",
      "195                                                              0265448c232e3a25aa5bcd29b1b14567-x9-10051.sat.sanitized.cnf                   TIMEOUT     1800.0048019886017\n",
      "196                                            25cc054738293c0d9377d7b72b78a341-string_compare_safety_cbmc_unwinding_900.cnf                   TIMEOUT     1800.1126971244812\n",
      "197                                                        385042a6042a0df5ddfec6e17f285ae7-lec_mult_CvD_11x11.sanitized.cnf                   TIMEOUT     1800.0043432712555\n",
      "198                                                                       8ffd718f763ed7a3f691cf46e57f8d98-mdp-32-10-sat.cnf                   TIMEOUT      1800.004384279251\n",
      "199                                                     b3167d999edd81291f33636464f2f8e6-Folkman-190-104806020.sanitized.cnf                   TIMEOUT     1800.0050957202911\n",
      "200                                           e8abb9c83efdd96a99990f3b914ea873-linked_list_swap_contents_safety_unwind57.cnf                   TIMEOUT     1800.1500759124756\n",
      "201                                                       7429e380834066c206394139c9e1e17d-af-synthesis_stb_50_100_9_sat.cnf                   TIMEOUT     1800.0063030719757\n",
      "202                                                                     3184e52e950ac39095a801c418f68d50-mdp-28-10-unsat.cnf                   TIMEOUT     1800.0099649429321\n",
      "203                                                                 2b60f9be1439f63b7f174c6b8ca7fedf-sgen1-unsat-121-100.cnf                   TIMEOUT     1800.0123744010925\n",
      "204                                                                  05c8e94aaee86390eaf6e68dd3ec3570-noL-11-2.sanitized.cnf                   TIMEOUT     1800.0107855796814\n",
      "205                                                                     b172b4c218f1e44e205575d2b51e82c4-Schur_161_5_d38.cnf                   TIMEOUT     1800.0082597732544\n",
      "206                                                             d7f2e4ec8e631387c928c79e1054c663-stb_418_125.apx_1_DC-AD.cnf                   TIMEOUT     1800.0080034732819\n",
      "207                                                     ef16970ab9da31165d3c401ff9b29168-Folkman-185-152478531.sanitized.cnf                   TIMEOUT     1800.0087826251984\n",
      "208                                                            16c5482d8e658b54e20d59cfd4b1d588-two-trees-511v.sanitized.cnf                   TIMEOUT     1800.0076081752777\n",
      "209                                                     170a7d847694ee4cfa5c421757672882-af-synthesis_stb_50_140_0_unsat.cnf                   TIMEOUT     1800.0136320590973\n",
      "210                                                                     4e366e723d75fe39bf6db9a24ffb059b-Dodecahedron-k7.cnf                   TIMEOUT     1800.0093495845795\n",
      "211                                                    f70788e8446c1bc42684316de0a6c164-hwmcc20miters-iso-mul3.sanitized.cnf                   TIMEOUT     1800.0131158828735\n",
      "212                                                    1f44da3598ccf8bcbe9a90fb1b2f1ebd-REGRandom-K3-L3-Seed25.sanitized.cnf                   TIMEOUT     1800.0110368728638\n",
      "213                                                             40737c2f46512069d54b0d2bb3b47a20-ER_400_20_4.apx_2_DC-AD.cnf                   TIMEOUT     1800.0077106952667\n",
      "214                                            7f2b4b2bde9b866b93fbfe01341213da-asconhashv12_opt64_H5_M2-xEJ8F_m0_3_U5.c.cnf                   TIMEOUT     1800.0109169483185\n",
      "215                                                                5a068ba62fe30e435a76e2ec80896ef4-ITC2021_Middle_1.xml.cnf                   TIMEOUT      1800.009156703949\n",
      "216                                    bfadd34e71516e95d0ba2a3a8fb2ba03-pcmax-scheduling-m19-10199-62102-UNSAT.sanitized.cnf                   TIMEOUT      1800.007277727127\n",
      "217                                                         96a6796405061571e12d383efe27b703-WS_500_16_70_10.apx_1_DC-ST.cnf                   TIMEOUT     1800.0088949203491\n",
      "218                                                        5b0f16de2c5419b80b608db37d07d994-lec_mult_KvW_11x10.sanitized.cnf                   TIMEOUT     1800.0088500976562\n",
      "219                                                                       ab3438b504b296fcee78ec9e71969863-mdp-36-14-sat.cnf                   TIMEOUT     1800.0113961696625\n",
      "220                                                        dacb03ea801d459ef08db23a8b1d104f-lec_mult_DvW_12x12.sanitized.cnf                   TIMEOUT     1800.0085892677307\n",
      "221                                  a552a058e6376a36b1f1b2724f228364-IBM_FV_2004_rule_batch_1_31_1_SAT_dat.k40.debugged.cnf                   TIMEOUT     1800.0167083740234\n",
      "222                                                              5e1c11b77cdf3717b81b957120f0f477-x9-12001.sat.sanitized.cnf                   TIMEOUT     1800.0091943740845\n",
      "223                                      31bb67fe67e57dbeb8c4819b01892588-hwmcc17miters-xits-iso-oski15a08b08s.sanitized.cnf                   TIMEOUT     1800.0456476211548\n",
      "224                                                                     37a11ae189df2834a3f294736f34608f-mdp-28-16-unsat.cnf                   TIMEOUT     1800.0068855285645\n",
      "225                                           8d4b50ec0cf99097e9ab0835937afee5-hwmcc17miters-xits-iso-6s281b35.sanitized.cnf                   TIMEOUT     1800.0715672969818\n",
      "226                                                                 29ab150158d641568f888e8401d2ac13-bmc_QICE_snp_vld_30.cnf                   TIMEOUT     1800.1398565769196\n",
      "227                                              f0426369f61595aee97055965ee7e6a3-hwmcc12miters-xits-iso-6s111.sanitized.cnf                   TIMEOUT     1800.0275366306305\n",
      "228                                                                            88d151d4e3c6ba9bd78454de141cd108-T105.2.0.cnf                   TIMEOUT     1800.0738060474396\n",
      "229                                                             cb2e8b7fada420c5046f587ea754d052-clique_n2_k10.sanitized.cnf                   TIMEOUT     1800.0071878433228\n",
      "230                                                 768956cc8d1f2d18ae1929f6bb26557a-cliquecolouring_n13_k8_c7.sanitized.cnf                   TIMEOUT     1800.0064289569855\n",
      "231                                                        e38601380bfbab35dd8c5927283533fb-lec_mult_CvK_11x11.sanitized.cnf                   TIMEOUT      1800.006745815277\n",
      "232                                                              1156429862d3c03160406d6d1a786f11-rphp_p8_r170.sanitized.cnf                   TIMEOUT     1800.0047788619995\n",
      "233                                                             4214f45042f8c504f9837afaec88bf8f-ER_400_20_7.apx_2_DC-AD.cnf                   TIMEOUT     1800.0045726299286\n",
      "234                                                       c76f3c6c2e2eae85fc5f3d499f3db88d-mp1-blockpuzzle_5x10_s7_free4.cnf                   TIMEOUT     1800.0045614242554\n",
      "235                                                        b76fa2f50a42120ec54fa98c29113326-lec_mult_DvK_12x12.sanitized.cnf                   TIMEOUT      1800.003846168518\n",
      "236                                                              fa5c6d6736a42650656c5bc018413254-bphp_p23_h22.sanitized.cnf                   TIMEOUT     1800.0037648677826\n",
      "237                                                    2b8a711debc3a6edd01e7b4e305342d9-REGRandom-K4-L2-Seed35.sanitized.cnf                   TIMEOUT     1800.0217907428741\n",
      "238                                                                     16ca57f2cdffd5b0260fc771017e1f04-mdp-36-10-unsat.cnf                   TIMEOUT     1800.0056760311127\n",
      "239                                                                           792495b57d6145fd21d7076eab2ab06a-grs-64-64.cnf                   TIMEOUT     1800.0069601535797\n",
      "240                                                      eed5189b73738270ae3fdb8b33bf31c8-Folkman-180-11710376.sanitized.cnf                   TIMEOUT     1800.0050301551819\n",
      "241                                                                          3b59c5d9b6729f39c8483446701f4ed0-g2-T93.2.1.cnf                   TIMEOUT     1800.1136856079102\n",
      "242                                           02f6ca52f1ada872d82035088701b66a-linked_list_swap_contents_safety_unwind69.cnf                   TIMEOUT     1800.2194530963898\n",
      "243                                                      461df1a7056560279d532bc2743022b6-Folkman-185-75415683.sanitized.cnf                   TIMEOUT     1800.0037274360657\n",
      "244                                                      14ea5857d75ae2b235a3ba97373ea732-af-synthesis_stb_50_40_9_unsat.cnf                   TIMEOUT      1800.004670381546\n",
      "245                                                1a20d903db74ab566efc68a35e6b0ec5-hwmcc20miters-iso-rast-p11.sanitized.cnf                   TIMEOUT      1800.108969926834\n",
      "246                                                             4a8285a53f30b35016b0c85ea17ba155-1-TC-256-K-68.sanitized.cnf                   TIMEOUT     1800.0039885044098\n",
      "247                                                                   41f4cb4992a481c9d43e2e1a4e2349ac-sgen1-sat-180-100.cnf                   TIMEOUT     1800.0051004886627\n",
      "248                                                             dd5fc8da5454a990a0e7999884158c64-1-ET-256-K-70.sanitized.cnf                   TIMEOUT     1800.0039749145508\n",
      "249                                                                           2c3c28f6d939d157e909c57a265fc908-mchess_17.cnf                   TIMEOUT      1800.004096031189\n",
      "250                                                         9e749596a8de36d8ab706c96cf128455-WS_500_16_70_10.apx_2_DC-AD.cnf                   TIMEOUT     1800.0044813156128\n",
      "251                                                            5ab3040d8617bf9f4d4cef4e56dcfd03-g2-hwmcc15deep-6s161-k17.cnf                   TIMEOUT     1800.0054912567139\n",
      "252                                                                     eda4aa84aeeb306468396fa82a6bba5a-pb_300_05_lb_17.cnf                   TIMEOUT     1800.0048322677612\n",
      "253                                           0d3160b80aa406bfc718c007265b9e73-linked_list_swap_contents_safety_unwind65.cnf                   TIMEOUT     1800.1917343139648\n",
      "254                                                           6a674acad2aeac729ecb39ad9e4a2298-1-ZC-1024-K-116.sanitized.cnf                   TIMEOUT     1800.0411665439606\n",
      "255                                                        00d1fe07ab948b348bb3fb423b1ef40d-lec_mult_KvW_12x11.sanitized.cnf                   TIMEOUT     1800.0035409927368\n",
      "256                                                      77b7f7bbf75faaee28f473b9941de103-Folkman-185-19924337.sanitized.cnf                   TIMEOUT     1800.0042340755463\n",
      "257                                                            4517202fdd0ccc8a97d83684b021ea96-VanDerWaerden_2-3-14_186.cnf                   TIMEOUT      1800.003844499588\n",
      "258                                                       12b4a08e412a3bffb513ca65639c7c69-Folkman-175-7416734.sanitized.cnf                   TIMEOUT     1800.0040488243103\n",
      "259                                                                     425c81fc3af7f5124b6282583d80df11-mdp-32-12-unsat.cnf                   TIMEOUT     1800.0038850307465\n",
      "260                                                       e067414e153ee98a7842bd6d6dafeee5-af-synthesis_stb_50_200_0_sat.cnf                   TIMEOUT     1800.0039057731628\n",
      "261                                                     94571e8a1081385029d1ecd53ffcdf8e-af-synthesis_stb_50_200_0_unsat.cnf                   TIMEOUT     1800.0044753551483\n",
      "262                                                         08be288536c3178e6874a5676493923c-g2-hwmcc15deep-bob12s02-k16.cnf                   TIMEOUT     1800.0138289928436\n",
      "263                                                      82d9e8d2cfea101dfe52c8359b7bb163-af-synthesis_stb_50_40_2_unsat.cnf                   TIMEOUT     1800.0140142440796\n",
      "264                                   dafc03ea784fee849febca7b64230558-pcmax-scheduling-m30-14113-167638-UNSAT.sanitized.cnf                   TIMEOUT     1800.0161652565002\n",
      "265                                                      07b476c83150d3cb7e08f5d045e255ab-marg5x5.shuffled-as.sat03-1455.cnf                   TIMEOUT     1800.0058691501617\n",
      "266                                    a5419a63d913bde0ba5bcd8a8571342f-asconhashv12_opt64_H11_M2-tBi5i1RIgRz_m0_1_U23.c.cnf                   TIMEOUT     1800.0107471942902\n",
      "267                                                                     1e74212168124b696531fed471b480b1-mdp-32-10-unsat.cnf                   TIMEOUT     1800.0129826068878\n",
      "268                                                               fb45d41807bcbbbe689de282c1310798-Break_unsat_16_27.xml.cnf                   TIMEOUT     1800.0114059448242\n",
      "269                                                                          9dcbf221b8d2bb01f30bcca283f3608d-EDP3-11000.cnf                   TIMEOUT      1800.025369644165\n",
      "270                                                     2a15a30186afdad41a49c5c5366d01be-Timetable_C_392_E_62_Cl_26_S_28.cnf                   TIMEOUT     1800.0288605690002\n",
      "271                                                    e06be7a69dbd6d6866a2799cd40c4bfe-hwmcc20miters-iso-mul7.sanitized.cnf                   TIMEOUT     1800.0312416553497\n",
      "272                                                                               aa39c6d885d283a085d033ab7b89f8c8-urq45.cnf                   TIMEOUT     1800.0062091350555\n",
      "273                                                      f18c7c7b8666458fd04ea7a253ae1e20-FmlaImplyChain_3_7_7.sanitized.cnf                   TIMEOUT     1800.0078992843628\n",
      "274                                                                  994783ef4ed3a0366842e1b6f9128a6f-noL-11-6.sanitized.cnf                   TIMEOUT     1800.0072507858276\n",
      "275                                                    912ce8c21d8fb8abc491cb205ec23e9f-hwmcc20miters-iso-mul2.sanitized.cnf                   TIMEOUT       1800.00998878479\n",
      "276                                      62dcbf79f5ecef5618c9d9a00311326c-pcmax-scheduling-m13-1655-9604-UNSAT.sanitized.cnf                   TIMEOUT     1800.0064132213593\n",
      "277                                                 973d699ec01b88da869233a79aaa1912-cliquecolouring_n13_k9_c8.sanitized.cnf                   TIMEOUT     1800.0059502124786\n",
      "278                                        305ad7d773d6c72be9b1efbf7bf482e0-asconhashv12_opt64_H9_M2-MIC4kfhiA_m0_6_U2.c.cnf                   TIMEOUT     1800.0102446079254\n",
      "279                                           c21cea390a50204944aa00babd56b53c-linked_list_swap_contents_safety_unwind70.cnf                   TIMEOUT     1800.2510373592377\n",
      "280                                                                   84c6d7e4a18aacf166105aaa3cd6e3de-128_100.sanitized.cnf                   TIMEOUT     1800.5513908863068\n",
      "281                                     6fc528fc3d0fd5a2c50992feb8bf0357-pcmax-scheduling-m24-17855-226744-SAT.sanitized.cnf                   TIMEOUT     1800.0096950531006\n",
      "282                                                       773f3bd29e202ff700d8b5b459857a2c-Folkman-175-9054056.sanitized.cnf                   TIMEOUT     1800.0074005126953\n",
      "283                                                                               34fa0922e66aee3b6f6ffe1760f459d4-f9idw.cnf                   TIMEOUT     1800.0158479213715\n",
      "284                                                                         1b3c7ac17705be94daaddb5f5ae4816b-rook-56-0-0.cnf                   TIMEOUT     1800.0074570178986\n",
      "285                                                 67c533489a498495525efee429340958-cliquecolouring_n15_k9_c8.sanitized.cnf                   TIMEOUT     1800.0034093856812\n",
      "286                                                        25447f441df2f134223d3e43319c99ad-lec_mult_CvW_12x11.sanitized.cnf                   TIMEOUT     1800.0036299228668\n",
      "287                                                         55199d55fa9c3751a525d6c0a0e649bd-WS_500_16_90_70.apx_2_DC-AD.cnf                   TIMEOUT       1800.00443816185\n",
      "288                                                        462d72f9b78ab1cd080667ff12a114ac-lec_mult_CvK_12x12.sanitized.cnf                   TIMEOUT     1800.0043365955353\n",
      "289                                                                     1548977b4e27032fe07cda357782cd6a-mdp-28-14-unsat.cnf                   TIMEOUT     1800.0040638446808\n",
      "290                                                               8bf63d50ebdb645b75a24370b8f94d31-sokoban-p20.sas.cr.25.cnf                   TIMEOUT     1800.0045578479767\n",
      "291                                                                     5411047bc24f4321f0e490e01e4a0910-goldcrest-and-9.cnf                   TIMEOUT     1800.0046145915985\n",
      "292                                                              b3c587501567db72e6e66c6930cf15ed-StConn_7_128.sanitized.cnf                   TIMEOUT     1800.0041346549988\n",
      "293                                             0e5ffe8651c6d9c3cccc3f6cb72be39a-g2-test_v5_r10_vr10_c1_s21502.smt2-cvc4.cnf                   TIMEOUT     1800.0135991573334\n",
      "294                                                    1ea68d0008f01c50a1e4fec77dd0775e-REGRandom-K4-L1-Seed30.sanitized.cnf                   TIMEOUT      1800.007390499115\n",
      "295                                                              11cc532ecddfb10a47e0a869b4867c1b-Break_triple_20_36.xml.cnf                   TIMEOUT     1800.0058813095093\n",
      "296                                                                       db15a85651e0d941a11bf8640625edd8-mdp-32-11-sat.cnf                   TIMEOUT     1800.0034863948822\n",
      "297                                       da929c2aefde5e59878ad87c4323e581-pcmax-scheduling-m12-8049-55035-SAT.sanitized.cnf                   TIMEOUT     1800.0045976638794\n",
      "298                                           1e0da402100982e53001a64281149dd9-linked_list_swap_contents_safety_unwind63.cnf                   TIMEOUT     1800.2303676605225\n",
      "299                                                           7db30d12cb06f0dc2f30abff80d96d6a-two-trees-1023v.sanitized.cnf                   TIMEOUT     1800.0057365894318\n",
      "300                                                             cd361d33986dccd7f2d86016d6c35241-ecarev-110-4099-22-30-7.cnf                   TIMEOUT      1800.012943983078\n",
      "301                                                                     1165e12b6addad01b491c4616306186c-mulhs016-sc2009.cnf                   TIMEOUT     1800.0046684741974\n",
      "302                                                              07c50b89d97a55b6e1629999a445a2ba-ctl_4201_555_unsat_pre.cnf                   TIMEOUT     1800.0053493976593\n",
      "303                      71ec94c233016219e12d671594dc88e5-circuit_32in32out_with_70gates_7in7out_dist128_seed1.sanitized.cnf                   TIMEOUT     1800.0046699047089\n",
      "304                                                 18c0eb461bda29214bd43b84199a3b61-cliquecolouring_n31_k5_c4.sanitized.cnf                   TIMEOUT     1800.0037217140198\n",
      "305                                                                       d11944fbca2dd6540f18bd05b6ccda0c-mdp-32-14-sat.cnf                   TIMEOUT     1800.0046043395996\n",
      "306                                                                 907e7dc703e16ad2c7e56a33fe3b3e5f-manol-pipe-g10bid_i.cnf                   TIMEOUT     1800.0084805488586\n",
      "307                                                      479b1413f8bd1d6fc0723856ffca9792-constraints_18_0.4_2.sanitized.cnf                   TIMEOUT     1800.0060424804688\n",
      "308                               0fa8623240b5c14c1308f863a6dc88ab-urqh1c5x5.shuffled-as.sat03-1468.cnf.mis-103.debugged.cnf                   TIMEOUT     1800.0060958862305\n",
      "309                                                                bd12a2b66110451b050bd5d2943b1854-ITC2021_Early_10.xml.cnf                   TIMEOUT     1800.0058891773224\n",
      "310                                                                     43e492bccfd57029b758897b17d7f04f-pb_300_09_lb_07.cnf                   TIMEOUT     1800.0054185390472\n",
      "311                                                              cd33a862b81ba7b887845d3510c41c62-MASG0_72_keystream76_0.cnf                   TIMEOUT     1800.0050444602966\n",
      "312                                                                     44c25de7963c45e92a2407ad839f6e8b-Break_20_72.xml.cnf                   TIMEOUT     1800.0067625045776\n",
      "313                                                           aa6e6f67719ca43aef922f2ebabc409a-1-ZC-1024-K-117.sanitized.cnf                   TIMEOUT      1800.027450799942\n",
      "314                                                                         6f0e236c999de0a5e3c46fd3463b1d0d-rook-56-1-1.cnf                   TIMEOUT     1800.0118396282196\n",
      "315                                                                  634a271f5fe339007a186539d615e92f-noL-11-4.sanitized.cnf                   TIMEOUT     1800.0052435398102\n",
      "316                                                        dd57f6ebf07a7f932a82bed1877788e6-lec_mult_KvW_10x10.sanitized.cnf                   TIMEOUT     1800.0043301582336\n",
      "317                                           6008b958ff6877f32c26b72728df4523-linked_list_swap_contents_safety_unwind76.cnf                   TIMEOUT     1800.2749636173248\n",
      "318                                     5432e4dd44480fadbd66647db1750c80-pcmax-scheduling-m24-24102-255206-SAT.sanitized.cnf                   TIMEOUT     1800.0048141479492\n",
      "319                                                             29d86d9a3b85c349e10ad68d7a713da2-1-ZC-512-K-63.sanitized.cnf                   TIMEOUT       1800.01220703125\n",
      "320                                                                         59825ed96dc3bbdbd8789a4870b323ec-hcp_CP18_18.cnf                   TIMEOUT     1800.0137655735016\n",
      "321                                                             2b032f8a8976a302ad125eb50a3e8445-1-ZC-512-K-60.sanitized.cnf                   TIMEOUT     1800.0175409317017\n",
      "322                                                             4d7f6efbae05b66ffbbd509a430d57f8-stb_418_125.apx_1_DC-ST.cnf                   TIMEOUT     1800.0070540904999\n",
      "323                                                    02e4f23ed59566d343b605dc2cc30a01-REGRandom-K4-L3-Seed40.sanitized.cnf                   TIMEOUT     1800.0825581550598\n",
      "324                                                             f15ad4206d95a3f8f1358b33301539be-ER_400_20_7.apx_1_DC-AD.cnf                   TIMEOUT     1800.0063767433167\n",
      "325                                                                       a9cb77454c6cfdf4092fb304d3aae8b7-mdp-32-16-sat.cnf                   TIMEOUT     1800.0056025981903\n",
      "326                                                                                 26254890fa7107f85242ec9190da2a7a-002.cnf                   TIMEOUT     1800.0065813064575\n",
      "327  83dc8f675657b3e6d148ef5cf897fb82-openstacks-sequencedstrips-nonadl-nonnegated-os-sequencedstrips-p30_3.025-NOTKNOWN.cnf                   TIMEOUT     1800.0081856250763\n",
      "328                                                               94dd280b1562ee7dae44b303b8fed233-Break_unsat_18_31.xml.cnf                   TIMEOUT     1800.0052001476288\n",
      "329                                                       0bad2ce307bf5b68db26fa34e252c9d4-af-synthesis_stb_50_100_4_sat.cnf                   TIMEOUT     1800.0048887729645\n",
      "330                                                                              6077d4e5f755124ddc84068d6ccb630f-Nb44T6.cnf                   TIMEOUT      1800.034877538681\n",
      "331                                                                     cc30e4eedbcf9c3bc110894d08781246-j3037_9_mdd_bm1.cnf                   TIMEOUT      1800.004600763321\n",
      "332                                                              9d9c4fa425282759eb9e98b82fb5f56e-x9-12087.sat.sanitized.cnf                   TIMEOUT     1800.0045065879822\n",
      "333                                                            1e3c3e8d349759b8b482a6f2721762c4-apn-sbox5-cut3-symmbreak.cnf                   TIMEOUT     1800.0045111179352\n",
      "334                                                 ac347c21ca0759079c0be9a758e4e924-cliquecolouring_n41_k5_c4.sanitized.cnf                   TIMEOUT     1800.0046117305756\n",
      "335                                     32585e66749f42e161e94fc49fd64750-pcmax-scheduling-m26-6398-62377-UNSAT.sanitized.cnf                   TIMEOUT      1800.004522562027\n",
      "336                                                                 52aaf653a79ca14fa1127cda32aa94ce-noL-11-10.sanitized.cnf                   TIMEOUT     1800.0065395832062\n",
      "337                                                            ee57608ea4b74f25aebca809d59711c9-oisc-subrv-sll-nested-13.cnf                   TIMEOUT      1800.213625907898\n",
      "338                                                        69e34d0f49c71a7e145d8104ebe0273f-sokoban-p16.sas.ex.15-sc2016.cnf                   TIMEOUT      1800.062406539917\n",
      "339                                                              c0e6e6eeebd48ca600cfc7d662fa804c-x9-10093.sat.sanitized.cnf                   TIMEOUT     1800.0043318271637\n",
      "340                      8942dca5dc0876fc3f723f738d72de1c-circuit_32in32out_with_64gates_8in6out_dist128_seed2.sanitized.cnf                   TIMEOUT       1800.00989818573\n",
      "341                                                                 d7627176604d5b86f34d2f5d14d8ee99-worker_40_80_40_0.9.cnf                   TIMEOUT       1800.00541305542\n",
      "342                                                     1b0f4ff7984b8d4cf2873200fb1680fb-af-synthesis_stb_50_140_1_unsat.cnf                   TIMEOUT     1800.0066018104553\n",
      "343                                     31e788a12ddc8b43ec20e77d53abaa23-pcmax-scheduling-m40-26287-324155-SAT.sanitized.cnf                   TIMEOUT      1800.006516456604\n",
      "344                                                                       a5dc6226e4c0bebb06926efe55640995-mdp-36-10-sat.cnf                   TIMEOUT     1800.0044329166412\n",
      "345                                                         44fd38bd0c5fc7336be468161dee4eda-post-cbmc-aes-ee-r3-noholes.cnf                   TIMEOUT     1800.0143971443176\n",
      "346                                                             a917b02a7c31e74c8ccf8b8f001abf37-1-ZC-512-K-67.sanitized.cnf                   TIMEOUT     1800.0061678886414\n",
      "347                                     207b28c3cefc55b143259222cbfd4962-pcmax-scheduling-m37-28831-324346-SAT.sanitized.cnf                   TIMEOUT     1800.0052592754364\n",
      "348                                                             a6ed647f85f20be4aedd1e9ce31cbdcd-1-ET-256-K-55.sanitized.cnf                   TIMEOUT     1800.0055928230286\n",
      "349                                                                            897acc1858ce0887f286aba5c0d56d71-Nb13T165.cnf                   TIMEOUT     1800.0410432815552\n",
      "350                                                             4be4ae25aae88528bc10f8369bba86df-ER_400_20_4.apx_1_DC-AD.cnf                   TIMEOUT     1800.0046074390411\n",
      "351                                                             7dc0e1b5fedbd8351907f1316b58e68f-1-ZC-512-K-65.sanitized.cnf                   TIMEOUT     1800.0109589099884\n",
      "352                                                             67b4844c4bfbaf961cbbd79b953aa5c2-stb_588_138.apx_1_DC-AD.cnf                   TIMEOUT     1800.0070617198944\n",
      "353                      9e6c2e7b0d6f58e449716deb9305525a-circuit_32in32out_with_96gates_7in7out_dist128_seed1.sanitized.cnf                   TIMEOUT     1800.0151197910309\n",
      "354                                                     c82eee3badbc8432dad72d1d575a0ea6-preimage_80r_495m_160h_seed_379.cnf                   TIMEOUT     1800.0044779777527\n",
      "355                                                           c69c5163c9087ffe770743bd4cdafcc9-tseitin_d3_n162.sanitized.cnf                   TIMEOUT       1800.00434923172\n",
      "356                                         39fba35826ce8c87cd8e8de1969b2dd2-SGI_30_80_26_70_4-log.shuffled-as.sat03-208.cnf                   TIMEOUT      1800.004585981369\n",
      "357                                                        4c54efa61599e33d514bf718ff23ad09-lec_mult_CvK_12x11.sanitized.cnf                   TIMEOUT     1800.0167274475098\n",
      "358                                                               dcf5b8224d1e0748871c83ee10067255-2dlx_ca_bp_f_liveness.cnf                   TIMEOUT     1800.0280203819275\n",
      "359                                                        778e0fd8f034b7cfd539b1b523ed4c4a-lec_mult_CvW_12x12.sanitized.cnf                   TIMEOUT     1800.0043795108795\n",
      "360                                                                          9777418f75e1dfbd7d34e9b0ba4bf0b5-grs-64-128.cnf                   TIMEOUT     1800.0117428302765\n",
      "361                                                                            e0bf26a7527cd162c7d3016d4b2ab9fb-md5_48_1.cnf                   TIMEOUT     1800.0053038597107\n",
      "362                                                         7fca438c38e5b7a70e38778ec146cfe0-ctl_4291_567_2_unsat-sc2013.cnf                   TIMEOUT      1800.006234407425\n",
      "363                                           573cc82ec61e01fd6e6308493ee56289-linked_list_swap_contents_safety_unwind80.cnf                   TIMEOUT      1800.254840373993\n",
      "364                                                               b3f53d4c67b234d27e21f5e56bfd39e8-StConn_8_32.sanitized.cnf                   TIMEOUT     1800.0054957866669\n",
      "365                                                              697c96ac45534726c7dbd96faa11a86a-x9-11094.sat.sanitized.cnf                   TIMEOUT     1800.0066525936127\n",
      "366                                      7b48f9354d25521bd542d859c0ecdde9-hwmcc17miters-xits-iso-oski15a08b00s.sanitized.cnf                   TIMEOUT      1800.028576374054\n",
      "367                                                       d40a68825bdbcdd7642b249325a7b6a2-Folkman-180-5383714.sanitized.cnf                   TIMEOUT     1800.0054683685303\n",
      "368                                                        bb591c6447fb6b4180f6d288eb2ff62a-lec_mult_DvW_11x10.sanitized.cnf                   TIMEOUT     1800.0055477619171\n",
      "369                                                               b087c195e5b7bb191d3a701aa0aa8cf1-Break_unsat_14_23.xml.cnf                   TIMEOUT     1800.0919642448425\n",
      "370                                                               16c27d738cb45b766b8823ca4f428cf0-rbsat-v760c43649gyes7.cnf                   TIMEOUT     1800.0042035579681\n",
      "371                                                     f16b7329ef5728f722c291156b3b098e-af-synthesis_stb_50_100_9_unsat.cnf                   TIMEOUT     1800.0048139095306\n",
      "372                                                                  0a27eb7c16c1e69ff4d087d217ac89cb-noL-11-0.sanitized.cnf                   TIMEOUT     1800.0074210166931\n",
      "373                                           b8f5fc2425facb3aec52a665f32a500c-asconhashv12_opt64_H6_M2-PgbpwX_m0_4_U1.c.cnf                   TIMEOUT     1800.0050172805786\n",
      "374                                         6f4df706246d29bf38adadd274fadab3-SGI_30_60_19_60_6-dir.shuffled-as.sat03-112.cnf                   TIMEOUT     1800.0045280456543\n",
      "375                                                             d5f6b2092795aad67d1df1fd3a329552-ER_400_20_7.apx_2_DC-ST.cnf                   TIMEOUT     1800.0046873092651\n",
      "376                                                        bdcdb52d16f87ea20f091ca4b0b6a36f-lec_mult_CvK_11x10.sanitized.cnf                   TIMEOUT     1800.0038576126099\n",
      "377                     fa50d69adc71fe4daf7f9088cd02864a-circuit_32in64out_with_150gates_6in6out_dist256_seed1.sanitized.cnf                   TIMEOUT     1800.0098285675049\n",
      "378                                                              09b61bbf19748094a7d896aac314ab36-x9-12063.sat.sanitized.cnf                   TIMEOUT      1800.004145860672\n",
      "379                                                   0ef029cb7616d4a89eb6d1f63e89f67e-or_randxor_k3_n510_m510.sanitized.cnf                   TIMEOUT     1800.0049681663513\n",
      "380                                                           556cf803f9de4b2ec688825b5ff7e325-tseitin_d3_n174.sanitized.cnf                   TIMEOUT     1800.0062727928162\n",
      "381                                                      30f0db845937bbda3ffde60e5ed4cb3f-Folkman-190-66337703.sanitized.cnf                   TIMEOUT     1800.0046396255493\n",
      "382                                                              195852083a05edee1902233698eec14a-x9-11077.sat.sanitized.cnf                   TIMEOUT      1800.005215883255\n",
      "383                                                        be18894105e006399cc018abc14b204f-af-synthesis_stb_50_40_9_sat.cnf                   TIMEOUT     1800.0048778057098\n",
      "384                                                              09c1b79b1cfe3522364fe60aef780703-x9-12092.sat.sanitized.cnf                   TIMEOUT     1800.0045328140259\n",
      "385                                                 096e485489025895cdc59887baafa08b-test_v7_r17_vr5_c1_s25451.smt2-cvc4.cnf                   TIMEOUT     1800.0072238445282\n",
      "386                                                                         2115182958f6cfb5a172a24f989b86dd-rook-42-0-1.cnf                   TIMEOUT     1800.0063209533691\n",
      "387                                                        a5fc113e7d4899f4e4af14b87b6fd6ae-Kakuro-easy-097-ext.xml.hg_4.cnf                   TIMEOUT     1800.1108994483948\n"
     ]
    }
   ],
   "source": [
    "no_gbc_df = extract_results(\"slurm-29516543.out\", \"2.\")\n",
    "print(no_gbc_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   file_name             result_no_gbc      time_limit_no_gbc                 result_gbc        time_limit_gbc\n",
      "0                                                                           ef330d1b144055436a2d576601191ea5-crn_11_99_u.cnf                     UNSAT      7.376698970794678                      UNSAT     9.137923240661621\n",
      "1                                                                edceb8782e72e290fa54757dbfdd0173-x9-09057.sat.sanitized.cnf                     UNSAT     375.36069917678833                      UNSAT     402.2980101108551\n",
      "2                                                    70ef2b6bdc4101a0e35cf3d165571fe3-qwh.50.1250.shuffled-as.sat03-1655.cnf                       SAT      224.8674235343933                        SAT      738.501430273056\n",
      "3                                                                  c5ae0ec49de0959cd14431ce851c14f8-Circuit_multiplier22.cnf                       SAT      707.7964084148407                        SAT     787.3294777870178\n",
      "4                                                                d195412a62cdcbb851136f60af76f463-x9-09098.sat.sanitized.cnf                     UNSAT       620.841639995575                      UNSAT     875.9901928901672\n",
      "5                                                                 08ccc34df5d8eb9e9d45278af3dc093d-simon-r16-1.sanitized.cnf                       SAT    0.03329801559448242                        SAT    0.3664882183074951\n",
      "6                                                                       22b4cf412c811872d6ba5078106aeb6c-j3037_10_rggt_b.cnf                       SAT      332.8107979297638                        SAT     1784.839429140091\n",
      "7                                                                   b8c7c777e76995e5bf7b517f2db234ba-noL-11-16.sanitized.cnf                       SAT      524.2708959579468                    TIMEOUT    1800.0164196491241\n",
      "8                                                                      88e89266e58f1f125ebc4e0f66e2c060-32_200.sanitized.cnf                       SAT      770.8946743011475                    TIMEOUT    1800.7862794399261\n",
      "9                                                               a58b8a5eb61d3110d9fa36a03de588d2-stb_495_168.apx_1_DC-AD.cnf                     UNSAT     1381.6919436454773                      UNSAT    1318.8199121952057\n",
      "10                                                               1afa2d7a3d817c3149da432eece66da8-worker_550_550_550_0.3.cnf                       SAT     156.67394828796387                        SAT    312.13178753852844\n",
      "11                                                                7f7109dce621ef361a72b3e8cee9a962-Break_unsat_06_07.xml.cnf                     UNSAT     6.9258811473846436  a non-zero exit -6 status    7.0252439975738525\n",
      "12                                              1009c791cee542cdf19651fe25e6881a-summle_X4053_steps8_I1-2-2-4-4-8-25-100.cnf                       SAT     1576.8737621307373                        SAT    1329.1933035850525\n",
      "13                                                                                                       track_main_2024.uri  a non-zero exit 1 status   0.006631612777709961   a non-zero exit 1 status  0.034845590591430664\n",
      "14                                                               ad4e151d80c7012d88dd79bcfceaade5-x9-08075.sat.sanitized.cnf                     UNSAT      182.2089774608612                      UNSAT    186.06001138687134\n",
      "15                                                                     210fe1c0ffefde5bcd03d59b1efa6984-32_325.sanitized.cnf                       SAT       1255.48628282547                    TIMEOUT    1800.5088603496552\n",
      "16                                                        72c0d81e16d91bcaed808efcde2e5069-Folkman-175-1251868.sanitized.cnf                       SAT       1276.52694106102                    TIMEOUT    1800.0139698982239\n",
      "17                                                               e08f11f0a3bd266ee5c78ce332de107f-x9-10096.sat.sanitized.cnf                     UNSAT       1640.28568649292                      UNSAT     1616.926666021347\n",
      "18                                                                 5ea72bcdccc86bd1a924029f7b81aec5-atco_enc1_opt1_10_15.cnf                       SAT     1573.2466278076172                    TIMEOUT    1800.0232467651367\n",
      "19                                                               13ae2628d8e113db1786dba41a65fe38-x9-10027.sat.sanitized.cnf                       SAT     1455.0293724536896                        SAT    1303.6495292186737\n",
      "20                                                                cdce6277b01ae06ddb95468c5f05de71-simon-r17-0.sanitized.cnf                       SAT    0.06718182563781738                        SAT   0.07528519630432129\n",
      "21                                                                      3988a60c6e93167763c6fd2a347d5859-Break_08_24.xml.cnf                       SAT     1.0175721645355225                        SAT     5.629793882369995\n",
      "22                                                                2b043efb4bde6d83f7c95a8e8e2d7bf8-simon-r21-0.sanitized.cnf                       SAT    0.06553506851196289                        SAT   0.11589694023132324\n",
      "23                                                       f8443057c188f6c9cfb711ff3d4aa6ff-constraints_17_0.5_2.sanitized.cnf                     UNSAT       275.585209608078  a non-zero exit -6 status    16.997069120407104\n",
      "24                                                         a38affaa741c958fc32769d5fe89b06c-frb65-12-2.used-as.sat04-874.cnf                       SAT      261.4643633365631                        SAT      97.6516604423523\n",
      "25                      303480ca7e8322d771c94caf4ebd4e95-circuit_48in64out_with_700gates_4in4out_dist128_seed1.sanitized.cnf                       SAT      210.7985589504242                        SAT    266.42641735076904\n",
      "26                                                                                                                  unzip.py  a non-zero exit 1 status  0.0052127838134765625   a non-zero exit 1 status   0.03413963317871094\n",
      "27                                                                     0cace8a29a1d6b225a8da561d35e8f5a-lru_10.sanitized.cnf                       SAT      331.8655457496643                        SAT    338.01461720466614\n",
      "28                                                                        04ded94454830d4ea960327e8b91f5a3-mdp-28-14-sat.cnf                       SAT      698.9475491046906                        SAT     566.1208407878876\n",
      "29                                                                      c8e64404361f2426490d39459832c66a-64_25.sanitized.cnf                       SAT     155.52382159233093                        SAT      773.176908493042\n",
      "30                      170b13af977e962321c493544b2bd0a9-circuit_48in64out_with_800gates_4in4out_dist128_seed4.sanitized.cnf                       SAT      316.0367782115936                        SAT     627.1558392047882\n",
      "31                                                                  ca14adcb9296a7b31d7815c2ed16d0f1-ITC2021_Early_3.xml.cnf                       SAT     2.3688273429870605                        SAT    12.432169437408447\n",
      "32                                                       6add7e416c1126607afeb2666af330ac-constraints_16_0.5_1.sanitized.cnf                     UNSAT     507.50465273857117                      UNSAT     571.8437790870667\n",
      "33                                                                 7083b70c1976162e2693d7a493717ffd-battleship-14-26-sat.cnf                       SAT      320.4366466999054                    TIMEOUT    1800.0052380561829\n",
      "34                                                                     f50eaf02a8041510b64104998cc81d2f-sted5_0x24204-50.cnf                       SAT      415.2292757034302                        SAT    1007.3599445819855\n",
      "35                                                               19e2c3a0865c8c1b4543d11213bebe5f-x9-09024.sat.sanitized.cnf                     UNSAT     237.43327927589417                      UNSAT    271.48260283470154\n",
      "36                                                               3a75ad246dbc750a7391ad887c5b0835-x9-11093.sat.sanitized.cnf                       SAT     1246.2681241035461                        SAT    252.96102213859558\n",
      "37                                                                  50019e4419d48196bb4b95933a8b5030-noL-11-14.sanitized.cnf                       SAT      1144.640493631363                        SAT    382.07967019081116\n",
      "38                                                                            ec84eecb124c63d4757e083dd0e5a9ff-mchess_15.cnf                     UNSAT     1014.3680686950684                      UNSAT       949.07559466362\n",
      "39                                                               915a25bd189357e4c6d7771b69a6849f-x9-09004.sat.sanitized.cnf                     UNSAT      842.1606323719025                      UNSAT     669.5578818321228\n",
      "40                                                                       e481a97a93fd6b4303f39024d19c2867-j3037_1_gmto_b.cnf                       SAT     373.71688866615295                    TIMEOUT     1800.004183769226\n",
      "41                                                        71a3dd6156463c0c55c4b38394faa753-af-synthesis_stb_50_120_4_sat.cnf                       SAT     1451.3811123371124                    TIMEOUT     1800.007712841034\n",
      "42                       70bfbd054dc9ffd394fab32845b492d3-circuit_32in32out_with_100gates_7in7out_dist64_seed2.sanitized.cnf                       SAT      926.4122495651245                    TIMEOUT    1800.0407621860504\n",
      "43                                                                5ee7de2bd112aa39485e79c9d487bf8f-simon-r23-1.sanitized.cnf                       SAT    0.06699275970458984                        SAT    0.0673668384552002\n",
      "44                                                               c5a98231dd54cbca06135293bb7e1985-x9-11053.sat.sanitized.cnf                       SAT       598.113231420517                        SAT     411.8127737045288\n",
      "45                                                               7b9d8b9b31b530effd634f5a8f1f4411-stb_531_83.apx_2_DC-ST.cnf                     UNSAT     1470.9252364635468                      UNSAT    1642.8901689052582\n",
      "46                       396fd56f3fd7b85afbba4254ea6e746c-circuit_32in32out_with_80gates_7in7out_dist128_seed1.sanitized.cnf                       SAT     1363.5233099460602                    TIMEOUT    1800.0433831214905\n",
      "47                                                                            8704094951693f99fd21403a039c8131-mchess_16.cnf                     UNSAT     1424.6406893730164                      UNSAT     580.1816735267639\n",
      "48                                                                     26b648475cfd06695a17ac95f4469744-128_75.sanitized.cnf                       SAT     1471.3590421676636                    TIMEOUT    1800.4645195007324\n",
      "49                       6f7a0e1cf94b6b26eafc08a827a692ce-circuit_64in64out_with_64gates_8in5out_dist256_seed1.sanitized.cnf                       SAT     376.67070746421814                        SAT     189.1146879196167\n",
      "50                                                               475803537529d9d14a034957f48a6d38-x9-09076.sat.sanitized.cnf                     UNSAT      335.3701696395874                      UNSAT     329.0576856136322\n",
      "51                                                            e6cdc2687fa53506021f05b60ad0c6a2-GracefulGraph-K05-P02_c18.cnf                       SAT       42.9225959777832                        SAT    429.04988145828247\n",
      "52                                                               f376d4c191518ed704326960b6b19a4b-x9-10084.sat.sanitized.cnf                     UNSAT       1568.57333445549                      UNSAT     1575.468246459961\n",
      "53                                                              2eb039a5aa6bfb6e41d3afd41071ca55-ER_500_30_3.apx_1_DC-ST.cnf                     UNSAT     1236.5666477680206                    TIMEOUT     1800.004774093628\n",
      "54                                                                                  ecca77b5350eca6a4323edd5b38208c6-004.cnf                       SAT      485.8935627937317                        SAT     455.5075840950012\n",
      "55                                             dc7817dfa2817916b266c1cfacd2ee66-constraints_25_4_5_12_12_0_0_0.sanitized.cnf                     UNSAT     1271.2375054359436  a non-zero exit -6 status     18.04038691520691\n",
      "56                                       bd8bc25be2b36c64b38459c17e815814-pcmax-scheduling-m11-1517-6802-UNSAT.sanitized.cnf                     UNSAT      554.9501452445984                      UNSAT     763.9916117191315\n",
      "57                    9276ce38c625b2d00de247f8588f1542-combined-crypto1-wff-seed-102-wffvars-500-cryptocplx-31-overlap-2.cnf                       SAT     1438.4343104362488                    TIMEOUT    1800.0040302276611\n",
      "58                                                                  0d81711a3d73c828e8c6e12607eda82d-noL-11-20.sanitized.cnf                       SAT      663.4637122154236                        SAT     746.9651825428009\n",
      "59                                                                     e371cd7cdde3e7ccb9834290fd4a92d0-32_350.sanitized.cnf                       SAT      1356.385588169098                    TIMEOUT    1800.4542167186737\n",
      "60                                                                  de7a6b03999e2bc5bd750831c2662a4d-noL-11-18.sanitized.cnf                       SAT      1048.843889951706                        SAT    1626.6760551929474\n",
      "61                                                                4073757aae06fc2b50c043f088b132b4-simon-r19-1.sanitized.cnf                       SAT    0.06565594673156738                        SAT   0.06573343276977539\n",
      "62                                             f054205a7cef98e5021016f864c69816-summle_X11112_steps6_I1-2-2-4-4-8-25-100.cnf                       SAT      860.8580038547516                        SAT     1485.724224805832\n",
      "63                                                         3ed56242f55e3653dbceeb4a70221787-rbsat-v945c61409gyes9-sc2009.cnf                       SAT     1351.6575939655304                    TIMEOUT    1800.0076596736908\n",
      "64                                      0ac6aaf6db6a0e4ec3a06e865d01086f-pcmax-scheduling-m13-2011-12813-UNSAT.sanitized.cnf                     UNSAT      879.1315953731537                      UNSAT    1102.7353370189667\n",
      "65                                                                2fcd8533eba981967292f1b6e41f7433-simon-r20-0.sanitized.cnf                       SAT    0.06647515296936035                        SAT       0.1177978515625\n",
      "66                                                                d3893e43819a907055a84e48a6ee97ba-g2-ak128boothbg2msaig.cnf                       SAT      4.872403621673584                        SAT    5.9773170948028564\n",
      "67                                                       8e720686372c5037f30b4fc7b1c71d48-constraints_17_0.4_1.sanitized.cnf                       SAT     31.105908393859863                        SAT    176.34985542297363\n",
      "68                                                                     07e6413459f92b613498a719125b6239-j3037_10_mdd_bm1.cnf                     UNSAT       452.024973154068                      UNSAT    1322.3377676010132\n",
      "69                                                               bbfed8974655bca520259deb10d2347b-x9-09054.sat.sanitized.cnf                       SAT      68.65973567962646                        SAT    281.30061531066895\n",
      "70                                                               f296fe701a562022c0de0cf565fbca7d-x9-08014.sat.sanitized.cnf                     UNSAT     227.41920399665833                      UNSAT     285.0987012386322\n",
      "71                                                       fb0e505b8bd19a34f1f80b8e020e7856-constraints_17_0.4_2.sanitized.cnf                     UNSAT      247.4937083721161                      UNSAT     514.4041433334351\n",
      "72                                                               eddd68e14d69cce7190b99f4e7abdafb-x9-10098.sat.sanitized.cnf                       SAT      37.91703271865845                        SAT    415.11739683151245\n",
      "73                                                               57f4ea7ab160d996e38e69fac59869c4-x9-09047.sat.sanitized.cnf                     UNSAT      395.7560987472534                      UNSAT    413.28000497817993\n",
      "74                                                   2a53e9c6a25a50d753a94ead66065826-mp1-ps_5000_21250_3_0_0.8_0_1.50_6.cnf                     UNSAT      335.8668301105499                      UNSAT     340.5752010345459\n",
      "75                                                                e99e266b422513a2898c13898a1de501-rbsat-v760c43649gyes9.cnf                       SAT      548.6508481502533                        SAT     469.6948320865631\n",
      "76                                                       b4b41b2ff14427e5715cb9bee06d4602-constraints_17_0.3_2.sanitized.cnf                     UNSAT      217.8538875579834                      UNSAT    394.00223183631897\n",
      "77                       7ac7fabd8c078aea420087a0c80e5563-circuit_32in32out_with_400gates_6in6out_dist64_seed1.sanitized.cnf                       SAT     1597.1843464374542                    TIMEOUT     1800.005405664444\n",
      "78                                                     0fa9521ff633b27be11525a7b0f7d8b6-jgiraldezlevy.2200.9086.08.40.41.cnf                       SAT     1623.3516631126404  a non-zero exit -6 status    0.9676859378814697\n",
      "79                                                                      0d3fcbb89bfb8e821058ba3ea4284de1-j3045_10_gmto_b.cnf                       SAT      934.3283338546753                    TIMEOUT    1800.0054881572723\n",
      "80                                                               ded23680dfeab2879c05bc0e4de21126-x9-09014.sat.sanitized.cnf                     UNSAT      511.9489929676056                      UNSAT     639.3673434257507\n",
      "81                                                     8b31606e10656ff7eb2936262b647443-stable-300-0.1-20-98765432130020.cnf                       SAT     1331.5935943126678                        SAT    1267.4575281143188\n",
      "82                                                                           04e219c640ed59dc68ea2d50493de5b5-mp1-Nb5T15.cnf                       SAT     123.63361144065857                        SAT    188.81486463546753\n",
      "83                           1427381a809c64c721838894ece6756d-shuffling-2-s25242449-of-bench-sat04-727.used-as.sat04-753.cnf                       SAT      34.26231241226196                        SAT     612.4407312870026\n",
      "84                                                       8530c911b75ab1b276042043d118a875-constraints_16_0.4_1.sanitized.cnf                     UNSAT      419.1235044002533                      UNSAT     525.0124001502991\n",
      "85                                            878bcc11e1e243680f7946bcf428f465-linked_list_swap_contents_safety_unwind45.cnf                     UNSAT     469.18995666503906                    TIMEOUT    1802.0185735225677\n",
      "86                                                                      812926407774771b3bd9885f7bfa4841-lru_9.sanitized.cnf                       SAT      299.7694089412689                        SAT    298.11488366127014\n",
      "87                      e2deb375e56da2360ecd08fc1179fb7d-circuit_48in24out_with_100gates_7in7out_dist128_seed1.sanitized.cnf                       SAT       648.273307800293                    TIMEOUT    1800.0351660251617\n",
      "88                                                                      e85fb114c33f450dc2622c78bc6fa019-lru_7.sanitized.cnf                       SAT     230.57322311401367                        SAT    230.87739896774292\n",
      "89                                                               c6568fc8805127e876c4c23551bf49fa-x9-10076.sat.sanitized.cnf                     UNSAT     1775.3398451805115                      UNSAT    1577.3451402187347\n",
      "90                                                          aacfb8797097f698d14337d3a04f3065-barman-pfile06-022.sas.ex.7.cnf                     UNSAT      6.674400329589844                      UNSAT     35.36276412010193\n",
      "91                                                                3129198788f182ce6955b18aa3c7e61e-simon-r24-1.sanitized.cnf                       SAT    0.06597065925598145                        SAT   0.16776108741760254\n",
      "92                                              3bf8ba6bb4e4ea9ad08b1b058661ba2e-summle_X4044_steps7_I1-2-2-4-4-8-25-100.cnf                       SAT     1043.1441295146942                        SAT     1093.916494846344\n",
      "93                                                                         f561c52b987bdb9031a477864070b759-Ptn-7824-b18.cnf                       SAT     368.95574855804443                    TIMEOUT    1800.0041253566742\n",
      "94                                                       61069e0e3339af80000b774a041b7d96-constraints_18_0.3_2.sanitized.cnf                     UNSAT     1505.7670259475708                    TIMEOUT    1800.0043349266052\n",
      "95                                                               8b18bb75459a4161633ba2a3c8ee183e-x9-11062.sat.sanitized.cnf                       SAT     1765.6675164699554                        SAT    1783.5335357189178\n",
      "96                                            c03f7f84f6ac9e523c17e391c3895183-linked_list_swap_contents_safety_unwind68.cnf                     UNSAT      672.9688153266907                    TIMEOUT    1800.1523356437683\n",
      "97                                                                       be6411f4784a3c879886dda807cdc607-j3037_10_mdd_b.cnf                       SAT      795.9820137023926                        SAT    1688.4262206554413\n",
      "98                                                              e84b00b1f134e0e071a2baf54cfc90e8-ER_400_20_4.apx_2_DC-ST.cnf                     UNSAT     1583.1410853862762                    TIMEOUT    1800.0067157745361\n",
      "99                                                                 c801a020a6c8bc3c287fea495203b114-worker_20_40_20_0.95.cnf                       SAT        2.1688232421875                        SAT     2.269409656524658\n",
      "100                                                              5cb1ca8fe8a9d9125ea9accd498445f1-stb_531_83.apx_1_DC-ST.cnf                     UNSAT     1716.1208493709564                      UNSAT    1671.9876749515533\n",
      "101                                                              f45e5faf1bcccbdd3065dd6367c3bd16-x9-10083.sat.sanitized.cnf                     UNSAT     1713.8383798599243                      UNSAT    1702.7818467617035\n",
      "102                                                               7e1d279559b202016e5797901e731a39-simon-r25-0.sanitized.cnf                       SAT     0.0687403678894043                        SAT   0.11684298515319824\n",
      "103                                                               089f909e37b3ef0c4d90687f7e22b68f-simon-r18-0.sanitized.cnf                       SAT    0.06573867797851562                        SAT   0.06708645820617676\n",
      "104                                                                           ddc0720fa5a91d9cc0dc726644ab9e9f-6s167-opt.cnf                     UNSAT      233.6772174835205                      UNSAT    139.74547839164734\n",
      "105                                                             6c52aca5b5fd284bf1940b84df90a367-1-TC-256-K-71.sanitized.cnf                     UNSAT      1683.166949748993                    TIMEOUT    1800.0038049221039\n",
      "106                     f86dad4ba35369eb720a0c9ddc45037a-combined-crypto1-wff-seed-1-wffvars-450-cryptocplx-40-overlap-2.cnf                       SAT       131.290922164917                        SAT    50.035874366760254\n",
      "107                                                      9cd3acdb765c15163bc239ae3a57f880-FmlaEquivChain_4_6_6.sanitized.cnf                     UNSAT     1688.1374084949493  a non-zero exit -6 status    17.990535259246826\n",
      "108                                                       0876c518e5653369e20fb1ee0bb8db40-mp1-klieber2017s-0500-023-t12.cnf                       SAT      959.7066051959991                    TIMEOUT    1800.0055129528046\n",
      "109                                                                     aceab5a3452e2901e645065bda3e8847-lru_8.sanitized.cnf                       SAT      262.7683570384979                        SAT    263.57140731811523\n",
      "110                                                                      02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12.cnf                     UNSAT      41.82173252105713  a non-zero exit -6 status    1.4679875373840332\n",
      "111                                                                      576ebc1333f2c466c2dec98792721e1f-j3037_9_rggt_b.cnf                       SAT      487.0581126213074                    TIMEOUT    1800.0045711994171\n",
      "112                                                                     282a02a1743eb47c6b340e52ecce40a2-lru_6.sanitized.cnf                       SAT     197.57694101333618                        SAT     197.9287142753601\n",
      "113                                                               0cfe9c90d3a51435a5e4dba7634b882f-g2-ak128boothbg2msisc.cnf                       SAT      3.520162343978882                        SAT    4.7717835903167725\n",
      "114                      37ca184832fc6fa43a22ae900f1756a2-circuit_32in32out_with_350gates_6in6out_dist64_seed1.sanitized.cnf                       SAT     1092.5742709636688                    TIMEOUT    1800.0136387348175\n",
      "115                                                                    af750c18578d52e60472315692ad83c0-si2-b03m-m800-03.cnf                       SAT      80.06874203681946                        SAT    167.18946385383606\n",
      "116                                                               7cbc3ce2052ba7c5b501f75af58ab3c4-simon-r22-1.sanitized.cnf                       SAT    0.06535625457763672                        SAT   0.11594986915588379\n",
      "117                     adf6dacdd64c93f9de1aa0eadf427faa-circuit_48in64out_with_800gates_4in4out_dist128_seed1.sanitized.cnf                       SAT     358.69352293014526                    TIMEOUT    1800.0051696300507\n",
      "118                                                              7fb202a51c0223f3119887a57086ca4d-x9-09051.sat.sanitized.cnf                     UNSAT      529.2611603736877                      UNSAT     716.2665989398956\n",
      "119                                              ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf                       SAT    0.31643152236938477                        SAT    0.3671703338623047\n",
      "120                                                              a45a0358685867bd4f1c7f7c0b0e379c-x9-10014.sat.sanitized.cnf                       SAT     431.34886264801025                        SAT    366.20573234558105\n",
      "121                                     7f1c7da531539f3be4a1e0e916913086-pcmax-scheduling-m19-2974-16501-UNSAT.sanitized.cnf                     UNSAT       712.863198518753                      UNSAT    1041.5570814609528\n",
      "122                                                             05ed64e4e6229f446082752936768489-stb_495_168.apx_2_DC-AD.cnf                     UNSAT     1466.4253423213959                      UNSAT    1462.3832039833069\n",
      "123                                                      fb51311320bb42bdb893249998a77f40-constraints_16_0.3_1.sanitized.cnf                     UNSAT      821.0706450939178                      UNSAT     953.5022830963135\n",
      "124                                                                     f7e855b18105170b0718086d3f5b5923-j3045_10_rggt_b.cnf                       SAT      376.9145140647888                    TIMEOUT    1800.0048730373383\n",
      "125                                                      761651d4b6454108403bdefe22720fb3-constraints_18_0.5_2.sanitized.cnf                     UNSAT     1722.9544517993927                    TIMEOUT      1800.00874209404\n",
      "126                                                                    65c5a5d228ec5e52d1a72f918086f584-32_100.sanitized.cnf                       SAT      376.9617793560028                    TIMEOUT    1800.1359343528748\n",
      "127                                           aaa3654cc92d80da84ecf5d18040c634-linked_list_swap_contents_safety_unwind62.cnf                     UNSAT      623.0283586978912                    TIMEOUT    1800.1751267910004\n",
      "128                                                                    c214f1eb2576af00e3fb09f7d0305764-64_150.sanitized.cnf                       SAT      938.1148698329926                    TIMEOUT    1800.3779528141022\n",
      "129                                                             bd19da88800086971e554e9d66bc641d-stb_588_138.apx_1_DC-ST.cnf                     UNSAT     1314.1917536258698                      UNSAT    1522.2179458141327\n",
      "130                                                                          c0293283432a9b15cd743702163f5184-mp1-Nb7T42.cnf                       SAT     1040.5262396335602                    TIMEOUT    1800.0065824985504\n",
      "131                                                              b44ea915362c3a140269003d45b1d053-x9-11034.sat.sanitized.cnf                       SAT      200.6296353340149                    TIMEOUT    1800.0061156749725\n",
      "132                                                                    a14d9ee17051ec08a4334ba43089502c-64_200.sanitized.cnf                       SAT     1261.3492050170898                    TIMEOUT    1800.4685900211334\n",
      "133                                                              54c2da6d387a6f5ad6e014ae4d4decfc-x9-10038.sat.sanitized.cnf                       SAT      1277.369847536087                    TIMEOUT    1800.0038795471191\n",
      "134                                                             46d75199933980fdb856f13e5b1817dd-1-TC-256-K-64.sanitized.cnf                   TIMEOUT      1800.004067659378  a non-zero exit -6 status    300.32532501220703\n",
      "135                                                      63732c330adb8de8dd47ce1693b86d0e-FmlaEquivChain_4_8_8.sanitized.cnf                   TIMEOUT     1800.0052812099457  a non-zero exit -6 status    172.75074291229248\n",
      "136                                                                       83b330c934d6dd35d56e1b1ca3638b3c-j3037_1_mdd_b.cnf                   TIMEOUT     1800.0056850910187                        SAT     1302.574092388153\n",
      "137                                                              14e4cfcf0d83b2185fad41684d00d4dc-x9-12035.sat.sanitized.cnf                   TIMEOUT     1800.0044691562653                        SAT    1409.4695329666138\n",
      "138                                                             dd7164f2592c9675618c5c99af9b70f3-1-ZC-512-K-64.sanitized.cnf                   TIMEOUT     1800.0047583580017  a non-zero exit -6 status    1443.2060046195984\n",
      "139                                                  543e67dd5abc272c37775b1b742a1d9a-qwh.60.1728.shuffled-as.sat03-1659.cnf                   TIMEOUT      1800.115089893341                        SAT     1590.087703704834\n",
      "140                                                           35789168b67a74985804902008251269-tseitingrid6x200_shuffled.cnf                   TIMEOUT     1800.0079407691956                    TIMEOUT    1800.0054864883423\n",
      "141                                                              b7273af3d468ea2595f11a6dbd6ef6ce-x9-10007.sat.sanitized.cnf                   TIMEOUT     1800.0075409412384                    TIMEOUT    1800.0076081752777\n",
      "142                                                              3c6e1d1c4b8d3d08aa4c1df3805f4f7d-x9-10031.sat.sanitized.cnf                   TIMEOUT     1800.0090794563293                    TIMEOUT    1800.0111331939697\n",
      "143                                                      8d2dfc50c1759dc11a6564d5c368c6df-FmlaImplyChain_3_7_8.sanitized.cnf                   TIMEOUT     1800.0109267234802                    TIMEOUT    1800.0098173618317\n",
      "144                                                                  47e1ada0070708c2953d322c06aea00d-noL-11-8.sanitized.cnf                   TIMEOUT      1800.012532711029                    TIMEOUT    1800.0079078674316\n",
      "145                                                                      a08e66296d00f480e9ccadd79fa8b904-j3045_4_gmto_b.cnf                   TIMEOUT     1800.0101253986359                    TIMEOUT    1800.0089240074158\n",
      "146                                                       2d6d26e1c5b4f2763c03697a6d00d3cf-fixedbandwidth-eq-31_shuffled.cnf                   TIMEOUT     1800.0110790729523                    TIMEOUT    1800.0131556987762\n",
      "147                                                           45a09efb026036ff4b8d19024a7563a9-fermat-931960058139995587.cnf                   TIMEOUT       1800.01122713089                    TIMEOUT    1800.0105838775635\n",
      "148                                     74f145bb935650f5c982d7eec6967945-pcmax-scheduling-m43-38782-385402-SAT.sanitized.cnf                   TIMEOUT      1800.006305217743                    TIMEOUT    1800.0160808563232\n",
      "149                                                        8b91c5bf4dfd87ffb46c5ef88a3ef1cd-lec_mult_DvW_12x11.sanitized.cnf                   TIMEOUT     1800.0086603164673                    TIMEOUT    1800.0114388465881\n",
      "150                                                               5e5fe73a2e0ffc8e19873298566919fb-rbsat-v760c43649gyes3.cnf                   TIMEOUT     1800.0081992149353                    TIMEOUT    1800.0126576423645\n",
      "151                                                 bbfe2b27182d2ee7fefdb557f458ac9c-cliquecolouring_n21_k6_c5.sanitized.cnf                   TIMEOUT       1800.01029753685                    TIMEOUT    1800.0103979110718\n",
      "152                                                           24bdfb34654b1aa703fae31ea91e7c7f-tseitin_d3_n158.sanitized.cnf                   TIMEOUT     1800.0093567371368                    TIMEOUT    1800.0116515159607\n",
      "153                                                              1507d9812624b3e0eaf15e40100be020-x9-12014.sat.sanitized.cnf                   TIMEOUT     1800.0072178840637                    TIMEOUT    1800.0117304325104\n",
      "154                                                                 c73edc350cfa8e07af58db50054aea45-noL-11-12.sanitized.cnf                   TIMEOUT     1800.0096290111542                    TIMEOUT     1800.009258031845\n",
      "155                                                      36dbc206c7d9773bebb03f752070cee9-tseitin_grid_n11_m20.sanitized.cnf                   TIMEOUT     1800.0085356235504                    TIMEOUT    1800.0115849971771\n",
      "156                                                                     fae08ac5b6d7eeb4f56f1cfac6c51703-j3045_4_mdd_bm1.cnf                   TIMEOUT     1800.0079538822174                    TIMEOUT    1800.0128667354584\n",
      "157                                                            e85e793a66a2c9d390f40acf29a5346b-1-ET-512-K-110.sanitized.cnf                   TIMEOUT     1800.0071985721588                    TIMEOUT    1800.0480601787567\n",
      "158                                                              695e287a447fcdd924985a6e73057a38-rbsat-v1150c84314gyes1.cnf                   TIMEOUT     1800.0123970508575                    TIMEOUT    1800.0111801624298\n",
      "159                                                                     9a06700729bf3426beb8a62040f92960-mdp-36-12-unsat.cnf                   TIMEOUT      1800.011340379715                    TIMEOUT     1800.010308265686\n",
      "160                                                     c9f511adbb3d6e1b37b410f972f270c6-af-synthesis_stb_50_140_3_unsat.cnf                   TIMEOUT      1800.006919145584                    TIMEOUT    1800.0183947086334\n",
      "161                                                                     23daec2c71ce3c1e346e6042f3dc42eb-mdp-32-14-unsat.cnf                   TIMEOUT     1800.0201487541199                    TIMEOUT    1800.0126011371613\n",
      "162                                        f48735588e3515135f039e8bc8efaee5-asconhashv12_opt64_H11_M2-MxJOnbQIXNd_m5_6.c.cnf                   TIMEOUT     1800.0148327350616                    TIMEOUT     1800.019085407257\n",
      "163                                                             b11374f7fe60b9a63fbfde24b1a0a439-stb_418_125.apx_2_DC-AD.cnf                   TIMEOUT      1800.009774684906                    TIMEOUT     1800.016399383545\n",
      "164                                                                      91e0db01b254eb78f6643328045bfeb9-sted5_0x1e3-20.cnf                   TIMEOUT     1800.0090641975403                    TIMEOUT    1800.0226938724518\n",
      "165                                                              af05a6b68a1cff165b684d9ff0ae3b3b-x9-12098.sat.sanitized.cnf                   TIMEOUT     1800.0086359977722                    TIMEOUT    1800.0185742378235\n",
      "166                                                                     4f2d6659b5c9bf594ea12e3e1a85799f-mdp-32-11-unsat.cnf                   TIMEOUT     1800.0113537311554                    TIMEOUT    1800.0154645442963\n",
      "167                                                     1a1c9d1dc1ae7fcff92e3c3499864b75-af-synthesis_stb_50_200_4_unsat.cnf                   TIMEOUT      1800.007064819336                    TIMEOUT    1800.0306794643402\n",
      "168                                                     e7248b57a310ad461924eb17956cdf3a-Folkman-190-358004741.sanitized.cnf                   TIMEOUT     1800.0167758464813                    TIMEOUT    1800.0115804672241\n",
      "169                                                                           5690b9b0380aa9508699e56cae5918b5-170058440.cnf                   TIMEOUT     1800.0100417137146                    TIMEOUT     1800.011248588562\n",
      "170                                                             ff9e9b01d50d01ecb6740472d7dd36cd-1-ZC-512-K-61.sanitized.cnf                   TIMEOUT     1800.0212922096252                    TIMEOUT    1800.0327188968658\n",
      "171                                                          79b9e24dd9af185dbec18c9b0a32b1e2-g2-slp-synthesis-aes-top30.cnf                   TIMEOUT      1800.009337425232                    TIMEOUT      1800.02210354805\n",
      "172                                                                           d7dbb2cba940af33b2bce37ea99b8110-6s130-opt.cnf                   TIMEOUT      1800.009972333908                    TIMEOUT    1800.0168585777283\n",
      "173                                                               f32bb347996c351bc3e9a91c58e8601d-DLTM_twitter774_83_17.cnf                   TIMEOUT     1800.0142784118652                    TIMEOUT    1800.0204997062683\n",
      "174                                                              aa9b67fd19d54ad51b93ee4ba5dc75fc-x9-10002.sat.sanitized.cnf                   TIMEOUT      1800.007788181305                    TIMEOUT    1800.0165894031525\n",
      "175                                                               0ac5ff376b826f68d384ba05a1d00de0-sokoban-p20.sas.cr.33.cnf                   TIMEOUT     1800.0115821361542                    TIMEOUT    1800.0481548309326\n",
      "176                                                                     3593875d75d6e836a3ac328c5426c1b5-Break_18_32.xml.cnf                   TIMEOUT     1800.0110576152802                    TIMEOUT     1800.017255783081\n",
      "177                                                              5865fb9a6575d2ae6542c36ab96646a9-x9-11088.sat.sanitized.cnf                   TIMEOUT      1800.011411190033                    TIMEOUT    1800.0149846076965\n",
      "178                                                                       d1940d3d830eb16a7d2270ca4af4acba-mdp-28-11-sat.cnf                   TIMEOUT     1800.0095133781433                    TIMEOUT    1800.0153391361237\n",
      "179                                                             cccee4b0d4c70a5d5422663e2d8887da-1-ET-512-K-96.sanitized.cnf                   TIMEOUT      1800.014059305191                    TIMEOUT     1800.052708864212\n",
      "180                                                              2dd4b14a8a820e7baa073554ae2c6cb0-rphp_p8_r160.sanitized.cnf                   TIMEOUT     1800.0105421543121                    TIMEOUT    1800.0208339691162\n",
      "181                                                                cdd131110acc861a5a01fae6c4936c91-6g_6color_366_050_04.cnf                   TIMEOUT     1800.0732080936432                    TIMEOUT    1800.1296381950378\n",
      "182                                           f54b352934be08821aa75b3424112956-linked_list_swap_contents_safety_unwind54.cnf                   TIMEOUT      1800.157734155655                    TIMEOUT    1800.4804527759552\n",
      "183                                           0f262afd5b117000f8b4734d175aadab-linked_list_swap_contents_safety_unwind73.cnf                   TIMEOUT     1800.2487163543701                    TIMEOUT    1800.6609058380127\n",
      "184                                                                   093fa3ff9f7bc9979c43f9d2310ac21d-128_125.sanitized.cnf                   TIMEOUT      1800.570250749588                    TIMEOUT    1801.1004102230072\n",
      "185                                                                5952b9f18b73dabb531c60e3ac85d26f-ctl_4291_567_9_unsat.cnf                   TIMEOUT     1800.0048632621765                    TIMEOUT    1800.0038528442383\n",
      "186                                                1545660e72158c7e90da5ad4c877a124-hwmcc20miters-iso-rast-p06.sanitized.cnf                   TIMEOUT     1800.0045969486237                    TIMEOUT    1800.0038685798645\n",
      "187                                                                 29e61b1317804fb897e84237613d42bb-worker_30_60_25_0.9.cnf                   TIMEOUT     1800.0042805671692                    TIMEOUT     1800.004323720932\n",
      "188                                                              df1bd67978b9b0ec1d326ba174bc273c-x9-12021.sat.sanitized.cnf                   TIMEOUT     1800.0062482357025                    TIMEOUT    1800.0087413787842\n",
      "189                                                            6543793076c30fc7cdfa5a3c819cedc7-oisc-subrv-sll-nested-11.cnf                   TIMEOUT     1800.1452481746674  a non-zero exit -9 status     313.8017773628235\n",
      "190                                                             ea827f81bd0e9735739adc0d9da1b446-ER_500_30_3.apx_2_DC-AD.cnf                   TIMEOUT      1800.005981206894                    TIMEOUT    1800.0063571929932\n",
      "191                                                                         334aa882de28856fc8c75885285d2a3c-HCP-446-105.cnf                   TIMEOUT     1800.0911910533905  a non-zero exit -6 status      918.214837551117\n",
      "192                                                                            f746dae2050dd4aa48bcd53e8bd897b4-ex065_25.cnf                   TIMEOUT       1800.00546169281                    TIMEOUT    1800.0059242248535\n",
      "193                                        c6e03256680b96b52e3345a497333c2b-hwb-n24-02-S786928571.shuffled-as.sat03-1618.cnf                   TIMEOUT     1800.0436940193176                    TIMEOUT    1800.0059814453125\n",
      "194                                                             39277cab188349aee0f229cb7341b5c5-crafted_n12_d6_c4_num23.cnf                   TIMEOUT     1800.0413436889648                    TIMEOUT     1800.100216627121\n",
      "195                                                              0265448c232e3a25aa5bcd29b1b14567-x9-10051.sat.sanitized.cnf                   TIMEOUT     1800.0048019886017                    TIMEOUT     1800.004546403885\n",
      "196                                            25cc054738293c0d9377d7b72b78a341-string_compare_safety_cbmc_unwinding_900.cnf                   TIMEOUT     1800.1126971244812                    TIMEOUT    1800.1504995822906\n",
      "197                                                        385042a6042a0df5ddfec6e17f285ae7-lec_mult_CvD_11x11.sanitized.cnf                   TIMEOUT     1800.0043432712555                    TIMEOUT    1800.0047686100006\n",
      "198                                                                       8ffd718f763ed7a3f691cf46e57f8d98-mdp-32-10-sat.cnf                   TIMEOUT      1800.004384279251                    TIMEOUT    1800.0038657188416\n",
      "199                                                     b3167d999edd81291f33636464f2f8e6-Folkman-190-104806020.sanitized.cnf                   TIMEOUT     1800.0050957202911                    TIMEOUT     1800.004268169403\n",
      "200                                           e8abb9c83efdd96a99990f3b914ea873-linked_list_swap_contents_safety_unwind57.cnf                   TIMEOUT     1800.1500759124756                    TIMEOUT    1803.7073583602905\n",
      "201                                                       7429e380834066c206394139c9e1e17d-af-synthesis_stb_50_100_9_sat.cnf                   TIMEOUT     1800.0063030719757                        SAT    1604.1653547286987\n",
      "202                                                                     3184e52e950ac39095a801c418f68d50-mdp-28-10-unsat.cnf                   TIMEOUT     1800.0099649429321                    TIMEOUT     1800.020488500595\n",
      "203                                                                 2b60f9be1439f63b7f174c6b8ca7fedf-sgen1-unsat-121-100.cnf                   TIMEOUT     1800.0123744010925                    TIMEOUT     1800.013333082199\n",
      "204                                                                  05c8e94aaee86390eaf6e68dd3ec3570-noL-11-2.sanitized.cnf                   TIMEOUT     1800.0107855796814                    TIMEOUT    1800.0138263702393\n",
      "205                                                                     b172b4c218f1e44e205575d2b51e82c4-Schur_161_5_d38.cnf                   TIMEOUT     1800.0082597732544                    TIMEOUT     1800.010621547699\n",
      "206                                                             d7f2e4ec8e631387c928c79e1054c663-stb_418_125.apx_1_DC-AD.cnf                   TIMEOUT     1800.0080034732819                    TIMEOUT    1800.0100059509277\n",
      "207                                                     ef16970ab9da31165d3c401ff9b29168-Folkman-185-152478531.sanitized.cnf                   TIMEOUT     1800.0087826251984                    TIMEOUT    1800.0088748931885\n",
      "208                                                            16c5482d8e658b54e20d59cfd4b1d588-two-trees-511v.sanitized.cnf                   TIMEOUT     1800.0076081752777                    TIMEOUT    1800.0081150531769\n",
      "209                                                     170a7d847694ee4cfa5c421757672882-af-synthesis_stb_50_140_0_unsat.cnf                   TIMEOUT     1800.0136320590973                    TIMEOUT     1800.008761882782\n",
      "210                                                                     4e366e723d75fe39bf6db9a24ffb059b-Dodecahedron-k7.cnf                   TIMEOUT     1800.0093495845795                    TIMEOUT    1800.0103487968445\n",
      "211                                                    f70788e8446c1bc42684316de0a6c164-hwmcc20miters-iso-mul3.sanitized.cnf                   TIMEOUT     1800.0131158828735                    TIMEOUT    1800.0233108997345\n",
      "212                                                    1f44da3598ccf8bcbe9a90fb1b2f1ebd-REGRandom-K3-L3-Seed25.sanitized.cnf                   TIMEOUT     1800.0110368728638                    TIMEOUT     1800.018126487732\n",
      "213                                                             40737c2f46512069d54b0d2bb3b47a20-ER_400_20_4.apx_2_DC-AD.cnf                   TIMEOUT     1800.0077106952667                    TIMEOUT    1800.0071511268616\n",
      "214                                            7f2b4b2bde9b866b93fbfe01341213da-asconhashv12_opt64_H5_M2-xEJ8F_m0_3_U5.c.cnf                   TIMEOUT     1800.0109169483185                    TIMEOUT    1800.0150129795074\n",
      "215                                                                5a068ba62fe30e435a76e2ec80896ef4-ITC2021_Middle_1.xml.cnf                   TIMEOUT      1800.009156703949                    TIMEOUT    1800.0090975761414\n",
      "216                                    bfadd34e71516e95d0ba2a3a8fb2ba03-pcmax-scheduling-m19-10199-62102-UNSAT.sanitized.cnf                   TIMEOUT      1800.007277727127                    TIMEOUT    1800.0106518268585\n",
      "217                                                         96a6796405061571e12d383efe27b703-WS_500_16_70_10.apx_1_DC-ST.cnf                   TIMEOUT     1800.0088949203491                    TIMEOUT     1800.010935306549\n",
      "218                                                        5b0f16de2c5419b80b608db37d07d994-lec_mult_KvW_11x10.sanitized.cnf                   TIMEOUT     1800.0088500976562                    TIMEOUT     1800.012903213501\n",
      "219                                                                       ab3438b504b296fcee78ec9e71969863-mdp-36-14-sat.cnf                   TIMEOUT     1800.0113961696625                    TIMEOUT    1800.0130553245544\n",
      "220                                                        dacb03ea801d459ef08db23a8b1d104f-lec_mult_DvW_12x12.sanitized.cnf                   TIMEOUT     1800.0085892677307                    TIMEOUT    1800.0100836753845\n",
      "221                                  a552a058e6376a36b1f1b2724f228364-IBM_FV_2004_rule_batch_1_31_1_SAT_dat.k40.debugged.cnf                   TIMEOUT     1800.0167083740234                    TIMEOUT     1800.023086309433\n",
      "222                                                              5e1c11b77cdf3717b81b957120f0f477-x9-12001.sat.sanitized.cnf                   TIMEOUT     1800.0091943740845                    TIMEOUT    1800.0117404460907\n",
      "223                                      31bb67fe67e57dbeb8c4819b01892588-hwmcc17miters-xits-iso-oski15a08b08s.sanitized.cnf                   TIMEOUT     1800.0456476211548                    TIMEOUT    1800.0851690769196\n",
      "224                                                                     37a11ae189df2834a3f294736f34608f-mdp-28-16-unsat.cnf                   TIMEOUT     1800.0068855285645                    TIMEOUT    1800.0071558952332\n",
      "225                                           8d4b50ec0cf99097e9ab0835937afee5-hwmcc17miters-xits-iso-6s281b35.sanitized.cnf                   TIMEOUT     1800.0715672969818                    TIMEOUT    1800.1176509857178\n",
      "226                                                                 29ab150158d641568f888e8401d2ac13-bmc_QICE_snp_vld_30.cnf                   TIMEOUT     1800.1398565769196                    TIMEOUT    1800.1660089492798\n",
      "227                                              f0426369f61595aee97055965ee7e6a3-hwmcc12miters-xits-iso-6s111.sanitized.cnf                   TIMEOUT     1800.0275366306305                    TIMEOUT    1800.2376537322998\n",
      "228                                                                            88d151d4e3c6ba9bd78454de141cd108-T105.2.0.cnf                   TIMEOUT     1800.0738060474396                    TIMEOUT     1800.523600578308\n",
      "229                                                             cb2e8b7fada420c5046f587ea754d052-clique_n2_k10.sanitized.cnf                   TIMEOUT     1800.0071878433228                    TIMEOUT     1800.007176399231\n",
      "230                                                 768956cc8d1f2d18ae1929f6bb26557a-cliquecolouring_n13_k8_c7.sanitized.cnf                   TIMEOUT     1800.0064289569855                    TIMEOUT     1800.006587266922\n",
      "231                                                        e38601380bfbab35dd8c5927283533fb-lec_mult_CvK_11x11.sanitized.cnf                   TIMEOUT      1800.006745815277                    TIMEOUT    1800.0051217079163\n",
      "232                                                              1156429862d3c03160406d6d1a786f11-rphp_p8_r170.sanitized.cnf                   TIMEOUT     1800.0047788619995                    TIMEOUT    1800.0032229423523\n",
      "233                                                             4214f45042f8c504f9837afaec88bf8f-ER_400_20_7.apx_2_DC-AD.cnf                   TIMEOUT     1800.0045726299286                    TIMEOUT    1800.0037252902985\n",
      "234                                                       c76f3c6c2e2eae85fc5f3d499f3db88d-mp1-blockpuzzle_5x10_s7_free4.cnf                   TIMEOUT     1800.0045614242554                    TIMEOUT    1800.0053594112396\n",
      "235                                                        b76fa2f50a42120ec54fa98c29113326-lec_mult_DvK_12x12.sanitized.cnf                   TIMEOUT      1800.003846168518                    TIMEOUT    1800.0036499500275\n",
      "236                                                              fa5c6d6736a42650656c5bc018413254-bphp_p23_h22.sanitized.cnf                   TIMEOUT     1800.0037648677826                    TIMEOUT    1800.0043303966522\n",
      "237                                                    2b8a711debc3a6edd01e7b4e305342d9-REGRandom-K4-L2-Seed35.sanitized.cnf                   TIMEOUT     1800.0217907428741                    TIMEOUT    1800.0196797847748\n",
      "238                                                                     16ca57f2cdffd5b0260fc771017e1f04-mdp-36-10-unsat.cnf                   TIMEOUT     1800.0056760311127                    TIMEOUT    1800.0084965229034\n",
      "239                                                                           792495b57d6145fd21d7076eab2ab06a-grs-64-64.cnf                   TIMEOUT     1800.0069601535797                    TIMEOUT    1800.0337798595428\n",
      "240                                                      eed5189b73738270ae3fdb8b33bf31c8-Folkman-180-11710376.sanitized.cnf                   TIMEOUT     1800.0050301551819                    TIMEOUT    1800.0044820308685\n",
      "241                                                                          3b59c5d9b6729f39c8483446701f4ed0-g2-T93.2.1.cnf                   TIMEOUT     1800.1136856079102                    TIMEOUT    1800.0916602611542\n",
      "242                                           02f6ca52f1ada872d82035088701b66a-linked_list_swap_contents_safety_unwind69.cnf                   TIMEOUT     1800.2194530963898                    TIMEOUT    1800.3825170993805\n",
      "243                                                      461df1a7056560279d532bc2743022b6-Folkman-185-75415683.sanitized.cnf                   TIMEOUT     1800.0037274360657                    TIMEOUT    1800.0043659210205\n",
      "244                                                      14ea5857d75ae2b235a3ba97373ea732-af-synthesis_stb_50_40_9_unsat.cnf                   TIMEOUT      1800.004670381546                    TIMEOUT    1800.0046973228455\n",
      "245                                                1a20d903db74ab566efc68a35e6b0ec5-hwmcc20miters-iso-rast-p11.sanitized.cnf                   TIMEOUT      1800.108969926834  a non-zero exit -6 status     590.4017977714539\n",
      "246                                                             4a8285a53f30b35016b0c85ea17ba155-1-TC-256-K-68.sanitized.cnf                   TIMEOUT     1800.0039885044098  a non-zero exit -6 status     788.8074033260345\n",
      "247                                                                   41f4cb4992a481c9d43e2e1a4e2349ac-sgen1-sat-180-100.cnf                   TIMEOUT     1800.0051004886627                    TIMEOUT    1800.0051605701447\n",
      "248                                                             dd5fc8da5454a990a0e7999884158c64-1-ET-256-K-70.sanitized.cnf                   TIMEOUT     1800.0039749145508  a non-zero exit -6 status     849.2520611286163\n",
      "249                                                                           2c3c28f6d939d157e909c57a265fc908-mchess_17.cnf                   TIMEOUT      1800.004096031189                    TIMEOUT    1800.1265442371368\n",
      "250                                                         9e749596a8de36d8ab706c96cf128455-WS_500_16_70_10.apx_2_DC-AD.cnf                   TIMEOUT     1800.0044813156128                    TIMEOUT    1800.0048224925995\n",
      "251                                                            5ab3040d8617bf9f4d4cef4e56dcfd03-g2-hwmcc15deep-6s161-k17.cnf                   TIMEOUT     1800.0054912567139                    TIMEOUT    1800.0042088031769\n",
      "252                                                                     eda4aa84aeeb306468396fa82a6bba5a-pb_300_05_lb_17.cnf                   TIMEOUT     1800.0048322677612                    TIMEOUT    1800.0090322494507\n",
      "253                                           0d3160b80aa406bfc718c007265b9e73-linked_list_swap_contents_safety_unwind65.cnf                   TIMEOUT     1800.1917343139648                    TIMEOUT    1800.1841583251953\n",
      "254                                                           6a674acad2aeac729ecb39ad9e4a2298-1-ZC-1024-K-116.sanitized.cnf                   TIMEOUT     1800.0411665439606                    TIMEOUT     1800.041759967804\n",
      "255                                                        00d1fe07ab948b348bb3fb423b1ef40d-lec_mult_KvW_12x11.sanitized.cnf                   TIMEOUT     1800.0035409927368                    TIMEOUT    1800.0044915676117\n",
      "256                                                      77b7f7bbf75faaee28f473b9941de103-Folkman-185-19924337.sanitized.cnf                   TIMEOUT     1800.0042340755463                    TIMEOUT    1800.0042097568512\n",
      "257                                                            4517202fdd0ccc8a97d83684b021ea96-VanDerWaerden_2-3-14_186.cnf                   TIMEOUT      1800.003844499588                    TIMEOUT    1800.0042514801025\n",
      "258                                                       12b4a08e412a3bffb513ca65639c7c69-Folkman-175-7416734.sanitized.cnf                   TIMEOUT     1800.0040488243103                    TIMEOUT     1800.004737854004\n",
      "259                                                                     425c81fc3af7f5124b6282583d80df11-mdp-32-12-unsat.cnf                   TIMEOUT     1800.0038850307465                    TIMEOUT    1800.0052444934845\n",
      "260                                                       e067414e153ee98a7842bd6d6dafeee5-af-synthesis_stb_50_200_0_sat.cnf                   TIMEOUT     1800.0039057731628                    TIMEOUT    1800.0040700435638\n",
      "261                                                     94571e8a1081385029d1ecd53ffcdf8e-af-synthesis_stb_50_200_0_unsat.cnf                   TIMEOUT     1800.0044753551483                    TIMEOUT     1800.004585981369\n",
      "262                                                         08be288536c3178e6874a5676493923c-g2-hwmcc15deep-bob12s02-k16.cnf                   TIMEOUT     1800.0138289928436                    TIMEOUT    1800.0103588104248\n",
      "263                                                      82d9e8d2cfea101dfe52c8359b7bb163-af-synthesis_stb_50_40_2_unsat.cnf                   TIMEOUT     1800.0140142440796                    TIMEOUT    1800.0097031593323\n",
      "264                                   dafc03ea784fee849febca7b64230558-pcmax-scheduling-m30-14113-167638-UNSAT.sanitized.cnf                   TIMEOUT     1800.0161652565002                    TIMEOUT    1800.0108964443207\n",
      "265                                                      07b476c83150d3cb7e08f5d045e255ab-marg5x5.shuffled-as.sat03-1455.cnf                   TIMEOUT     1800.0058691501617                    TIMEOUT    1800.0069024562836\n",
      "266                                    a5419a63d913bde0ba5bcd8a8571342f-asconhashv12_opt64_H11_M2-tBi5i1RIgRz_m0_1_U23.c.cnf                   TIMEOUT     1800.0107471942902                    TIMEOUT    1800.0121607780457\n",
      "267                                                                     1e74212168124b696531fed471b480b1-mdp-32-10-unsat.cnf                   TIMEOUT     1800.0129826068878                    TIMEOUT     1800.008240699768\n",
      "268                                                               fb45d41807bcbbbe689de282c1310798-Break_unsat_16_27.xml.cnf                   TIMEOUT     1800.0114059448242                    TIMEOUT    1800.0103149414062\n",
      "269                                                                          9dcbf221b8d2bb01f30bcca283f3608d-EDP3-11000.cnf                   TIMEOUT      1800.025369644165                    TIMEOUT    1800.0133202075958\n",
      "270                                                     2a15a30186afdad41a49c5c5366d01be-Timetable_C_392_E_62_Cl_26_S_28.cnf                   TIMEOUT     1800.0288605690002                    TIMEOUT    1800.0238156318665\n",
      "271                                                    e06be7a69dbd6d6866a2799cd40c4bfe-hwmcc20miters-iso-mul7.sanitized.cnf                   TIMEOUT     1800.0312416553497                    TIMEOUT     1800.022893190384\n",
      "272                                                                               aa39c6d885d283a085d033ab7b89f8c8-urq45.cnf                   TIMEOUT     1800.0062091350555                    TIMEOUT    1800.0064039230347\n",
      "273                                                      f18c7c7b8666458fd04ea7a253ae1e20-FmlaImplyChain_3_7_7.sanitized.cnf                   TIMEOUT     1800.0078992843628                    TIMEOUT    1800.0089709758759\n",
      "274                                                                  994783ef4ed3a0366842e1b6f9128a6f-noL-11-6.sanitized.cnf                   TIMEOUT     1800.0072507858276                    TIMEOUT    1800.0117659568787\n",
      "275                                                    912ce8c21d8fb8abc491cb205ec23e9f-hwmcc20miters-iso-mul2.sanitized.cnf                   TIMEOUT       1800.00998878479                    TIMEOUT    1800.0097305774689\n",
      "276                                      62dcbf79f5ecef5618c9d9a00311326c-pcmax-scheduling-m13-1655-9604-UNSAT.sanitized.cnf                   TIMEOUT     1800.0064132213593                    TIMEOUT    1800.0064289569855\n",
      "277                                                 973d699ec01b88da869233a79aaa1912-cliquecolouring_n13_k9_c8.sanitized.cnf                   TIMEOUT     1800.0059502124786                    TIMEOUT    1800.0065925121307\n",
      "278                                        305ad7d773d6c72be9b1efbf7bf482e0-asconhashv12_opt64_H9_M2-MIC4kfhiA_m0_6_U2.c.cnf                   TIMEOUT     1800.0102446079254                    TIMEOUT    1800.0103106498718\n",
      "279                                           c21cea390a50204944aa00babd56b53c-linked_list_swap_contents_safety_unwind70.cnf                   TIMEOUT     1800.2510373592377                    TIMEOUT    1800.2428550720215\n",
      "280                                                                   84c6d7e4a18aacf166105aaa3cd6e3de-128_100.sanitized.cnf                   TIMEOUT     1800.5513908863068                    TIMEOUT     1800.613404750824\n",
      "281                                     6fc528fc3d0fd5a2c50992feb8bf0357-pcmax-scheduling-m24-17855-226744-SAT.sanitized.cnf                   TIMEOUT     1800.0096950531006                    TIMEOUT      1800.00657248497\n",
      "282                                                       773f3bd29e202ff700d8b5b459857a2c-Folkman-175-9054056.sanitized.cnf                   TIMEOUT     1800.0074005126953                    TIMEOUT    1800.0063285827637\n",
      "283                                                                               34fa0922e66aee3b6f6ffe1760f459d4-f9idw.cnf                   TIMEOUT     1800.0158479213715                    TIMEOUT    1800.0190916061401\n",
      "284                                                                         1b3c7ac17705be94daaddb5f5ae4816b-rook-56-0-0.cnf                   TIMEOUT     1800.0074570178986                    TIMEOUT    1800.0095896720886\n",
      "285                                                 67c533489a498495525efee429340958-cliquecolouring_n15_k9_c8.sanitized.cnf                   TIMEOUT     1800.0034093856812                    TIMEOUT     1800.004343032837\n",
      "286                                                        25447f441df2f134223d3e43319c99ad-lec_mult_CvW_12x11.sanitized.cnf                   TIMEOUT     1800.0036299228668                    TIMEOUT     1800.003727197647\n",
      "287                                                         55199d55fa9c3751a525d6c0a0e649bd-WS_500_16_90_70.apx_2_DC-AD.cnf                   TIMEOUT       1800.00443816185                    TIMEOUT     1800.004753112793\n",
      "288                                                        462d72f9b78ab1cd080667ff12a114ac-lec_mult_CvK_12x12.sanitized.cnf                   TIMEOUT     1800.0043365955353                    TIMEOUT    1800.0050191879272\n",
      "289                                                                     1548977b4e27032fe07cda357782cd6a-mdp-28-14-unsat.cnf                   TIMEOUT     1800.0040638446808                    TIMEOUT     1800.004992723465\n",
      "290                                                               8bf63d50ebdb645b75a24370b8f94d31-sokoban-p20.sas.cr.25.cnf                   TIMEOUT     1800.0045578479767                    TIMEOUT     1800.003642320633\n",
      "291                                                                     5411047bc24f4321f0e490e01e4a0910-goldcrest-and-9.cnf                   TIMEOUT     1800.0046145915985                    TIMEOUT    1800.0056953430176\n",
      "292                                                              b3c587501567db72e6e66c6930cf15ed-StConn_7_128.sanitized.cnf                   TIMEOUT     1800.0041346549988                    TIMEOUT    1800.0055329799652\n",
      "293                                             0e5ffe8651c6d9c3cccc3f6cb72be39a-g2-test_v5_r10_vr10_c1_s21502.smt2-cvc4.cnf                   TIMEOUT     1800.0135991573334                    TIMEOUT      1800.01304769516\n",
      "294                                                    1ea68d0008f01c50a1e4fec77dd0775e-REGRandom-K4-L1-Seed30.sanitized.cnf                   TIMEOUT      1800.007390499115                    TIMEOUT    1800.0115728378296\n",
      "295                                                              11cc532ecddfb10a47e0a869b4867c1b-Break_triple_20_36.xml.cnf                   TIMEOUT     1800.0058813095093                    TIMEOUT    1800.0080802440643\n",
      "296                                                                       db15a85651e0d941a11bf8640625edd8-mdp-32-11-sat.cnf                   TIMEOUT     1800.0034863948822                    TIMEOUT     1800.004471063614\n",
      "297                                       da929c2aefde5e59878ad87c4323e581-pcmax-scheduling-m12-8049-55035-SAT.sanitized.cnf                   TIMEOUT     1800.0045976638794                    TIMEOUT    1800.0038414001465\n",
      "298                                           1e0da402100982e53001a64281149dd9-linked_list_swap_contents_safety_unwind63.cnf                   TIMEOUT     1800.2303676605225                    TIMEOUT    1800.1943845748901\n",
      "299                                                           7db30d12cb06f0dc2f30abff80d96d6a-two-trees-1023v.sanitized.cnf                   TIMEOUT     1800.0057365894318                    TIMEOUT    1800.0057654380798\n",
      "300                                                             cd361d33986dccd7f2d86016d6c35241-ecarev-110-4099-22-30-7.cnf                   TIMEOUT      1800.012943983078  a non-zero exit -6 status     6.926194667816162\n",
      "301                                                                     1165e12b6addad01b491c4616306186c-mulhs016-sc2009.cnf                   TIMEOUT     1800.0046684741974                    TIMEOUT    1800.0056216716766\n",
      "302                                                              07c50b89d97a55b6e1629999a445a2ba-ctl_4201_555_unsat_pre.cnf                   TIMEOUT     1800.0053493976593                    TIMEOUT    1800.0723984241486\n",
      "303                      71ec94c233016219e12d671594dc88e5-circuit_32in32out_with_70gates_7in7out_dist128_seed1.sanitized.cnf                   TIMEOUT     1800.0046699047089                    TIMEOUT     1800.004799604416\n",
      "304                                                 18c0eb461bda29214bd43b84199a3b61-cliquecolouring_n31_k5_c4.sanitized.cnf                   TIMEOUT     1800.0037217140198                    TIMEOUT    1800.0046091079712\n",
      "305                                                                       d11944fbca2dd6540f18bd05b6ccda0c-mdp-32-14-sat.cnf                   TIMEOUT     1800.0046043395996                    TIMEOUT    1800.0045738220215\n",
      "306                                                                 907e7dc703e16ad2c7e56a33fe3b3e5f-manol-pipe-g10bid_i.cnf                   TIMEOUT     1800.0084805488586                    TIMEOUT    1800.0044522285461\n",
      "307                                                      479b1413f8bd1d6fc0723856ffca9792-constraints_18_0.4_2.sanitized.cnf                   TIMEOUT     1800.0060424804688                    TIMEOUT    1800.0043494701385\n",
      "308                               0fa8623240b5c14c1308f863a6dc88ab-urqh1c5x5.shuffled-as.sat03-1468.cnf.mis-103.debugged.cnf                   TIMEOUT     1800.0060958862305                    TIMEOUT     1800.004417181015\n",
      "309                                                                bd12a2b66110451b050bd5d2943b1854-ITC2021_Early_10.xml.cnf                   TIMEOUT     1800.0058891773224                    TIMEOUT    1800.0045583248138\n",
      "310                                                                     43e492bccfd57029b758897b17d7f04f-pb_300_09_lb_07.cnf                   TIMEOUT     1800.0054185390472                    TIMEOUT    1800.0046164989471\n",
      "311                                                              cd33a862b81ba7b887845d3510c41c62-MASG0_72_keystream76_0.cnf                   TIMEOUT     1800.0050444602966                    TIMEOUT     1800.004114151001\n",
      "312                                                                     44c25de7963c45e92a2407ad839f6e8b-Break_20_72.xml.cnf                   TIMEOUT     1800.0067625045776                    TIMEOUT    1800.0049397945404\n",
      "313                                                           aa6e6f67719ca43aef922f2ebabc409a-1-ZC-1024-K-117.sanitized.cnf                   TIMEOUT      1800.027450799942                    TIMEOUT     1800.032216310501\n",
      "314                                                                         6f0e236c999de0a5e3c46fd3463b1d0d-rook-56-1-1.cnf                   TIMEOUT     1800.0118396282196                    TIMEOUT    1800.0112943649292\n",
      "315                                                                  634a271f5fe339007a186539d615e92f-noL-11-4.sanitized.cnf                   TIMEOUT     1800.0052435398102                    TIMEOUT    1800.0053567886353\n",
      "316                                                        dd57f6ebf07a7f932a82bed1877788e6-lec_mult_KvW_10x10.sanitized.cnf                   TIMEOUT     1800.0043301582336                    TIMEOUT    1800.0038404464722\n",
      "317                                           6008b958ff6877f32c26b72728df4523-linked_list_swap_contents_safety_unwind76.cnf                   TIMEOUT     1800.2749636173248                    TIMEOUT     1800.215030670166\n",
      "318                                     5432e4dd44480fadbd66647db1750c80-pcmax-scheduling-m24-24102-255206-SAT.sanitized.cnf                   TIMEOUT     1800.0048141479492                    TIMEOUT    1800.0133407115936\n",
      "319                                                             29d86d9a3b85c349e10ad68d7a713da2-1-ZC-512-K-63.sanitized.cnf                   TIMEOUT       1800.01220703125                    TIMEOUT    1800.0163893699646\n",
      "320                                                                         59825ed96dc3bbdbd8789a4870b323ec-hcp_CP18_18.cnf                   TIMEOUT     1800.0137655735016                    TIMEOUT     1800.013833284378\n",
      "321                                                             2b032f8a8976a302ad125eb50a3e8445-1-ZC-512-K-60.sanitized.cnf                   TIMEOUT     1800.0175409317017                    TIMEOUT    1800.0151450634003\n",
      "322                                                             4d7f6efbae05b66ffbbd509a430d57f8-stb_418_125.apx_1_DC-ST.cnf                   TIMEOUT     1800.0070540904999                    TIMEOUT     1800.005304813385\n",
      "323                                                    02e4f23ed59566d343b605dc2cc30a01-REGRandom-K4-L3-Seed40.sanitized.cnf                   TIMEOUT     1800.0825581550598                    TIMEOUT    1800.0832915306091\n",
      "324                                                             f15ad4206d95a3f8f1358b33301539be-ER_400_20_7.apx_1_DC-AD.cnf                   TIMEOUT     1800.0063767433167                    TIMEOUT    1800.0067210197449\n",
      "325                                                                       a9cb77454c6cfdf4092fb304d3aae8b7-mdp-32-16-sat.cnf                   TIMEOUT     1800.0056025981903                    TIMEOUT    1800.0060226917267\n",
      "326                                                                                 26254890fa7107f85242ec9190da2a7a-002.cnf                   TIMEOUT     1800.0065813064575                    TIMEOUT    1800.0064826011658\n",
      "327  83dc8f675657b3e6d148ef5cf897fb82-openstacks-sequencedstrips-nonadl-nonnegated-os-sequencedstrips-p30_3.025-NOTKNOWN.cnf                   TIMEOUT     1800.0081856250763                    TIMEOUT    1800.0101170539856\n",
      "328                                                               94dd280b1562ee7dae44b303b8fed233-Break_unsat_18_31.xml.cnf                   TIMEOUT     1800.0052001476288                    TIMEOUT    1800.0043907165527\n",
      "329                                                       0bad2ce307bf5b68db26fa34e252c9d4-af-synthesis_stb_50_100_4_sat.cnf                   TIMEOUT     1800.0048887729645                    TIMEOUT    1800.0040547847748\n",
      "330                                                                              6077d4e5f755124ddc84068d6ccb630f-Nb44T6.cnf                   TIMEOUT      1800.034877538681                    TIMEOUT    1800.0362203121185\n",
      "331                                                                     cc30e4eedbcf9c3bc110894d08781246-j3037_9_mdd_bm1.cnf                   TIMEOUT      1800.004600763321                    TIMEOUT    1800.0045948028564\n",
      "332                                                              9d9c4fa425282759eb9e98b82fb5f56e-x9-12087.sat.sanitized.cnf                   TIMEOUT     1800.0045065879822                    TIMEOUT    1800.0049214363098\n",
      "333                                                            1e3c3e8d349759b8b482a6f2721762c4-apn-sbox5-cut3-symmbreak.cnf                   TIMEOUT     1800.0045111179352                    TIMEOUT    1800.0040345191956\n",
      "334                                                 ac347c21ca0759079c0be9a758e4e924-cliquecolouring_n41_k5_c4.sanitized.cnf                   TIMEOUT     1800.0046117305756                    TIMEOUT    1800.0044434070587\n",
      "335                                     32585e66749f42e161e94fc49fd64750-pcmax-scheduling-m26-6398-62377-UNSAT.sanitized.cnf                   TIMEOUT      1800.004522562027                    TIMEOUT     1800.004954814911\n",
      "336                                                                 52aaf653a79ca14fa1127cda32aa94ce-noL-11-10.sanitized.cnf                   TIMEOUT     1800.0065395832062                        SAT    1504.3001515865326\n",
      "337                                                            ee57608ea4b74f25aebca809d59711c9-oisc-subrv-sll-nested-13.cnf                   TIMEOUT      1800.213625907898  a non-zero exit -9 status    438.45024061203003\n",
      "338                                                        69e34d0f49c71a7e145d8104ebe0273f-sokoban-p16.sas.ex.15-sc2016.cnf                   TIMEOUT      1800.062406539917                    TIMEOUT    1800.1104607582092\n",
      "339                                                              c0e6e6eeebd48ca600cfc7d662fa804c-x9-10093.sat.sanitized.cnf                   TIMEOUT     1800.0043318271637                    TIMEOUT     1800.005212545395\n",
      "340                      8942dca5dc0876fc3f723f738d72de1c-circuit_32in32out_with_64gates_8in6out_dist128_seed2.sanitized.cnf                   TIMEOUT       1800.00989818573                    TIMEOUT     1800.011203289032\n",
      "341                                                                 d7627176604d5b86f34d2f5d14d8ee99-worker_40_80_40_0.9.cnf                   TIMEOUT       1800.00541305542                    TIMEOUT    1800.0187737941742\n",
      "342                                                     1b0f4ff7984b8d4cf2873200fb1680fb-af-synthesis_stb_50_140_1_unsat.cnf                   TIMEOUT     1800.0066018104553                    TIMEOUT    1800.0047974586487\n",
      "343                                     31e788a12ddc8b43ec20e77d53abaa23-pcmax-scheduling-m40-26287-324155-SAT.sanitized.cnf                   TIMEOUT      1800.006516456604                    TIMEOUT    1800.0368802547455\n",
      "344                                                                       a5dc6226e4c0bebb06926efe55640995-mdp-36-10-sat.cnf                   TIMEOUT     1800.0044329166412                    TIMEOUT    1800.0098848342896\n",
      "345                                                         44fd38bd0c5fc7336be468161dee4eda-post-cbmc-aes-ee-r3-noholes.cnf                   TIMEOUT     1800.0143971443176  a non-zero exit -6 status      436.597820520401\n",
      "346                                                             a917b02a7c31e74c8ccf8b8f001abf37-1-ZC-512-K-67.sanitized.cnf                   TIMEOUT     1800.0061678886414                    TIMEOUT    1800.0113060474396\n",
      "347                                     207b28c3cefc55b143259222cbfd4962-pcmax-scheduling-m37-28831-324346-SAT.sanitized.cnf                   TIMEOUT     1800.0052592754364                    TIMEOUT    1800.0069682598114\n",
      "348                                                             a6ed647f85f20be4aedd1e9ce31cbdcd-1-ET-256-K-55.sanitized.cnf                   TIMEOUT     1800.0055928230286                    TIMEOUT    1800.0061740875244\n",
      "349                                                                            897acc1858ce0887f286aba5c0d56d71-Nb13T165.cnf                   TIMEOUT     1800.0410432815552                    TIMEOUT    1800.0499694347382\n",
      "350                                                             4be4ae25aae88528bc10f8369bba86df-ER_400_20_4.apx_1_DC-AD.cnf                   TIMEOUT     1800.0046074390411                    TIMEOUT    1800.0042383670807\n",
      "351                                                             7dc0e1b5fedbd8351907f1316b58e68f-1-ZC-512-K-65.sanitized.cnf                   TIMEOUT     1800.0109589099884                    TIMEOUT    1800.0044286251068\n",
      "352                                                             67b4844c4bfbaf961cbbd79b953aa5c2-stb_588_138.apx_1_DC-AD.cnf                   TIMEOUT     1800.0070617198944                    TIMEOUT    1800.0075943470001\n",
      "353                      9e6c2e7b0d6f58e449716deb9305525a-circuit_32in32out_with_96gates_7in7out_dist128_seed1.sanitized.cnf                   TIMEOUT     1800.0151197910309                    TIMEOUT    1800.0049240589142\n",
      "354                                                     c82eee3badbc8432dad72d1d575a0ea6-preimage_80r_495m_160h_seed_379.cnf                   TIMEOUT     1800.0044779777527                    TIMEOUT    1800.0041344165802\n",
      "355                                                           c69c5163c9087ffe770743bd4cdafcc9-tseitin_d3_n162.sanitized.cnf                   TIMEOUT       1800.00434923172                    TIMEOUT    1800.0063939094543\n",
      "356                                         39fba35826ce8c87cd8e8de1969b2dd2-SGI_30_80_26_70_4-log.shuffled-as.sat03-208.cnf                   TIMEOUT      1800.004585981369                    TIMEOUT    1800.0044858455658\n",
      "357                                                        4c54efa61599e33d514bf718ff23ad09-lec_mult_CvK_12x11.sanitized.cnf                   TIMEOUT     1800.0167274475098                    TIMEOUT    1800.0033872127533\n",
      "358                                                               dcf5b8224d1e0748871c83ee10067255-2dlx_ca_bp_f_liveness.cnf                   TIMEOUT     1800.0280203819275                    TIMEOUT    1800.0774521827698\n",
      "359                                                        778e0fd8f034b7cfd539b1b523ed4c4a-lec_mult_CvW_12x12.sanitized.cnf                   TIMEOUT     1800.0043795108795                    TIMEOUT     1800.004276752472\n",
      "360                                                                          9777418f75e1dfbd7d34e9b0ba4bf0b5-grs-64-128.cnf                   TIMEOUT     1800.0117428302765                    TIMEOUT    1800.1061265468597\n",
      "361                                                                            e0bf26a7527cd162c7d3016d4b2ab9fb-md5_48_1.cnf                   TIMEOUT     1800.0053038597107                    TIMEOUT    1800.0057022571564\n",
      "362                                                         7fca438c38e5b7a70e38778ec146cfe0-ctl_4291_567_2_unsat-sc2013.cnf                   TIMEOUT      1800.006234407425                    TIMEOUT    1800.0052013397217\n",
      "363                                           573cc82ec61e01fd6e6308493ee56289-linked_list_swap_contents_safety_unwind80.cnf                   TIMEOUT      1800.254840373993                    TIMEOUT    1800.2287056446075\n",
      "364                                                               b3f53d4c67b234d27e21f5e56bfd39e8-StConn_8_32.sanitized.cnf                   TIMEOUT     1800.0054957866669                    TIMEOUT     1800.005299091339\n",
      "365                                                              697c96ac45534726c7dbd96faa11a86a-x9-11094.sat.sanitized.cnf                   TIMEOUT     1800.0066525936127                    TIMEOUT    1800.0045564174652\n",
      "366                                      7b48f9354d25521bd542d859c0ecdde9-hwmcc17miters-xits-iso-oski15a08b00s.sanitized.cnf                   TIMEOUT      1800.028576374054                    TIMEOUT    1800.0248413085938\n",
      "367                                                       d40a68825bdbcdd7642b249325a7b6a2-Folkman-180-5383714.sanitized.cnf                   TIMEOUT     1800.0054683685303                    TIMEOUT    1800.0060985088348\n",
      "368                                                        bb591c6447fb6b4180f6d288eb2ff62a-lec_mult_DvW_11x10.sanitized.cnf                   TIMEOUT     1800.0055477619171                    TIMEOUT       1800.0047955513\n",
      "369                                                               b087c195e5b7bb191d3a701aa0aa8cf1-Break_unsat_14_23.xml.cnf                   TIMEOUT     1800.0919642448425                    TIMEOUT    1800.0046854019165\n",
      "370                                                               16c27d738cb45b766b8823ca4f428cf0-rbsat-v760c43649gyes7.cnf                   TIMEOUT     1800.0042035579681                        SAT    1197.9142060279846\n",
      "371                                                     f16b7329ef5728f722c291156b3b098e-af-synthesis_stb_50_100_9_unsat.cnf                   TIMEOUT     1800.0048139095306                    TIMEOUT    1800.0044021606445\n",
      "372                                                                  0a27eb7c16c1e69ff4d087d217ac89cb-noL-11-0.sanitized.cnf                   TIMEOUT     1800.0074210166931                    TIMEOUT    1800.0064170360565\n",
      "373                                           b8f5fc2425facb3aec52a665f32a500c-asconhashv12_opt64_H6_M2-PgbpwX_m0_4_U1.c.cnf                   TIMEOUT     1800.0050172805786                    TIMEOUT    1800.0077352523804\n",
      "374                                         6f4df706246d29bf38adadd274fadab3-SGI_30_60_19_60_6-dir.shuffled-as.sat03-112.cnf                   TIMEOUT     1800.0045280456543                    TIMEOUT    1800.0045652389526\n",
      "375                                                             d5f6b2092795aad67d1df1fd3a329552-ER_400_20_7.apx_2_DC-ST.cnf                   TIMEOUT     1800.0046873092651                    TIMEOUT    1800.0064043998718\n",
      "376                                                        bdcdb52d16f87ea20f091ca4b0b6a36f-lec_mult_CvK_11x10.sanitized.cnf                   TIMEOUT     1800.0038576126099                    TIMEOUT    1800.0042386054993\n",
      "377                     fa50d69adc71fe4daf7f9088cd02864a-circuit_32in64out_with_150gates_6in6out_dist256_seed1.sanitized.cnf                   TIMEOUT     1800.0098285675049                    TIMEOUT    1800.0165367126465\n",
      "378                                                              09b61bbf19748094a7d896aac314ab36-x9-12063.sat.sanitized.cnf                   TIMEOUT      1800.004145860672                    TIMEOUT     1800.004239320755\n",
      "379                                                   0ef029cb7616d4a89eb6d1f63e89f67e-or_randxor_k3_n510_m510.sanitized.cnf                   TIMEOUT     1800.0049681663513                    TIMEOUT    1800.0043931007385\n",
      "380                                                           556cf803f9de4b2ec688825b5ff7e325-tseitin_d3_n174.sanitized.cnf                   TIMEOUT     1800.0062727928162                    TIMEOUT      1800.13290476799\n",
      "381                                                      30f0db845937bbda3ffde60e5ed4cb3f-Folkman-190-66337703.sanitized.cnf                   TIMEOUT     1800.0046396255493                    TIMEOUT    1800.0046226978302\n",
      "382                                                              195852083a05edee1902233698eec14a-x9-11077.sat.sanitized.cnf                   TIMEOUT      1800.005215883255                        SAT    1646.7619407176971\n",
      "383                                                        be18894105e006399cc018abc14b204f-af-synthesis_stb_50_40_9_sat.cnf                   TIMEOUT     1800.0048778057098                    TIMEOUT    1800.0039417743683\n",
      "384                                                              09c1b79b1cfe3522364fe60aef780703-x9-12092.sat.sanitized.cnf                   TIMEOUT     1800.0045328140259                    TIMEOUT    1800.0058319568634\n",
      "385                                                 096e485489025895cdc59887baafa08b-test_v7_r17_vr5_c1_s25451.smt2-cvc4.cnf                   TIMEOUT     1800.0072238445282                    TIMEOUT    1800.0076513290405\n",
      "386                                                                         2115182958f6cfb5a172a24f989b86dd-rook-42-0-1.cnf                   TIMEOUT     1800.0063209533691                    TIMEOUT    1800.0062725543976\n",
      "387                                                        a5fc113e7d4899f4e4af14b87b6fd6ae-Kakuro-easy-097-ext.xml.hg_4.cnf                   TIMEOUT     1800.1108994483948                    TIMEOUT     1800.109860420227\n"
     ]
    }
   ],
   "source": [
    "merged_df = no_gbc_df.merge(gbc_df, on='file_name', suffixes=('_no_gbc', '_gbc'))\n",
    "\n",
    "print(merged_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to generate dot plot\n",
    "def plot_dot_chart(result_df):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Define marker styles for different results\n",
    "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'TIMEOUT': 'D'}\n",
    "    \n",
    "    # Create plot with different colors and markers\n",
    "    for result_type, marker in marker_styles.items():\n",
    "        subset = result_df[result_df['result_gbc'] == result_type]\n",
    "        plt.scatter(pd.to_numeric(subset['time_limit_no_gbc']), pd.to_numeric(subset['time_limit_gbc']), label=result_type, marker=marker, alpha=0.7)\n",
    "    \n",
    "    # Set axis labels and title\n",
    "    plt.xlabel('Original Time')\n",
    "    plt.ylabel(f'Time with GBC')\n",
    "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
    "    \n",
    "    # Adjust x-axis and y-axis scales to show proper numbers\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks()\n",
    "    \n",
    "    # Add an x=y reference line\n",
    "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
    "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
    "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
    "    \n",
    "    # Customize grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
    "\n",
    "plot_dot_chart(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                file_name result_no_gbc   time_limit_no_gbc result_gbc      time_limit_gbc\n",
      "24                                      a38affaa741c958fc32769d5fe89b06c-frb65-12-2.used-as.sat04-874.cnf           SAT   261.4643633365631        SAT    97.6516604423523\n",
      "36                                            3a75ad246dbc750a7391ad887c5b0835-x9-11093.sat.sanitized.cnf           SAT  1246.2681241035461        SAT  252.96102213859558\n",
      "37                                               50019e4419d48196bb4b95933a8b5030-noL-11-14.sanitized.cnf           SAT   1144.640493631363        SAT  382.07967019081116\n",
      "39                                            915a25bd189357e4c6d7771b69a6849f-x9-09004.sat.sanitized.cnf         UNSAT   842.1606323719025      UNSAT   669.5578818321228\n",
      "44                                            c5a98231dd54cbca06135293bb7e1985-x9-11053.sat.sanitized.cnf           SAT    598.113231420517        SAT   411.8127737045288\n",
      "47                                                         8704094951693f99fd21403a039c8131-mchess_16.cnf         UNSAT  1424.6406893730164      UNSAT   580.1816735267639\n",
      "49    6f7a0e1cf94b6b26eafc08a827a692ce-circuit_64in64out_with_64gates_8in5out_dist256_seed1.sanitized.cnf           SAT  376.67070746421814        SAT   189.1146879196167\n",
      "104                                                        ddc0720fa5a91d9cc0dc726644ab9e9f-6s167-opt.cnf         UNSAT   233.6772174835205      UNSAT  139.74547839164734\n",
      "106  f86dad4ba35369eb720a0c9ddc45037a-combined-crypto1-wff-seed-1-wffvars-450-cryptocplx-40-overlap-2.cnf           SAT    131.290922164917        SAT  50.035874366760254\n",
      "136                                                    83b330c934d6dd35d56e1b1ca3638b3c-j3037_1_mdd_b.cnf       TIMEOUT  1800.0056850910187        SAT   1302.574092388153\n",
      "137                                           14e4cfcf0d83b2185fad41684d00d4dc-x9-12035.sat.sanitized.cnf       TIMEOUT  1800.0044691562653        SAT  1409.4695329666138\n",
      "370                                            16c27d738cb45b766b8823ca4f428cf0-rbsat-v760c43649gyes7.cnf       TIMEOUT  1800.0042035579681        SAT  1197.9142060279846\n"
     ]
    }
   ],
   "source": [
    "improved_df = merged_df[\n",
    "    (pd.to_numeric(merged_df['time_limit_gbc']) <= 0.8 * pd.to_numeric(merged_df['time_limit_no_gbc'])) &\n",
    "    (merged_df['result_gbc'].isin(['SAT', 'UNSAT']))\n",
    "]\n",
    "\n",
    "print(improved_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "from debugger.proof_analyzer import ProofAnalyzer\n",
      "from debugger.tree_parser import *\n",
      " 1/2: p = ProofAnalyzer(\"example_proof_4.13.0.proof\")\n",
      " 1/3: p = ProofAnalyzer(\"../example_proof_4.13.0.proof\")\n",
      " 1/4: print(p.get_qi_counts())\n",
      " 1/5:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      " 1/6: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      " 1/7: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      " 1/8:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      " 1/9: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      "1/10: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      "1/11: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      "1/12:\n",
      "from debugger.proof_analyzer import ProofAnalyzer\n",
      "from debugger.tree_parser import *\n",
      "1/13: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      " 3/1:\n",
      "from debugger.proof_analyzer import ProofAnalyzer\n",
      "from debugger.tree_parser import *\n",
      " 3/2: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      " 3/3: p = ProofAnalyzer(\"../example_proof_4.13.0.proof\")\n",
      " 4/1:\n",
      "from debugger.proof_analyzer import ProofAnalyzer\n",
      "from debugger.tree_parser import *\n",
      " 4/2: p = ProofAnalyzer(\"../example_proof_4.13.0.proof\")\n",
      " 4/3:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      " 4/4: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      " 4/5:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      " 5/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,120), edgecolor='black')\n",
      "    plt.title(f'Histogram of {output_file}')\n",
      "    plt.xlabel('Value')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      " 5/2:\n",
      "folder_path = 'results/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      " 5/3:\n",
      "output_file = folder_path[7:] + \"_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      " 5/4:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title(f'Histogram of {output_file}')\n",
      "    plt.xlabel('Value')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      " 5/5: plot_histogram(all_numbers, output_file, max_range = 10)\n",
      " 6/1:\n",
      "from debugger.proof_analyzer import ProofAnalyzer\n",
      "from debugger.tree_parser import *\n",
      " 6/2: p = ProofAnalyzer(\"../example_proof_4.8.5.proof\")\n",
      " 6/3:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      " 6/4: p = ProofAnalyzer(\"../example_proof_4.13.0.proof\")\n",
      " 6/5:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      " 6/6:\n",
      "# p = ProofAnalyzer(\"../example_proof_4.13.0.proof\")\n",
      "p = ProofAnalyzer(\"../example_proof_correct_proof\")\n",
      " 6/7:\n",
      "# p = ProofAnalyzer(\"../example_proof_4.13.0.proof\")\n",
      "p = ProofAnalyzer(\"../example_proof_correct.proof\")\n",
      " 6/8:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      " 7/1:\n",
      "output_file = folder_path[9:] + \"_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      " 7/2:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title(f'Histogram of {output_file}')\n",
      "    plt.xlabel('Value')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      " 7/3:\n",
      "folder_path = 'results/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      " 7/4:\n",
      "output_file = folder_path[9:] + \"_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      " 7/5:\n",
      "output_file = folder_path[9:] + \"_histogram2.png\"\n",
      "plot_histogram(all_numbers, output_file, max_range = 10)\n",
      " 8/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title(f'Size of Globally blocked clauses')\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      " 8/2:\n",
      "folder_path = 'results/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      " 8/3:\n",
      "output_file = folder_path[9:] + \"_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      " 8/4:\n",
      "output_file = folder_path[9:] + \"_histogram2.png\"\n",
      "plot_histogram(all_numbers, output_file, max_range = 10)\n",
      "10/1:\n",
      "from debugger.proof_analyzer import ProofAnalyzer\n",
      "from debugger.tree_parser import *\n",
      "10/2:\n",
      "# p = ProofAnalyzer(\"../example_proof_4.13.0.proof\")\n",
      "p = ProofAnalyzer(\"../dbg/b6f117b674/proofs/rename.4647448081623179666.proof\")\n",
      "10/3:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      "10/4:\n",
      "# p = ProofAnalyzer(\"../example_proof_4.13.0.proof\")\n",
      "p = ProofParser(\"../example_proof_4.13.0.proof\")\n",
      "10/5:\n",
      "# p = ProofAnalyzer(\"../dbg/b6f117b674/proofs/rename.4647448081623179666.proof\")\n",
      "p = ProofParser(\"../dbg/b6f117b674/proofs/rename.4647448081623179666.proof\")\n",
      "10/6:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      "10/7:\n",
      "p = ProofAnalyzer(\"../dbg/b6f117b674/proofs/rename.4647448081623179666.proof\")\n",
      "# p = ProofParser(\"../dbg/b6f117b674/proofs/rename.4647448081623179666.proof\")\n",
      "10/8:\n",
      "qi_counts = p.get_qi_counts()\n",
      "\n",
      "print(qi_counts)\n",
      "print(sum(qi_counts.values()))\n",
      "11/1:\n",
      "import os\n",
      "os.chdir(\"/home/yizhou7/mariposa\")\n",
      "\n",
      "from debugger.demo_utils import *\n",
      "from pandas import DataFrame \n",
      "from debugger3 import Debugger3\n",
      "from debugger.bench_viewer import BenchViewer\n",
      "import numpy as np\n",
      "11/2:\n",
      "import os\n",
      "# os.chdir(\"/home/yizhou7/mariposa\")\n",
      "\n",
      "from debugger.demo_utils import *\n",
      "from pandas import DataFrame \n",
      "from debugger3 import Debugger3\n",
      "from debugger.bench_viewer import BenchViewer\n",
      "import numpy as np\n",
      "13/1:\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "13/2:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "13/3:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "13/4:\n",
      "from tabulate import tabulate\n",
      "from src.utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "14/1:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "14/2:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "14/3:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"../data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "14/4:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "14/5:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "14/6:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "15/1:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "15/2:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "15/3:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(load_cache)\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "15/4:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "15/5:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "15/6:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "17/1:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "17/2:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "17/3:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"cache/{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "18/1:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "18/2:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"cache/{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "19/1:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "19/2:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"cache/{get_name_hash(file)}.report\")#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "21/1:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "21/2:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"../cache/{get_name_hash(file)}.report\", relativize=True)#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "22/1:\n",
      "from tabulate import tabulate\n",
      "from utils.cache_utils import load_cache\n",
      "import sys\n",
      "\n",
      "def get_name_hash(filename):\n",
      "    import hashlib\n",
      "    # TODO: this should probably do fine?\n",
      "    return hashlib.sha256(filename.encode()).hexdigest()[0:10]\n",
      "22/2:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"../cache/{get_name_hash(file)}.report\", relativize=True)#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "22/3:\n",
      "file_name = \"d_fvbkv--Betree-BetreeInv.i.dfy.Impl__BetreeInv.__default.InitImpliesInv.smt2\"\n",
      "\n",
      "file= f\"data/projs/bench_unstable/base.z3/{file_name}\"\n",
      "\n",
      "\n",
      "print(get_name_hash(file))\n",
      "report = load_cache(f\"../cache/{get_name_hash(file)}.report\", relativize=False)#load_cache(\"cc1d9c148f.report\")\n",
      "\n",
      "print(report)\n",
      "\n",
      "extended = report.freq.merge(report.tested, on=\"qname\")\n",
      "print(extended.columns)\n",
      "extended[\"rank\"] = extended[\"trace_count\"].rank(method='min', ascending=False)\n",
      "extended[\"stabilized\"] = extended[\"qname\"].isin(report.stabilized[\"qname\"])\n",
      "extended = extended.sort_values(by=\"trace_count\", ascending=False)\n",
      "extended = extended[[\"rank\", \"qname\", \"trace_count\",  'proof_count', \"stabilized\", 'action', 'result', \"time\"]]#, \"edit_path\"]]\n",
      "print(tabulate(extended, headers='keys', tablefmt='psql'))\n",
      "24/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title(f'Size of Globally blocked clauses')\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "24/2:\n",
      "folder_path = 'results_original_short/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      "24/3:\n",
      "output_file = folder_path[9:] + \"_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      "24/4:\n",
      "output_file = \"propagation_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      "24/5:\n",
      "output_file = folder_path[9:] + \"_histogram2.png\"\n",
      "plot_histogram(all_numbers, output_file, max_range = 10)\n",
      "24/6:\n",
      "output_file = folder_path[9:] + \"_histogram2.png\"\n",
      "plot_histogram(all_numbers, output_file, max_range = 10)\n",
      "24/7:\n",
      "output_file = \"propagation_histogram2.png\"\n",
      "plot_histogram(all_numbers, output_file, max_range = 10)\n",
      "24/8:\n",
      "from statistics import mean, median\n",
      "output_file = \"propagation_histogram2.png\"\n",
      "plot_histogram(all_numbers, output_file, max_range = 10)\n",
      "\n",
      "print(f\"Mean: {mean(all_numbers)}\")\n",
      "print(f\"Median: {median(all_numbers)}\")\n",
      "24/9:\n",
      "print(f\"Mean: {mean(all_numbers)}\")\n",
      "print(f\"Median: {median(all_numbers)}\")\n",
      "\n",
      "output_file = \"propagation_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      "24/10:\n",
      "folder_path = 'results_bcp_short/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      "24/11:\n",
      "output_file = \"bcp_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      "24/12:\n",
      "output_file = \"bcp_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file)\n",
      "\n",
      "print(f\"Mean: {mean(all_numbers)}\")\n",
      "print(f\"Median: {median(all_numbers)}\")\n",
      "25/1:\n",
      "folder_path = 'results_noshrink_short/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      "25/2:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "25/3:\n",
      "folder_path = 'results_original_short/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      "25/4:\n",
      "print(f\"Mean: {mean(all_numbers)}\")\n",
      "print(f\"Median: {median(all_numbers)}\")\n",
      "\n",
      "output_file = \"propagation_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file, title = \" using propagation to shrink\")\n",
      "25/5:\n",
      "output_file = \"propagation_histogram2.png\"\n",
      "plot_histogram(all_numbers, output_file, max_range = 10, title =\" using propagation to shrink\")\n",
      "25/6:\n",
      "folder_path = 'results_bcp_short/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      "25/7:\n",
      "output_file = \"bcp_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file, title = \" using binary clauses to shrink\")\n",
      "\n",
      "print(f\"Mean: {mean(all_numbers)}\")\n",
      "print(f\"Median: {median(all_numbers)}\")\n",
      "25/8:\n",
      "folder_path = 'results_noshrink_short/global_clauses'  # Change to your folder path\n",
      "# folder_paths = ['global_clauses/random_false_learn_false', 'global_clauses/random_true_learn_false', 'global_clauses/random_false_learn_true', 'global_clauses/random_true_learn_true']\n",
      "# for folder_path in folder_paths:\n",
      "all_numbers = process_files_in_folder(folder_path)\n",
      "print(f'Total numbers found: {len(all_numbers)}')\n",
      "25/9:\n",
      "output_file = \"noshrink_histogram.png\"\n",
      "plot_histogram(all_numbers, output_file, title = \" without shrinking\")\n",
      "\n",
      "print(f\"Mean: {mean(all_numbers)}\")\n",
      "print(f\"Median: {median(all_numbers)}\")\n",
      "25/10:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file {(.*?)} returned {(.*?)} in time {(.*?)}!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "df = extract_results(\"run_short_original.txt\")\n",
      "print(df)\n",
      "28/1:\n",
      "import os\n",
      "os.chdir(\"/home/yizhou7/mariposa\")\n",
      "\n",
      "from debugger.demo_utils import *\n",
      "from debugger3 import Debugger3\n",
      "from debugger.tree_node import NodeRef\n",
      "\n",
      "from pandas import DataFrame \n",
      "import networkx as nx\n",
      "28/2:\n",
      "import os\n",
      "# os.chdir(\"/home/yizhou7/mariposa\")\n",
      "\n",
      "from debugger.demo_utils import *\n",
      "from debugger3 import Debugger3\n",
      "from debugger.tree_node import NodeRef\n",
      "\n",
      "from pandas import DataFrame \n",
      "import networkx as nx\n",
      "30/1:\n",
      "cadical_df = extract_results(\"slurm-29194003.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(cadical_df.to_string())\n",
      "30/2:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "30/3:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"run_short_original.txt\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "30/4:\n",
      "cadical_df = extract_results(\"slurm-29194003.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(cadical_df.to_string())\n",
      "31/1: print(len(['02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_86.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_83.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_87.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_65.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_32.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_34.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_60.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_22.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_203.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_28.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_16.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_168.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_118.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_155.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_137.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_4.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_98.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_250.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_19.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_41.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_39.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_134.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_127.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_76.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_69.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_106.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_15.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_147.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_113.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_9.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_40.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_73.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_44.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_49.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_42.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_91.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_37.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_23.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_93.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_27.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_72.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_89.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_120.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_107.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_174.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_55.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_12.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_47.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_110.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_46.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_61.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_123.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_108.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_84.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_176.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_121.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_112.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_260.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_175.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_258.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_52.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_154.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_99.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_25.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_125.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_111.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_220.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_5.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_94.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_66.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_133.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_17.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_261.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_11.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_126.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_158.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_77.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_117.cnf', '02066c116dbacc40ec5cca2067db26c0-mrpp_4x4#12_12_163.cnf']))\n",
      "35/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "35/2:\n",
      "# with the non-trivial globally blocked clause\n",
      "cadical_df = extract_results(\"results_original_no_chrono.txt\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(cadical_df.to_string())\n",
      "35/3:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"run_short_original.txt\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "35/4:\n",
      "# with the non-trivial globally blocked clause\n",
      "cadical_df = extract_results(\"results_original_no_chrono.txt\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(cadical_df.to_string())\n",
      "36/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "36/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"run_short_original.txt\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "36/3:\n",
      "# with the non-trivial globally blocked clause\n",
      "cadical_df = extract_results(\"slurm-29366589.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(cadical_df.to_string())\n",
      "42/1:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "42/2:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "42/3:\n",
      "bcp_df = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(bcp_df.to_string())\n",
      "42/4:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(bcp_df.to_string())\n",
      "42/5:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "42/6:\n",
      "gbc_df2 = extract_results(\"slurm-29478374.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df2.to_string())\n",
      "42/7:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = global_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = global_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set(original_df[\"file_name\"])\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "42/8:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set(original_df[\"file_name\"])\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "42/9:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set(original_df[\"file_name\"])\n",
      "print(original_files)\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "42/10:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set([file.replace(\".cnf\", \"\") for file in original_df[\"file_name\"]])\n",
      "print(original_files)\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "42/11:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "42/12:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "42/13:\n",
      "gbc_df2 = extract_results(\"slurm-29478374.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df2.to_string())\n",
      "42/14:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set([file.replace(\".cnf\", \"\") for file in original_df[\"file_name\"]])\n",
      "print(original_files)\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "43/1:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "43/2:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "43/3:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "43/4:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "43/5:\n",
      "gbc_df2 = extract_results(\"slurm-29478374.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df2.to_string())\n",
      "43/6:\n",
      "gbc_df2 = extract_results(\"slurm-29478374.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df2.to_string())\n",
      "43/7:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set([file.replace(\".cnf\", \"\") for file in original_df[\"file_name\"]])\n",
      "print(original_files)\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "43/8:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "global_df1[\"file_name\"] = global_df1[\"file_name\"].str.split(\"/\").str[0]\n",
      "global_df2[\"file_name\"] = global_df2[\"file_name\"].str.split(\"/\").str[0]\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"].str.split(\"/\").str[0]\n",
      "\n",
      "# Merge global_df1 and global_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(global_df1, global_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "# Find files that are in merged_df but not in original_df\n",
      "missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# Display the result\n",
      "print(missing_files)\n",
      "43/9:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0]\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0]\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"].str.split(\"/\").str[0]\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "# Find files that are in merged_df but not in original_df\n",
      "missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# Display the result\n",
      "print(missing_files)\n",
      "43/10:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"].str.split(\"/\").str[0]\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "# Find files that are in merged_df but not in original_df\n",
      "missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# Display the result\n",
      "print(missing_files)\n",
      "43/11:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"].str.split(\"/\").str[0]\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "# Find files that are in merged_df but not in original_df\n",
      "missing_files = merged_df[merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# Display the result\n",
      "print(missing_files)\n",
      "43/12:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"].str.split(\"/\").str[0]\n",
      "\n",
      "print(gbc_df2[\"file_name\"])\n",
      "print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "# Find files that are in merged_df but not in original_df\n",
      "missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# Display the result\n",
      "print(missing_files)\n",
      "43/13:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"].str.split(\"/\").str[0]\n",
      "\n",
      "print(gbc_df2[\"file_name\"])\n",
      "print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "# Find files that are in merged_df but not in original_df\n",
      "missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# Display the result\n",
      "print(missing_files)\n",
      "43/14:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"]\n",
      "\n",
      "print(gbc_df2[\"file_name\"])\n",
      "print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "merged_df2 = pd.merge(merged_df, original_df, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "print(merged_df2.to_string())\n",
      "# # Find files that are in merged_df but not in original_df\n",
      "# missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# # Display the result\n",
      "# print(missing_files)\n",
      "43/15:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"]\n",
      "\n",
      "# print(gbc_df2[\"file_name\"])\n",
      "# print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "merged_df2 = pd.merge(merged_df, original_df, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "print(merged_df2.to_string())\n",
      "# # Find files that are in merged_df but not in original_df\n",
      "# missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# # Display the result\n",
      "# print(missing_files)\n",
      "43/16:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] \n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] \n",
      "original_df[\"file_name\"] = original_df[\"file_name\"]\n",
      "\n",
      "# print(gbc_df2[\"file_name\"])\n",
      "# print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "merged_df2 = pd.merge(merged_df, original_df, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "print(merged_df2.to_string())\n",
      "# # Find files that are in merged_df but not in original_df\n",
      "# missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# # Display the result\n",
      "# print(missing_files)\n",
      "44/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "44/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "44/3:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "44/4:\n",
      "gbc_df2 = extract_results(\"slurm-29478374.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df2.to_string())\n",
      "44/5:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set([file.replace(\".cnf\", \"\") for file in original_df[\"file_name\"]])\n",
      "print(original_files)\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "44/6:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] \n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] \n",
      "original_df[\"file_name\"] = original_df[\"file_name\"]\n",
      "\n",
      "# print(gbc_df2[\"file_name\"])\n",
      "# print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "merged_df2 = pd.merge(merged_df, original_df, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "print(merged_df2.to_string())\n",
      "# # Find files that are in merged_df but not in original_df\n",
      "# missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# # Display the result\n",
      "# print(missing_files)\n",
      "45/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "45/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "45/3:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "45/4:\n",
      "gbc_df2 = extract_results(\"slurm-29478374.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df2.to_string())\n",
      "45/5:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set([file.replace(\".cnf\", \"\") for file in original_df[\"file_name\"]])\n",
      "print(original_files)\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "45/6:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"]\n",
      "\n",
      "# print(gbc_df2[\"file_name\"])\n",
      "# print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "merged_df2 = pd.merge(merged_df, original_df, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "print(merged_df2.to_string())\n",
      "# # Find files that are in merged_df but not in original_df\n",
      "# missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# # Display the result\n",
      "# print(missing_files)\n",
      "46/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "46/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "46/3:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "46/4:\n",
      "gbc_df2 = extract_results(\"slurm-29478374.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df2.to_string())\n",
      "46/5:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set([file.replace(\".cnf\", \"\") for file in original_df[\"file_name\"]])\n",
      "print(original_files)\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "46/6:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"mutant_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"mutant_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"]\n",
      "\n",
      "# print(gbc_df2[\"file_name\"])\n",
      "# print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "merged_df2 = pd.merge(merged_df, original_df, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "print(merged_df2.to_string())\n",
      "# # Find files that are in merged_df but not in original_df\n",
      "# missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# # Display the result\n",
      "# print(missing_files)\n",
      "47/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "47/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "47/3:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "47/4:\n",
      "gbc_df2 = extract_results(\"slurm-29478374.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df2.to_string())\n",
      "47/5:\n",
      "import pandas as pd\n",
      "\n",
      "# Extract unique file names before the first \"/\" in global_df1 and global_df2\n",
      "global_files1 = gbc_df1[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "global_files2 = gbc_df2[\"file_name\"].str.split(\"/\").str[0].unique()\n",
      "\n",
      "# Combine unique file names from both dataframes\n",
      "global_files = set(global_files1) | set(global_files2)\n",
      "\n",
      "# Extract unique file names before the first \"/\" in original_df\n",
      "original_files = set([file.replace(\".cnf\", \"\") for file in original_df[\"file_name\"]])\n",
      "print(original_files)\n",
      "\n",
      "# Find files that are in global_df1 or global_df2 but not in original_df\n",
      "missing_files = global_files - original_files\n",
      "\n",
      "# Print or inspect the missing files\n",
      "print(\"Files in global_df1 or global_df2 but not in original_df:\", missing_files)\n",
      "47/6:\n",
      "import pandas as pd\n",
      "\n",
      "# Ensure file_name only contains everything before the first \"/\"\n",
      "gbc_df1[\"mutant_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[1] + \".cnf\"\n",
      "gbc_df2[\"mutant_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[1] + \".cnf\"\n",
      "\n",
      "gbc_df1[\"file_name\"] = gbc_df1[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "gbc_df2[\"file_name\"] = gbc_df2[\"file_name\"].str.split(\"/\").str[0] + \".cnf\"\n",
      "original_df[\"file_name\"] = original_df[\"file_name\"]\n",
      "\n",
      "# print(gbc_df2[\"file_name\"])\n",
      "# print(original_df[\"file_name\"] )\n",
      "\n",
      "# Merge global_df1 and gbc_df2 on file_name, keeping separate columns\n",
      "merged_df = pd.merge(gbc_df1, gbc_df2, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "merged_df2 = pd.merge(merged_df, original_df, on=\"file_name\", how=\"outer\", suffixes=(\"_df1\", \"_df2\"))\n",
      "\n",
      "print(merged_df2.to_string())\n",
      "# # Find files that are in merged_df but not in original_df\n",
      "# missing_files = merged_df[~merged_df[\"file_name\"].isin(original_df[\"file_name\"])]\n",
      "\n",
      "# # Display the result\n",
      "# print(missing_files)\n",
      "48/1:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out (.*?) in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "48/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "49/1:\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "49/2:\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "49/3:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "49/4:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "49/5:\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "49/6:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "49/7:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "50/1:\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "50/2:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "50/3:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "50/4:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "50/5:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "50/6:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.str.split(\"/\")\n",
      "                file_name, mutant_name = string_split.str[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\" \"result\", \"time_limit\"])\n",
      "    return df\n",
      "50/7:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "50/8:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\" \"result\", \"time_limit\"])\n",
      "    return df\n",
      "50/9:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "50/10:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\" \"result\", \"time_limit\"])\n",
      "    return df\n",
      "50/11:\n",
      "gbc_df1 = extract_results(\"slurm-29478370.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df1.to_string())\n",
      "51/1:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "51/2:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                print(string_split)\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\" \"result\", \"time_limit\"])\n",
      "    return df\n",
      "51/3:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "54/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "54/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "54/3:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\" \"result\", \"time_limit\"])\n",
      "    return df\n",
      "54/4:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "54/5:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                print(string_split)\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\"\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\" \"result\", \"time_limit\"])\n",
      "    return df\n",
      "54/6:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "54/7:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\" \"result\", \"time_limit\"])\n",
      "    return df\n",
      "54/8:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "54/9:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "54/10:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "54/11:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "print(globally_blocked_clauses_dfs[\"gbc6\"])\n",
      "54/12:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "print(globally_blocked_clauses_dfs[\"gbc6\"].to_string())\n",
      "54/13:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "54/14:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "54/15:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "54/16:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, \"TIMEOUT\", time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "54/17:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "54/18:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "# print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "54/19:\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['column']\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string)\n",
      "54/20:\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['column']\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string)\n",
      "54/21:\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string)\n",
      "54/22:\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/23:\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        best_performance_mutant = combined_df.loc[blocked_times.idxmin(), 'mutant_name']\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/24:\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        best_performance_mutant = combined_df.loc[blocked_times.idxmin(), 'mutant_name']\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/25:\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        best_performance_mutant = combined_df.loc[blocked_times.min(), 'mutant_name']\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/26:\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        best_performance_mutant = combined_df.loc[blocked_times.idxmin(), 'mutant_name']\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/27:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty and blocked_times.notna().any():\n",
      "            best_performance_mutant = combined_df.loc[blocked_times.idxmin(), 'mutant_name']\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/28:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty and blocked_times.notna().any():\n",
      "            best_performance_mutant = combined_df.loc[blocked_times.idxmin(), 'mutant_name']\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/29:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty and blocked_times.notna().any():\n",
      "            print(blocked_times)\n",
      "            best_performance_mutant = combined_df.loc[blocked_times.idxmin(), 'mutant_name']\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/30:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        import pandas as pd\n",
      "\n",
      "# Function to process and validate data\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit'].dropna()\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# Example usage\n",
      "# result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "\n",
      "\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/31:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        import pandas as pd\n",
      "\n",
      "# Function to process and validate data\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit'].dropna()\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "# Example usage\n",
      "# result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "\n",
      "\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/32:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/33:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df)\n",
      "54/34:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        min_time = blocked_times.min()\n",
      "        max_time = blocked_times.max()\n",
      "        median_time = blocked_times.median()\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "54/35:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "54/36:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        print(blocked_times)\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "54/37:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "55/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "55/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "55/3:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, \"TIMEOUT\", time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "55/4:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "# print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "55/5:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "55/6:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "55/7:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "55/8:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result):\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "        if 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = 'ALWAYS TIMEOUT'\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "55/9:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = \n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "55/10:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "55/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    plt.legend()\n",
      "    plt.grid(True, linestyle='--', alpha=0.6)\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "55/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    plt.legend()\n",
      "    plt.grid(True, linestyle='--', alpha=0.6)\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "56/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "56/3:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, \"TIMEOUT\", time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "56/4:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "# print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "56/5:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    plt.legend()\n",
      "    plt.grid(True, linestyle='--', alpha=0.6)\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    plt.legend()\n",
      "    plt.grid(True, linestyle='--', alpha=0.6)\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    plt.legend()\n",
      "    plt.grid(True, linestyle='--', alpha=0.6)\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([min_val, max_val], [min_val, max_val], linestyle='--', color='gray', alpha=0.7, label='x = y')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 0], [1800, 1800], linestyle='--', color='gray', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(20, 6))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 0], [1800, 1800], linestyle='--', color='gray', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    # plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 0], [1800, 1800], linestyle='--', color='gray', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 0], [1800, 1800], linestyle='--', color='gray', alpha=0.7, label='x=y')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 0], [1800, 1800], linestyle='--', color='black', alpha=0.7, label='x=y')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='x=y')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'ALWAYS TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 100], [0, 1800], linestyle='--', color='black', alpha=0.7, label='x=y')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 100], [0, 1800], linestyle='--', color='black', alpha=0.7, label='x=y')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/18:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limi'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/19:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 100], [0, 1800], linestyle='--', color='black', alpha=0.7, label='x=y')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time')\n",
      "    plt.title('Dot Plot of Original Time vs Min Time')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/23:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = pd.to_numeric(original_row['result'], errors ='coerce')\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        # if not blocked_times.empty:\n",
      "        #     best_index = blocked_times.idxmin()\n",
      "        #     if best_index in combined_df.index:\n",
      "        #         best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "        #     else:\n",
      "        #         best_performance_mutant = None\n",
      "        # else:\n",
      "        #     best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time with GBC')\n",
      "    plt.title('Dot Plot of Original Formula Time vs Min Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/25:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = pd.to_numeric(original_row['result'], errors ='coerce')\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/26:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/27:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/28:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/29:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['mutant_name']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scrambilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scrambilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "results_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq. smallest gbc length']] = results_df.apply(process_row, axis=1)\n",
      "56/30:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['mutant_name']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scrambilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scrambilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq. smallest gbc length']] = results_df.apply(process_row, axis=1)\n",
      "56/31:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['mutant_name']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scrambilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scrambilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq. smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/32:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scrambilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scrambilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq. smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/33:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq. smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/34:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq. smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/35:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/36:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/37:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/38:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        num_solutions = len(combined_df)\n",
      "        \n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/39:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            # Find all indices with the minimum time limit\n",
      "            min_time = blocked_times.min()\n",
      "            best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "            if not best_indexes.empty:\n",
      "                # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "                best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "            else:\n",
      "                best_performance_mutants = None\n",
      "        else:\n",
      "            best_performance_mutants = None\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/40:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            # Find all indices with the minimum time limit\n",
      "            min_time = blocked_times.min()\n",
      "            best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "            if not best_indexes.empty:\n",
      "                # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "                best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "            else:\n",
      "                best_performance_mutants = None\n",
      "        else:\n",
      "            best_performance_mutants = None\n",
      "\n",
      "        print(best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/41:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            # Find all indices with the minimum time limit\n",
      "            min_time = blocked_times.min()\n",
      "            best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "            if not best_indexes.empty:\n",
      "                # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "                best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "            else:\n",
      "                best_performance_mutants = None\n",
      "        else:\n",
      "            best_performance_mutants = None\n",
      "\n",
      "        if len(best_performance_mutants) == 1:\n",
      "            best_performance_mutant = best_performance_mutants[0]\n",
      "        else:\n",
      "            raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/42:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            # Find all indices with the minimum time limit\n",
      "            min_time = blocked_times.min()\n",
      "            best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "            if not best_indexes.empty:\n",
      "                # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "                best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "            else:\n",
      "                best_performance_mutants = None\n",
      "        else:\n",
      "            best_performance_mutants = None\n",
      "\n",
      "        if len(best_performance_mutants) == 1:\n",
      "            best_performance_mutant = best_performance_mutants[0]\n",
      "        else:\n",
      "            raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/43:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            # Find all indices with the minimum time limit\n",
      "            min_time = blocked_times.min()\n",
      "            best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "            if not best_indexes.empty:\n",
      "                # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "                best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "            else:\n",
      "                best_performance_mutants = None\n",
      "        else:\n",
      "            best_performance_mutants = None\n",
      "\n",
      "        if len(best_performance_mutants) == 1:\n",
      "            best_performance_mutant = best_performance_mutants[0]\n",
      "        else:\n",
      "            print(blocked_times)\n",
      "            print(min_time)\n",
      "            raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/44:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit', 'mutant_name'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            # Find all indices with the minimum time limit\n",
      "            min_time = blocked_times.min()\n",
      "            best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "            if not best_indexes.empty:\n",
      "                # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "                best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "            else:\n",
      "                best_performance_mutants = None\n",
      "        else:\n",
      "            best_performance_mutants = None\n",
      "\n",
      "        if len(best_performance_mutants) == 1:\n",
      "            best_performance_mutant = best_performance_mutants[0]\n",
      "        else:\n",
      "            print(blocked_times)\n",
      "            print(min_time)\n",
      "            raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/45:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            # Find all indices with the minimum time limit\n",
      "            min_time = blocked_times.min()\n",
      "            best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "            if not best_indexes.empty:\n",
      "                # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "                best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "            else:\n",
      "                best_performance_mutants = None\n",
      "        else:\n",
      "            best_performance_mutants = None\n",
      "\n",
      "        if len(best_performance_mutants) == 1:\n",
      "            best_performance_mutant = best_performance_mutants[0]\n",
      "        else:\n",
      "            print(blocked_times)\n",
      "            print(min_time)\n",
      "            raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(best_performance_mutant, type(best_performance_mutant))\n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/46:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            # 'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/47:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/48:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/49:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/50:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    print(filtered_clauses_path)\n",
      "    smallest_gbc_length = min(line_lengths)\n",
      "    freq_smallest_gbc_length = line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/51:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\")\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/52:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/53:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time with GBC')\n",
      "    plt.title('Dot Plot of Original Formula Time vs Min Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/54:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['median_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Median Time with GBC')\n",
      "    plt.title('Dot Plot of Original Formula Time vs Median Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/55:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['max_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Max Time with GBC')\n",
      "    plt.title('Dot Plot of Original Formula Time vs Max Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/56:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    print(filtered_clauses_path)\n",
      "    smallest_gbc_length = \"N/A\" if line_lengths == [] else min(line_lengths)\n",
      "    freq_smallest_gbc_length = \"N/A\" if line_lengths == [] else line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "result_df[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/57: print(result_df.to_string())\n",
      "56/58: print(result_df.to_string())\n",
      "56/59:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "56/60:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "56/61:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, \"TIMEOUT\", time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "56/62:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "# print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "56/63:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/64:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['min_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Min Time with GBC')\n",
      "    plt.title('Dot Plot of Original Formula Time vs Min Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/65:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['median_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Median Time with GBC')\n",
      "    plt.title('Dot Plot of Original Formula Time vs Median Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/66:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset['max_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel('Max Time with GBC')\n",
      "    plt.title('Dot Plot of Original Formula Time vs Max Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "\n",
      "plot_dot_chart(result_df)\n",
      "56/67:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    print(filtered_clauses_path)\n",
      "    smallest_gbc_length = \"N/A\" if line_lengths == [] else min(line_lengths)\n",
      "    freq_smallest_gbc_length = \"N/A\" if line_lengths == [] else line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "\n",
      "result_df_new_info = result_df\n",
      "result_df_new_info[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/68:\n",
      "print(result_df_new_info.to_string())\n",
      "\n",
      "result_df_new_info.to_csv(\"results_3.6.csv\")\n",
      "56/69:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df, mode):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset[f'{mode}_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'{mode} Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs {mode} Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    plt.savefig(f'original_vs_{mode}_gbc_3.6.png')\n",
      "\n",
      "plot_dot_chart(result_df, 'min')\n",
      "56/70:\n",
      "\n",
      "\n",
      "plot_dot_chart(result_df, 'median')\n",
      "56/71:\n",
      "\n",
      "\n",
      "plot_dot_chart(result_df, 'max')\n",
      "56/72:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "56/73:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    # print(filtered_clauses_path)\n",
      "    smallest_gbc_length = \"N/A\" if line_lengths == [] else min(line_lengths)\n",
      "    freq_smallest_gbc_length = \"N/A\" if line_lengths == [] else line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    # print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "\n",
      "result_df_new_info = result_df\n",
      "result_df_new_info[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "56/74:\n",
      "print(result_df_new_info.to_string())\n",
      "\n",
      "result_df_new_info.to_csv(\"results_3.6.csv\")\n",
      "56/75:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df, mode):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset[f'{mode}_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'{mode} Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs {mode} Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'original_vs_{mode}_gbc_3.6.png')\n",
      "\n",
      "plot_dot_chart(result_df, 'min')\n",
      "56/76:\n",
      "\n",
      "\n",
      "plot_dot_chart(result_df, 'median')\n",
      "56/77:\n",
      "\n",
      "\n",
      "plot_dot_chart(result_df, 'max')\n",
      "57/1: original_dfs_iterated = {}\n",
      "57/2:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "57/3:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "57/4:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, \"TIMEOUT\", time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "57/5:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "# print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "57/6:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "57/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df, mode):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset[f'{mode}_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'{mode} Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs {mode} Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'original_vs_{mode}_gbc_3.6.png')\n",
      "\n",
      "plot_dot_chart(result_df, 'min')\n",
      "57/8:\n",
      "\n",
      "\n",
      "plot_dot_chart(result_df, 'median')\n",
      "57/9:\n",
      "\n",
      "\n",
      "plot_dot_chart(result_df, 'max')\n",
      "57/10:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    # print(filtered_clauses_path)\n",
      "    smallest_gbc_length = \"N/A\" if line_lengths == [] else min(line_lengths)\n",
      "    freq_smallest_gbc_length = \"N/A\" if line_lengths == [] else line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    # print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "\n",
      "result_df_new_info = result_df\n",
      "result_df_new_info[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "57/11:\n",
      "print(result_df_new_info.to_string())\n",
      "\n",
      "result_df_new_info.to_csv(\"results_3.6.csv\")\n",
      "57/12: original_dfs_iterated = {}\n",
      "57/13:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs_iterated):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs_iterated\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs_iterated.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time_orii': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, original_dfs_iterated)\n",
      "print(result_df.to_string())\n",
      "57/14:\n",
      "original_dfs_iterated = {\n",
      "    \"df0\": \"slurm-29487980.out\",\n",
      "    \"df1\": \"slurm-29487981.out\",\n",
      "    \"df2\": \"slurm-29487982.out\",\n",
      "    \"df3\": \"slurm-29487983.out\",\n",
      "    \"df4\": \"slurm-29487984.out\",\n",
      "    \"df5\": \"slurm-29487985.out\",\n",
      "    \"df6\": \"slurm-29487986.out\",\n",
      "    \"df7\": \"slurm-29487988.out\",\n",
      "    \"df8\": \"slurm-29487989.out\",\n",
      "    \"df9\": \"slurm-29487990.out\",\n",
      "}\n",
      "57/15:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs_iterated):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs_iterated\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs_iterated.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, original_dfs_iterated)\n",
      "print(result_df.to_string())\n",
      "57/16:\n",
      "original_files = {\n",
      "    \"df0\": \"slurm-29487980.out\",\n",
      "    \"df1\": \"slurm-29487981.out\",\n",
      "    \"df2\": \"slurm-29487982.out\",\n",
      "    \"df3\": \"slurm-29487983.out\",\n",
      "    \"df4\": \"slurm-29487984.out\",\n",
      "    \"df5\": \"slurm-29487985.out\",\n",
      "    \"df6\": \"slurm-29487986.out\",\n",
      "    \"df7\": \"slurm-29487988.out\",\n",
      "    \"df8\": \"slurm-29487989.out\",\n",
      "    \"df9\": \"slurm-29487990.out\",\n",
      "}\n",
      "\n",
      "original_dfs = {}\n",
      "\n",
      "for key in original_files.keys():\n",
      "    df = extract_results_gbc(original_files[key])\n",
      "    original_dfs[key] = df\n",
      "57/17:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs_iterated):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs_iterated\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs_iterated.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, original_dfs_iterated)\n",
      "print(result_df.to_string())\n",
      "57/18:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, original_dfs)\n",
      "print(result_df.to_string())\n",
      "57/19:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "57/20:\n",
      "original_files = {\n",
      "    \"df0\": \"slurm-29487980.out\",\n",
      "    \"df1\": \"slurm-29487981.out\",\n",
      "    \"df2\": \"slurm-29487982.out\",\n",
      "    \"df3\": \"slurm-29487983.out\",\n",
      "    \"df4\": \"slurm-29487984.out\",\n",
      "    \"df5\": \"slurm-29487985.out\",\n",
      "    \"df6\": \"slurm-29487986.out\",\n",
      "    \"df7\": \"slurm-29487988.out\",\n",
      "    \"df8\": \"slurm-29487989.out\",\n",
      "    \"df9\": \"slurm-29487990.out\",\n",
      "}\n",
      "\n",
      "original_dfs = {}\n",
      "\n",
      "for key in original_files.keys():\n",
      "    df = extract_results_gbc(original_files[key])\n",
      "    original_dfs[key] = df\n",
      "\n",
      "print(original_dfs[\"df6\"])\n",
      "57/21:\n",
      "original_files = {\n",
      "    \"df0\": \"slurm-29487980.out\",\n",
      "    \"df1\": \"slurm-29487981.out\",\n",
      "    \"df2\": \"slurm-29487982.out\",\n",
      "    \"df3\": \"slurm-29487983.out\",\n",
      "    \"df4\": \"slurm-29487984.out\",\n",
      "    \"df5\": \"slurm-29487985.out\",\n",
      "    \"df6\": \"slurm-29487986.out\",\n",
      "    \"df7\": \"slurm-29487988.out\",\n",
      "    \"df8\": \"slurm-29487989.out\",\n",
      "    \"df9\": \"slurm-29487990.out\",\n",
      "}\n",
      "\n",
      "original_dfs = {}\n",
      "\n",
      "for key in original_files.keys():\n",
      "    df = extract_results_gbc(original_files[key])\n",
      "    original_dfs[key] = df\n",
      "\n",
      "print(original_dfs[\"df6\"].to_string())\n",
      "57/22:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        print(combined_df.to_string())\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "58/1:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        print(combined_df.to_string())\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append(original_row + {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "59/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "59/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "59/3:\n",
      "import re\n",
      "def extract_results_gbc(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                string_split = file_name.split(\"/\")\n",
      "                file_name, mutant_name = string_split[0] + \".cnf\", string_split[1]\n",
      "                data.append((file_name, mutant_name, \"TIMEOUT\", time_limit))\n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"mutant_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "59/4:\n",
      "globally_blocked_clause_files = {\n",
      "    \"gbc0\": \"slurm-29479444.out\",\n",
      "    \"gbc1\": \"slurm-29479445.out\",\n",
      "    \"gbc2\": \"slurm-29479447.out\",\n",
      "    \"gbc3\": \"slurm-29479448.out\",\n",
      "    \"gbc4\": \"slurm-29479449.out\",\n",
      "    \"gbc5\": \"slurm-29479450.out\",\n",
      "    \"gbc6\": \"slurm-29479451.out\",\n",
      "    \"gbc7\": \"slurm-29479554.out\",\n",
      "    \"gbc8\": \"slurm-29479453.out\",\n",
      "    \"gbc9\": \"slurm-29479454.out\",\n",
      "}\n",
      "\n",
      "globally_blocked_clauses_dfs = {}\n",
      "\n",
      "for key in globally_blocked_clause_files.keys():\n",
      "    df = extract_results_gbc(globally_blocked_clause_files[key])\n",
      "    globally_blocked_clauses_dfs[key] = df\n",
      "\n",
      "# print(globally_blocked_clauses_dfs[\"gbc7\"].to_string())\n",
      "59/5:\n",
      "\n",
      "def process_and_validate_data(original_df, globally_blocked_clauses_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from globally_blocked_clauses_dfs\n",
      "        blocked_entries = []\n",
      "        for df in globally_blocked_clauses_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append({\n",
      "            'file_name': file_name,\n",
      "            'result': final_result,\n",
      "            'original_time': original_time,\n",
      "            'min_time': min_time,\n",
      "            'max_time': max_time,\n",
      "            'median_time': median_time,\n",
      "            'num_solutions': num_solutions,\n",
      "            'best_performance_mutant': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df = process_and_validate_data(original_df, globally_blocked_clauses_dfs)\n",
      "print(result_df.to_string())\n",
      "59/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df, mode):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['original_time'], subset[f'{mode}_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'{mode} Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs {mode} Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'original_vs_{mode}_gbc_3.6.png')\n",
      "\n",
      "plot_dot_chart(result_df, 'min')\n",
      "59/7:\n",
      "\n",
      "\n",
      "plot_dot_chart(result_df, 'median')\n",
      "59/8:\n",
      "\n",
      "\n",
      "plot_dot_chart(result_df, 'max')\n",
      "59/9:\n",
      "# Assuming results_df is already defined and loaded with data\n",
      "\n",
      "# Function to process each row in the DataFrame\n",
      "def process_row(row):\n",
      "    file_name = row['file_name']\n",
      "    mutant_name = row['best_performance_mutant']\n",
      "\n",
      "    # Define the file paths based on the given pattern\n",
      "    file_name_without_cnf = file_name.replace(\".cnf\", \"\")\n",
      "    mutant_name_without_scramble = mutant_name.replace(\"scramble\", \"\")\n",
      "    \n",
      "    global_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt\"\n",
      "    filtered_clauses_path = f\"results_scranfilized/{file_name_without_cnf}/{mutant_name}/global_clauses/{mutant_name_without_scramble}.global_clauses.txt.filtered\"\n",
      "    \n",
      "    # Check if the files exist\n",
      "    if not os.path.exists(global_clauses_path) or not os.path.exists(filtered_clauses_path):\n",
      "        raise FileNotFoundError(f\"One of the files does not exist: {global_clauses_path} or {filtered_clauses_path}\")\n",
      "    \n",
      "    # Read the filtered file to analyze the line lengths\n",
      "    with open(filtered_clauses_path, 'r') as f:\n",
      "        lines = f.readlines()\n",
      "    \n",
      "    num_gbc_unfiltered = sum(1 for line in open(global_clauses_path))\n",
      "    num_gbc_filtered = len(lines)\n",
      "    \n",
      "    # Get the lengths of each line\n",
      "    line_lengths = [len(line.split()) for line in lines]\n",
      "    \n",
      "    # Calculate the smallest length and frequency of that length\n",
      "    # print(filtered_clauses_path)\n",
      "    smallest_gbc_length = \"N/A\" if line_lengths == [] else min(line_lengths)\n",
      "    freq_smallest_gbc_length = \"N/A\" if line_lengths == [] else line_lengths.count(smallest_gbc_length)\n",
      "    \n",
      "    # print(\"here\")\n",
      "    return pd.Series({\n",
      "        \"num gbc unfiltered\": num_gbc_unfiltered,\n",
      "        \"num gbc filtered\": num_gbc_filtered,\n",
      "        \"smallest gbc length\": smallest_gbc_length,\n",
      "        \"freq. smallest gbc length\": freq_smallest_gbc_length\n",
      "    })\n",
      "\n",
      "# Apply the function to each row in the DataFrame\n",
      "\n",
      "result_df_new_info = result_df\n",
      "result_df_new_info[['num gbc unfiltered', 'num gbc filtered', 'smallest gbc length', 'freq smallest gbc length']] = result_df.apply(process_row, axis=1)\n",
      "59/10:\n",
      "print(result_df_new_info.to_string())\n",
      "\n",
      "result_df_new_info.to_csv(\"results_3.6.csv\")\n",
      "59/11:\n",
      "original_files = {\n",
      "    \"df0\": \"slurm-29487980.out\",\n",
      "    \"df1\": \"slurm-29487981.out\",\n",
      "    \"df2\": \"slurm-29487982.out\",\n",
      "    \"df3\": \"slurm-29487983.out\",\n",
      "    \"df4\": \"slurm-29487984.out\",\n",
      "    \"df5\": \"slurm-29487985.out\",\n",
      "    \"df6\": \"slurm-29487986.out\",\n",
      "    \"df7\": \"slurm-29487988.out\",\n",
      "    \"df8\": \"slurm-29487989.out\",\n",
      "    \"df9\": \"slurm-29487990.out\",\n",
      "}\n",
      "\n",
      "original_dfs = {}\n",
      "\n",
      "for key in original_files.keys():\n",
      "    df = extract_results_gbc(original_files[key])\n",
      "    original_dfs[key] = df\n",
      "\n",
      "print(original_dfs[\"df6\"].to_string())\n",
      "59/12:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        original_time = pd.to_numeric(original_row['time_limit'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        print(combined_df.to_string())\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append(original_row + {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "59/13:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        # original_time = pd.to_numeric(original_row['original_time'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        print(combined_df.to_string())\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append(original_row + {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "59/14:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        # original_time = pd.to_numeric(original_row['original_time'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        print(combined_df.to_string())\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        print(original_row)\n",
      "        results.append(original_row + {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "59/15:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        # original_time = pd.to_numeric(original_row['original_time'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append(original_row.to_dict() + {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "59/16:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        # original_time = pd.to_numeric(original_row['original_time'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        results.append(original_row.to_dict() | {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "59/17:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        # original_time = pd.to_numeric(original_row['original_time'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        original_row_dict = original_row.to_dict()\n",
      "\n",
      "        original_row.update ( {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        }\n",
      "\n",
      "        )\n",
      "        results.append(original_row_dict)\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "59/18:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        # original_time = pd.to_numeric(original_row['original_time'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        original_row_dict = original_row.to_dict()\n",
      "\n",
      "        original_row_dict.update ( {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "\n",
      "        results.append(original_row_dict)\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "59/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart_multiple_originals(result_df, mode):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset[f'{mode}_time_original'], subset[f'{mode}_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('{mode} Original Time')\n",
      "    plt.ylabel(f'{mode} Time with GBC')\n",
      "    plt.title(f'Dot Plot of {mode} Original Formula Time vs {mode} Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'{mode}_original_vs_{mode}_gbc_3.6.png')\n",
      "\n",
      "plot_dot_chart(result_df_multiple_originals, 'min')\n",
      "59/20: plot_dot_chart(result_df_multiple_originals, 'median')\n",
      "59/21: plot_dot_chart(result_df_multiple_originals, 'max')\n",
      "59/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart_multiple_originals(result_df, mode):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset[f'{mode}_time_original'], subset[f'{mode}_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('{mode} Original Time')\n",
      "    plt.ylabel(f'{mode} Time with GBC')\n",
      "    plt.title(f'Dot Plot of {mode} Original Formula Time vs {mode} Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'{mode}_original_vs_{mode}_gbc_3.6.png')\n",
      "\n",
      "plot_dot_chart_multiple_originals(result_df_multiple_originals, 'min')\n",
      "59/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart_multiple_originals(result_df, mode):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset[f'{mode}_time_original'], subset[f'{mode}_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel(f'{mode} Original Time')\n",
      "    plt.ylabel(f'{mode} Time with GBC')\n",
      "    plt.title(f'Dot Plot of {mode} Original Formula Time vs {mode} Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'{mode}_original_vs_{mode}_gbc_3.6.png')\n",
      "\n",
      "plot_dot_chart_multiple_originals(result_df_multiple_originals, 'min')\n",
      "59/24: plot_dot_chart_multiple_originals(result_df_multiple_originals, 'median')\n",
      "59/25: plot_dot_chart_multiple_originals(result_df_multiple_originals, 'max')\n",
      "59/26:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart_multiple_originals(result_df, mode):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset[f'{mode}_time_original'], subset[f'{mode}_time'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel(f'{mode} Original Time')\n",
      "    plt.ylabel(f'{mode} Time with GBC')\n",
      "    plt.title(f'Dot Plot of {mode} Scranfilized Original Formula Time vs {mode} Time of Scranfilized Formula + GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'{mode}_original_vs_{mode}_gbc_3.6.png')\n",
      "\n",
      "plot_dot_chart_multiple_originals(result_df_multiple_originals, 'min')\n",
      "59/27: plot_dot_chart_multiple_originals(result_df_multiple_originals, 'median')\n",
      "59/28: plot_dot_chart_multiple_originals(result_df_multiple_originals, 'max')\n",
      "59/29:\n",
      "\n",
      "def process_and_validate_data(original_df, original_dfs):\n",
      "    conflicting_files = []\n",
      "    results = []\n",
      "    \n",
      "    for file_name in original_df['file_name']:\n",
      "        # Get original result and time\n",
      "        original_row = original_df[original_df['file_name'] == file_name].iloc[0]\n",
      "        original_result = original_row['result']\n",
      "        # original_time = original_row['time_limit']\n",
      "        # original_time = pd.to_numeric(original_row['original_time'], errors='coerce')\n",
      "# result_df['min_time'] = pd.to_numeric(result_df['min_time'], errors='coerce')\n",
      "\n",
      "        \n",
      "        # Collect all matching entries from original_dfs\n",
      "        blocked_entries = []\n",
      "        for df in original_dfs.values():\n",
      "            subset = df[df['file_name'] == file_name]\n",
      "            if not subset.empty:\n",
      "                blocked_entries.append(subset)\n",
      "        \n",
      "        if not blocked_entries:\n",
      "            continue  # Skip if file_name does not appear in any globally blocked dataframe\n",
      "        \n",
      "        combined_df = pd.concat(blocked_entries)\n",
      "        combined_df = combined_df.reset_index(drop=True)\n",
      "        blocked_results = set(combined_df['result'])\n",
      "        num_solutions = len(combined_df)\n",
      "\n",
      "        # blocked_times = combined_df['time_limit']\n",
      "        blocked_times = pd.to_numeric(combined_df['time_limit'], errors='coerce').dropna()\n",
      "\n",
      "        if not blocked_times.empty:\n",
      "            best_index = blocked_times.idxmin()\n",
      "            if best_index in combined_df.index:\n",
      "                best_performance_mutant = combined_df.loc[best_index, 'mutant_name']\n",
      "            else:\n",
      "                best_performance_mutant = None\n",
      "        else:\n",
      "            best_performance_mutant = None\n",
      "        # if not blocked_times.empty:\n",
      "        #     # Find all indices with the minimum time limit\n",
      "        #     min_time = blocked_times.min()\n",
      "        #     best_indexes = blocked_times[blocked_times == min_time].index\n",
      "            \n",
      "        #     if not best_indexes.empty:\n",
      "        #         # Retrieve the 'mutant_name' values for all the rows with the minimum time limit\n",
      "        #         best_performance_mutants = combined_df.loc[best_indexes, 'mutant_name'].tolist()\n",
      "        #     else:\n",
      "        #         best_performance_mutants = None\n",
      "        # else:\n",
      "        #     best_performance_mutants = None\n",
      "\n",
      "        # if len(best_performance_mutants) == 1:\n",
      "        #     best_performance_mutant = best_performance_mutants[0]\n",
      "        # else:\n",
      "        #     print(blocked_times)\n",
      "        #     print(min_time)\n",
      "        #     raise ValueError(\"multiple best performing mutants\", best_performance_mutants)\n",
      "        \n",
      "        # Validate results\n",
      "        if ('SAT' in blocked_results and 'UNSAT' == original_result) or ('UNSAT' in blocked_results and 'SAT' == original_result) or ('SAT' in blocked_results and 'UNSAT' in blocked_results):\n",
      "            print(f\"ERROR: {file_name}\")\n",
      "            conflicting_files.append(file_name)\n",
      "        \n",
      "        # Determine final result\n",
      "\n",
      "        # if 'SAT' in blocked_results and 'UNSAT' in blocked_results:\n",
      "\n",
      "        elif 'SAT' in blocked_results:\n",
      "            final_result = 'SAT'\n",
      "        elif 'UNSAT' in blocked_results:\n",
      "            final_result = 'UNSAT'\n",
      "        else:\n",
      "            final_result = original_result\n",
      "        \n",
      "        # Aggregate time statistics\n",
      "        # print(blocked_times.to_list())\n",
      "        if not blocked_times.empty:\n",
      "            min_time = blocked_times.min()\n",
      "            max_time = blocked_times.max()\n",
      "            median_time = blocked_times.median()\n",
      "\n",
      "            # Sanity check\n",
      "            if min_time > max_time:\n",
      "                raise ValueError(f\"Inconsistent min/max times for file {file_name}: min_time={min_time}, max_time={max_time}\")\n",
      "        else:\n",
      "            min_time = max_time = median_time = None\n",
      "        \n",
      "        original_row_dict = original_row.to_dict()\n",
      "\n",
      "        original_row_dict.update ( {\n",
      "            # 'file_name': file_name,\n",
      "            # 'result': final_result,\n",
      "            # 'original_time': original_time,\n",
      "            'min_time_original': min_time,\n",
      "            'max_time_original': max_time,\n",
      "            'median_time_original': median_time,\n",
      "            'best_performance_mutant_original': best_performance_mutant.replace(\".cnf\", \"\"),\n",
      "            # 'fixes': combined_df['mutant_name'].to_list()\n",
      "        })\n",
      "\n",
      "        results.append(original_row_dict)\n",
      "    \n",
      "    if conflicting_files:\n",
      "        raise ValueError(f'Conflicting results found for files: {conflicting_files}')\n",
      "    \n",
      "    return pd.DataFrame(results)\n",
      "\n",
      "\n",
      "# Example usage\n",
      "result_df_multiple_originals = process_and_validate_data(result_df_new_info, original_dfs)\n",
      "print(result_df_multiple_originals.to_string())\n",
      "result_df_multiple_originals.to_csv(\"results_multiple_originals_3.6.csv\")\n",
      "65/1:\n",
      "import os\n",
      "import re\n",
      "import matplotlib.pyplot as plt\n",
      "from statistics import mean, median\n",
      "\n",
      "\n",
      "def count_numbers_in_line(line):\n",
      "    \"\"\"Count the number of numbers (integers or floats) in a line.\"\"\"\n",
      "    return len(re.findall(r'-?\\d+\\.?\\d*', line))\n",
      "\n",
      "def process_files_in_folder(folder_path):\n",
      "    \"\"\"Process each file in the folder and return the combined list of numbers.\"\"\"\n",
      "    all_numbers = []\n",
      "\n",
      "    # Iterate over all files in the folder\n",
      "    clause_lengths = []\n",
      "    for filename in os.listdir(folder_path):\n",
      "        file_path = os.path.join(folder_path, filename)\n",
      "\n",
      "        # Only process text files\n",
      "        if os.path.isfile(file_path):\n",
      "            with open(file_path, 'r') as file:\n",
      "                for line in file:\n",
      "                    # Count numbers in each line\n",
      "                    if line.strip():\n",
      "                        clause_lengths += [len(line.split())]\n",
      "                    \n",
      "\n",
      "    return clause_lengths\n",
      "\n",
      "def plot_histogram(numbers, output_file, max_range = 120, title  = f'Size of Globally blocked clauses'):\n",
      "    \"\"\"Plot a histogram of the numbers and save it to a file.\"\"\"\n",
      "    plt.figure(figsize=(10, 6))\n",
      "    plt.hist(numbers, bins=100, range=(0,max_range), edgecolor='black')\n",
      "    plt.title('Size of Globally blocked clauses ' + title)\n",
      "    plt.xlabel('Size of Clause')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.savefig(output_file)\n",
      "    print(f'Histogram saved to {output_file}')\n",
      "65/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "70/1:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "70/2:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            print(line)\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, result, time_limit = match.groups()\n",
      "                data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            match = pattern.search(line)\n",
      "            if match:\n",
      "                file_name, time_limit = match.groups()\n",
      "                data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "70/3:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path,start):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, result, time_limit = match.groups()\n",
      "                    data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, time_limit = match.groups()\n",
      "                    data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29474880.out\", \"1.\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "70/4:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path,start):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, result, time_limit = match.groups()\n",
      "                    data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, time_limit = match.groups()\n",
      "                    data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29516543.out\", \"1.\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "70/5:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path,start):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, result, time_limit = match.groups()\n",
      "                    data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, time_limit = match.groups()\n",
      "                    data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29516543.out\", \"1.\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "70/6:\n",
      "other_df = extract_results(\"slurm-29516543.out\", \"2.\")\n",
      "print(other_df.to_string())\n",
      "70/7:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path,start):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, result, time_limit = match.groups()\n",
      "                    data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, time_limit = match.groups()\n",
      "                    data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "original_df = extract_results(\"slurm-29516543.out\", \"1.\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(original_df.to_string())\n",
      "70/8:\n",
      "other_df = extract_results(\"slurm-29516543.out\", \"2.\")\n",
      "print(other_df.to_string())\n",
      "70/9:\n",
      "merged_df = no_gbc_df.merge(gbc_df, on='file_name', suffixes=('_no_gbc', '_gbc'))\n",
      "\n",
      "print(merged_df.to_string())\n",
      "70/10:\n",
      "import re\n",
      "import pandas as pd\n",
      "\n",
      "def extract_results(file_path,start):\n",
      "    pattern = re.compile(r\"The file (.*?) returned (.*?) in time (.*?)!\")\n",
      "    data = []\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, result, time_limit = match.groups()\n",
      "                    data.append((file_name, result, time_limit))\n",
      "\n",
      "    pattern = re.compile(r\"The file (.*?) timed out in time (.*?)!\")\n",
      "    \n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        for line in file:\n",
      "            if line.startswith(start):\n",
      "                match = pattern.search(line)\n",
      "                if match:\n",
      "                    file_name, time_limit = match.groups()\n",
      "                    data.append((file_name, \"TIMEOUT\", time_limit)) \n",
      "    \n",
      "    df = pd.DataFrame(data, columns=[\"file_name\", \"result\", \"time_limit\"])\n",
      "    return df\n",
      "\n",
      "gbc_df = extract_results(\"slurm-29516543.out\", \"1.\")\n",
      "\n",
      "# The file ae9522ea003ea9f75891b2d37a5e264b-srhd-sgi-m37-q446.25-n35-p30-s33692332.cnf returned SAT in time 4.4467597007751465!\n",
      "\n",
      "print(gbc_df.to_string())\n",
      "70/11:\n",
      "no_gbc_df = extract_results(\"slurm-29516543.out\", \"2.\")\n",
      "print(no_gbc_df.to_string())\n",
      "70/12:\n",
      "merged_df = no_gbc_df.merge(gbc_df, on='file_name', suffixes=('_no_gbc', '_gbc'))\n",
      "\n",
      "print(merged_df.to_string())\n",
      "70/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['time_limit_no_gbc'], subset[f'time_limit_gbc'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result'] == result_type]\n",
      "        plt.scatter(subset['time_limit_no_gbc'], subset[f'time_limit_gbc'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result_no_gbc'] == result_type]\n",
      "        plt.scatter(subset['time_limit_no_gbc'], subset[f'time_limit_gbc'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result_no_gbc'] == result_type]\n",
      "        plt.scatter(subset['time_limit_no_gbc'], subset[f'time_limit_gbc'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    # plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result_no_gbc'] == result_type]\n",
      "        plt.scatter(subset['time_limit_no_gbc'], subset[f'time_limit_gbc'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    # plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    # plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o'}#, 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result_no_gbc'] == result_type]\n",
      "        plt.scatter(subset['time_limit_no_gbc'], subset['time_limit_gbc'], label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    # plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    # plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o'}#, 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result_no_gbc'] == result_type]\n",
      "        plt.scatter(subset['time_limit_no_gbc'], subset['time_limit_gbc']*2, label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    # plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    # plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o'}#, 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result_no_gbc'] == result_type]\n",
      "        plt.scatter(pd.to_numeric(subset['time_limit_no_gbc']), pd.to_numeric(subset['time_limit_gbc']), label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's'}#, 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result_no_gbc'] == result_type]\n",
      "        plt.scatter(pd.to_numeric(subset['time_limit_no_gbc']), pd.to_numeric(subset['time_limit_gbc']), label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Function to generate dot plot\n",
      "def plot_dot_chart(result_df):\n",
      "    plt.figure(figsize=(10, 10))\n",
      "    \n",
      "    # Define marker styles for different results\n",
      "    marker_styles = {'SAT': 'o', 'UNSAT': 's', 'TIMEOUT': 'D'}\n",
      "    \n",
      "    # Create plot with different colors and markers\n",
      "    for result_type, marker in marker_styles.items():\n",
      "        subset = result_df[result_df['result_gbc'] == result_type]\n",
      "        plt.scatter(pd.to_numeric(subset['time_limit_no_gbc']), pd.to_numeric(subset['time_limit_gbc']), label=result_type, marker=marker, alpha=0.7)\n",
      "    \n",
      "    # Set axis labels and title\n",
      "    plt.xlabel('Original Time')\n",
      "    plt.ylabel(f'Time with GBC')\n",
      "    plt.title(f'Dot Plot of Original Formula Time vs Time with GBC')\n",
      "    \n",
      "    # Adjust x-axis and y-axis scales to show proper numbers\n",
      "    plt.xticks(rotation=45)\n",
      "    plt.yticks()\n",
      "    \n",
      "    # Add an x=y reference line\n",
      "    # min_val = min(result_df['original_time'].min(), result_df['min_time'].min())\n",
      "    # max_val = max(result_df['original_time'].max(), result_df['min_time'].max())\n",
      "    plt.plot([0, 1800], [0, 1800], linestyle='--', color='black', alpha=0.7, label='')\n",
      "    \n",
      "    # Customize grid\n",
      "    plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.7)\n",
      "    plt.legend()\n",
      "    # plt.show()\n",
      "    plt.savefig(f'cadical_vs_cadical_gbc_time_gbc_3.8.png')\n",
      "\n",
      "plot_dot_chart(merged_df)\n",
      "70/23:\n",
      "improved_df = merged_df[\n",
      "    (merged_df['time_limit_gbc'] <= 0.8 * merged_df['time_limit_no_gbc']) &\n",
      "    (merged_df['result_gbc'].isin(['SAT', 'UNSAT']))\n",
      "]\n",
      "\n",
      "print(improved_df.to_string())\n",
      "70/24:\n",
      "improved_df = merged_df[\n",
      "    (pd.to_numeric(merged_df['time_limit_gbc']) <= 0.8 * pd.to_numeric(merged_df['time_limit_no_gbc'])) &\n",
      "    (pd.to_numeric(merged_df['result_gbc']).isin(['SAT', 'UNSAT']))\n",
      "]\n",
      "\n",
      "print(improved_df.to_string())\n",
      "70/25:\n",
      "improved_df = merged_df[\n",
      "    (pd.to_numeric(merged_df['time_limit_gbc']) <= 0.8 * pd.to_numeric(merged_df['time_limit_no_gbc'])) &\n",
      "    (merged_df['result_gbc'].isin(['SAT', 'UNSAT']))\n",
      "]\n",
      "\n",
      "print(improved_df.to_string())\n",
      "   1: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
